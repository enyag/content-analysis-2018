{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here](http://nlp.stanford.edu:8080/corenlp/) to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "#additionally...\n",
    "import chardet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run this _once_ to download everything, you will also need [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) if you are using Windows or MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads, this will take 5-10 minutes\n",
      "../stanford-NLP/parser already exists, skipping download\n",
      "../stanford-NLP/ner already exists, skipping download\n",
      "../stanford-NLP/postagger already exists, skipping download\n",
      "../stanford-NLP/core already exists, skipping download\n",
      "Done setting up the Stanford NLP collection\n"
     ]
    }
   ],
   "source": [
    "lucem_illud.setupStanfordNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have stanford-NLP setup before importing, so we are doing the import here. IF you have stanford-NLP working, you can import at the beginning like you would with any other library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import lucem_illud.stanford as stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Information Extraction is a module packaged within the Stanford Core NLP package, but it is not yet supported by `nltk`. As a result, we have defining our own `lucem_illud` function that runs the Stanford Core NLP java code right here. For other projects, it is often useful to use Java or other programs (in C, C++) within a python workflow, and this is an example. `stanford.openIE()` takes in a string or list of strings and then produces as output all the subject, verb, object (SVO) triples Stanford Corenlp can find, as a DataFrame. You can do this through links to the Stanford Core NLP project that we provide here, or play with their interface directly (in the penultimate cell of this notebook), which produces data in \"pretty graphics\" like this example parsing of the first sentence in the \"Shooting of Trayvon Martin\" Wikipedia article:\n",
    "\n",
    "![Output 1](../data/stanford_core1.png)\n",
    "![Output 2](../data/stanford_core2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will illustrate these tools on some *very* short examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [nltk.word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. The Stanford POS tagger uses the [Penn Treebank tag set]('http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports') to POS tag words from input sentences. As discussed in the second assignment, this is a relatively precise tagset, which allows more informative tags, and also more opportunities to err :-).\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NNS'), ('.', '.')], [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')], [('While', 'IN'), ('in', 'IN'), ('France', 'NNP'), (',', ','), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('interview', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Wall', 'NNP'), ('Street', 'NNP'), ('Journal', 'NNP'), ('.', '.')], [('Trayvon', 'NNP'), ('Benjamin', 'NNP'), ('Martin', 'NNP'), ('was', 'VBD'), ('an', 'DT'), ('African', 'NNP'), ('American', 'NNP'), ('from', 'IN'), ('Miami', 'NNP'), ('Gardens', 'NNP'), (',', ','), ('Florida', 'NNP'), (',', ','), ('who', 'WP'), (',', ','), ('at', 'IN'), ('17', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('was', 'VBD'), ('fatally', 'RB'), ('shot', 'VBN'), ('by', 'IN'), ('George', 'NNP'), ('Zimmerman', 'NNP'), (',', ','), ('a', 'DT'), ('neighborhood', 'NN'), ('watch', 'NN'), ('volunteer', 'NN'), (',', ','), ('in', 'IN'), ('Sanford', 'NNP'), (',', ','), ('Florida', 'NNP'), ('.', '.')], [('Buffalo', 'NNP'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "pos_sents = stanford.postTagger.tag_sents(tokenized_text)\n",
    "print(pos_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite good. Now we will try POS tagging with a somewhat larger corpus. We consider a few of the top posts from the reddit data we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the 10 highest scoring posts and tokenizing the sentences. Once again, notice that we aren't going to do any kind of stemming this week (although *semantic* normalization may be performed where we translate synonyms into the same focal word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldie-gold</td>\n",
       "      <td>False</td>\n",
       "      <td>12650</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>This just happened...  So, I had a laptop syst...</td>\n",
       "      <td>Engineer is doing drugs!! No. No they aren't.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[This, just, happened, ...], [So, ,, I, had, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheDroolinFool</td>\n",
       "      <td>False</td>\n",
       "      <td>13152</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>Another tale from the out of hours IT desk... ...</td>\n",
       "      <td>\"I need you to fix Google Bing immediately!\"</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[Another, tale, from, the, out, of, hours, IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clickity_clickity</td>\n",
       "      <td>False</td>\n",
       "      <td>13404</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>[Part 1](http://www.reddit.com/r/talesfromtech...</td>\n",
       "      <td>Jack, the Worst End User, Part 4</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[[, Part, 1, ], (, http, :, //www.reddit.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECGaz</td>\n",
       "      <td>False</td>\n",
       "      <td>13724</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>&gt; $Me  - Hello, IT.   &gt; $Usr - Hi, I am still ...</td>\n",
       "      <td>Hi, I am still off sick but I am not.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[&gt;, $, Me, -, Hello, ,, IT, .], [&gt;, $, Usr, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guitarsdontdance</td>\n",
       "      <td>False</td>\n",
       "      <td>14089</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>So my story starts on what was a normal day ta...</td>\n",
       "      <td>\"Don't bother sending a tech, I'll be dead by ...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[So, my, story, starts, on, what, was, a, nor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author  over_18  score                subreddit  \\\n",
       "4        goldie-gold    False  12650  Tales From Tech Support   \n",
       "3     TheDroolinFool    False  13152  Tales From Tech Support   \n",
       "2  Clickity_clickity    False  13404  Tales From Tech Support   \n",
       "1             SECGaz    False  13724  Tales From Tech Support   \n",
       "0   guitarsdontdance    False  14089  Tales From Tech Support   \n",
       "\n",
       "                                                text  \\\n",
       "4  This just happened...  So, I had a laptop syst...   \n",
       "3  Another tale from the out of hours IT desk... ...   \n",
       "2  [Part 1](http://www.reddit.com/r/talesfromtech...   \n",
       "1  > $Me  - Hello, IT.   > $Usr - Hi, I am still ...   \n",
       "0  So my story starts on what was a normal day ta...   \n",
       "\n",
       "                                               title  \\\n",
       "4      Engineer is doing drugs!! No. No they aren't.   \n",
       "3       \"I need you to fix Google Bing immediately!\"   \n",
       "2                   Jack, the Worst End User, Part 4   \n",
       "1              Hi, I am still off sick but I am not.   \n",
       "0  \"Don't bother sending a tech, I'll be dead by ...   \n",
       "\n",
       "                                                 url  \\\n",
       "4  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "0  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "                                           sentences  \n",
       "4  [[This, just, happened, ...], [So, ,, I, had, ...  \n",
       "3  [[Another, tale, from, the, out, of, hours, IT...  \n",
       "2  [[[, Part, 1, ], (, http, :, //www.reddit.com/...  \n",
       "1  [[>, $, Me, -, Hello, ,, IT, .], [>, $, Usr, -...  \n",
       "0  [[So, my, story, starts, on, what, was, a, nor...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-10:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, JJ), (year, NN), (,, ,), (Help, NN), ...\n",
       "8    [[(First, JJ), (post, NN), (in, IN), (quite, R...\n",
       "7    [[([, NNP), (Original, NNP), (Post, NNP), (], ...\n",
       "6    [[(I, PRP), (witnessed, VBD), (this, DT), (ast...\n",
       "5    [[(I, PRP), (work, VBP), (Helpdesk, NNP), (for...\n",
       "4    [[(This, DT), (just, RB), (happened, VBN), (.....\n",
       "3    [[(Another, DT), (tale, NN), (from, IN), (the,...\n",
       "2    [[([, NNP), (Part, NNP), (1, CD), (], FW), ((,...\n",
       "1    [[(>, JJR), ($, $), (Me, PRP), (-, :), (Hello,...\n",
       "0    [[(So, RB), (my, PRP$), (story, NN), (starts, ...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of `NN` (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('password', 21),\n",
       " ('(', 19),\n",
       " ('time', 14),\n",
       " (')', 14),\n",
       " ('lot', 12),\n",
       " ('computer', 12),\n",
       " ('life', 11),\n",
       " ('email', 11),\n",
       " ('**Genius**', 10),\n",
       " ('message', 9),\n",
       " ('**Me**', 9),\n",
       " ('system', 9),\n",
       " ('day', 9),\n",
       " ('call', 8),\n",
       " ('laptop', 8),\n",
       " ('office', 8),\n",
       " ('part', 8),\n",
       " ('today', 8),\n",
       " ('story', 8),\n",
       " ('user', 7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of top verbs (`VB`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 18),\n",
       " ('have', 17),\n",
       " ('get', 14),\n",
       " ('do', 11),\n",
       " ('change', 9),\n",
       " ('make', 8),\n",
       " ('know', 7),\n",
       " ('say', 7),\n",
       " ('help', 6),\n",
       " ('look', 6),\n",
       " ('tell', 6),\n",
       " ('send', 6),\n",
       " ('go', 5),\n",
       " ('work', 4),\n",
       " ('use', 4),\n",
       " ('receive', 4),\n",
       " ('thank', 4),\n",
       " ('feel', 4),\n",
       " ('want', 4),\n",
       " ('call', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the adjectives that modify the word, \"computer\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unrestricted', 'own'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'computer'\n",
    "NResults = set()\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating POS tagger\n",
    "\n",
    "We can check the POS tagger by running it on a manually tagged corpus and identifying a reasonable error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank = nltk.corpus.treebank\n",
    "treeBank.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanfordTags = stanford.postTagger.tag_sents(treeBank.sents()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Dutch  \tStanford: JJ\tTreebank: NNP\n",
      "Word: publishing  \tStanford: NN\tTreebank: VBG\n",
      "Word: used  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: later  \tStanford: RB\tTreebank: JJ\n",
      "Word: New  \tStanford: NNP\tTreebank: JJ\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: replaced  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: JJ\n",
      "Word: expected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: study  \tStanford: VBD\tTreebank: VBP\n",
      "Word: studied  \tStanford: VBD\tTreebank: VBN\n",
      "Word: industrialized  \tStanford: JJ\tTreebank: VBN\n",
      "Word: Lorillard  \tStanford: NNP\tTreebank: NN\n",
      "Word: found  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: rejected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: poured  \tStanford: VBN\tTreebank: VBD\n",
      "Word: in  \tStanford: IN\tTreebank: RP\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "The Precision is 96.547%\n"
     ]
    }
   ],
   "source": [
    "NumDiffs = 0\n",
    "for sentIndex in range(len(stanfordTags)):\n",
    "    for wordIndex in range(len(stanfordTags[sentIndex])):\n",
    "        if stanfordTags[sentIndex][wordIndex][1] != treeBank.tagged_sents()[sentIndex][wordIndex][1]:\n",
    "            if treeBank.tagged_sents()[sentIndex][wordIndex][1] != '-NONE-':\n",
    "                print(\"Word: {}  \\tStanford: {}\\tTreebank: {}\".format(stanfordTags[sentIndex][wordIndex][0], stanfordTags[sentIndex][wordIndex][1], treeBank.tagged_sents()[sentIndex][wordIndex][1]))\n",
    "                NumDiffs += 1\n",
    "total = sum([len(s) for s in stanfordTags])\n",
    "print(\"The Precision is {:.3f}%\".format((total-NumDiffs)/total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the stanford POS tagger is quite good. Nevertheless, for a 20 word sentence, we only have a 66% chance ($1-.96^{20}$) of tagging (and later parsing) it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional frequencies (e.g., adjectives associated with nouns of interest or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date                                              title  \\\n",
      "0    12/1/16  Preventing the Exploitation of Information and...   \n",
      "1   10/10/16  Keynote Address at the Singapore International...   \n",
      "2    9/28/16  The Persistent Threat of North Korea and Devel...   \n",
      "3    9/19/16  Statement Before the Presidential Commission o...   \n",
      "4     6/3/16                                   TEDx Tysons Talk   \n",
      "5    5/25/16  International Cybersecurity Strategy: Deterrin...   \n",
      "6     3/1/16  Remarks by Attorney General Loretta E. Lynch a...   \n",
      "7    2/26/16  Inter-American Committee Against Terrorism (CI...   \n",
      "8    2/11/16  The New Face of Terrorism: Countering Violent ...   \n",
      "9    7/29/15  Remarks for Panel Session \"Development of Cybe...   \n",
      "10    5/4/15  Department of Commerce Cybersecurity Trade Mis...   \n",
      "11   2/24/15                      New Reward for Cyber Fugitive   \n",
      "12   1/13/15  The North Korean Threat: Nuclear, Missiles and...   \n",
      "13    3/4/14  As Prepared Remarks at Georgetown University I...   \n",
      "14   12/3/13  Remarks at a Cyber Partnership Agreement Signi...   \n",
      "15  10/21/13             Remarks at High Level Leader's Meeting   \n",
      "16  11/26/12  Cyber Security: Project Cyber Monday 3 and Pro...   \n",
      "17   10/5/12    Video Remarks for the Budapest Cyber Conference   \n",
      "18   9/18/12                    International Law in Cyberspace   \n",
      "19   8/31/12  The Organization of American States' Inter-Ame...   \n",
      "20    3/7/12       Strengthening Cyber Security in the Americas   \n",
      "21  11/17/11  Leading With Diplomacy to Strengthen Stability...   \n",
      "22  10/31/11  Background Briefing: Secretary Clinton's Parti...   \n",
      "23  10/18/11                               Cybersecurity Update   \n",
      "24   5/18/11  Release of the Obama Administration's Internat...   \n",
      "25   5/16/11  Remarks on the Release of President Obama Admi...   \n",
      "\n",
      "                                                 text  \n",
      "0   I would like to begin by thanking the UN Count...  \n",
      "1   Good morning! Thank you to Prime Minister Lee ...  \n",
      "2   Introduction. Chairman Gardner, Ranking Member...  \n",
      "3   Chairman Donilon, Vice Chairman Palmisano, and...  \n",
      "4   So what do zombies have to do with diplomacy? ...  \n",
      "5   Chairman Gardner, Ranking Member Cardin, membe...  \n",
      "6   Good afternoon, and thank you for that warm we...  \n",
      "7   Background: The Sixteenth Regular Session of t...  \n",
      "8   Good afternoon. IÍm looking forward to our dis...  \n",
      "9   Thank you for the opportunity to discuss devel...  \n",
      "10  Great. Well, thank you, and first of all good ...  \n",
      "11  ASSISTANT ATTORNEY GENERAL CALDWELL: Thank you...  \n",
      "12  Mr. Chairman, Ranking Member Engel, and Member...  \n",
      "13  ItÍs a pleasure to be here today. I want to be...  \n",
      "14  MODERATOR: Good afternoon, and thank you all f...  \n",
      "15  Good morning, Minister, Chairman Sasongko, Mr....  \n",
      "16  MODERATOR: Good afternoon, everyone. The Forei...  \n",
      "17  I am delighted to be able to send greetings to...  \n",
      "18  Thank you, Colonel Brown, for your kind invita...  \n",
      "19  Good Morning. Mr. Assistant Secretary General ...  \n",
      "20  I want to thank the Committee for the opportun...  \n",
      "21  Thank you for your kind introduction. I am hon...  \n",
      "22  MS. FULTON: All right. Thank you. And thanks e...  \n",
      "23  MR. PAINTER: Thanks, and thanks for coming her...  \n",
      "24  MODERATOR: Thank you, everyone, for coming to ...  \n",
      "25  Well, thank you very much. As you can guess fr...  \n"
     ]
    }
   ],
   "source": [
    "#get corpus of 26 DOS speeches\n",
    "DOSSpeeches = '/Users/Enya/Desktop/DOS_Speeches.csv'\n",
    "DOSSpeechesDF = pandas.read_csv(DOSSpeeches, encoding='Latin-1')\n",
    "print (DOSSpeechesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOSSpeechesDF['tokenized-text'] = DOSSpeechesDF['text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOSSpeechesDF['sentences'] = DOSSpeechesDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "DOSSpeechesDF['POS-sents'] = DOSSpeechesDF['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[(I, PRP), (would, MD), (like, VB), (to, TO),...\n",
       "1     [[(Good, JJ), (morning, NN), (!, .)], [(Thank,...\n",
       "2     [[(Introduction, NN), (., .)], [(Chairman, NNP...\n",
       "3     [[(Chairman, NNP), (Donilon, NNP), (,, ,), (Vi...\n",
       "4     [[(So, RB), (what, WP), (do, VBP), (zombies, N...\n",
       "5     [[(Chairman, NNP), (Gardner, NNP), (,, ,), (Ra...\n",
       "6     [[(Good, JJ), (afternoon, NN), (,, ,), (and, C...\n",
       "7     [[(Background, NN), (:, :), (The, DT), (Sixtee...\n",
       "8     [[(Good, JJ), (afternoon, NN), (., .)], [(IÍm,...\n",
       "9     [[(Thank, VB), (you, PRP), (for, IN), (the, DT...\n",
       "10    [[(Great, JJ), (., .)], [(Well, RB), (,, ,), (...\n",
       "11    [[(ASSISTANT, NNP), (ATTORNEY, NNP), (GENERAL,...\n",
       "12    [[(Mr., NNP), (Chairman, NNP), (,, ,), (Rankin...\n",
       "13    [[(ItÍs, VBZ), (a, DT), (pleasure, NN), (to, T...\n",
       "14    [[(MODERATOR, NN), (:, :), (Good, JJ), (aftern...\n",
       "15    [[(Good, JJ), (morning, NN), (,, ,), (Minister...\n",
       "16    [[(MODERATOR, NN), (:, :), (Good, JJ), (aftern...\n",
       "17    [[(I, PRP), (am, VBP), (delighted, JJ), (to, T...\n",
       "18    [[(Thank, VB), (you, PRP), (,, ,), (Colonel, N...\n",
       "19    [[(Good, JJ), (Morning, NN), (., .)], [(Mr., N...\n",
       "20    [[(I, PRP), (want, VBP), (to, TO), (thank, VB)...\n",
       "21    [[(Thank, VB), (you, PRP), (for, IN), (your, P...\n",
       "22    [[(MS., NNP), (FULTON, NNP), (:, :), (All, DT)...\n",
       "23    [[(MR., NNP), (PAINTER, NNP), (:, :), (Thanks,...\n",
       "24    [[(MODERATOR, NN), (:, :), (Thank, VB), (you, ...\n",
       "25    [[(Well, RB), (,, ,), (thank, VB), (you, PRP),...\n",
       "Name: POS-sents, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOSSpeechesDF['POS-sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cyber', 265),\n",
       " ('cyberspace', 202),\n",
       " ('law', 145),\n",
       " ('security', 123),\n",
       " ('world', 84),\n",
       " ('policy', 83),\n",
       " ('information', 82),\n",
       " ('internet', 73),\n",
       " ('', 71),\n",
       " ('behavior', 69),\n",
       " ('space', 66),\n",
       " ('today', 64),\n",
       " ('cooperation', 63),\n",
       " ('state', 63),\n",
       " ('stability', 63),\n",
       " ('capacity', 56),\n",
       " ('technology', 52),\n",
       " ('Internet', 51),\n",
       " ('work', 49),\n",
       " ('consensus', 48),\n",
       " ('development', 48),\n",
       " ('time', 46),\n",
       " ('role', 45),\n",
       " ('building', 45),\n",
       " ('cybersecurity', 45)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in DOSSpeechesDF['POS-sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 166),\n",
       " ('do', 52),\n",
       " ('build', 49),\n",
       " ('promote', 46),\n",
       " ('continue', 45),\n",
       " ('have', 39),\n",
       " ('work', 34),\n",
       " ('take', 34),\n",
       " ('make', 34),\n",
       " ('ensure', 33),\n",
       " ('Thank', 30),\n",
       " ('address', 29),\n",
       " ('engage', 26),\n",
       " ('help', 22),\n",
       " ('strengthen', 20),\n",
       " ('thank', 19),\n",
       " ('protect', 18),\n",
       " ('bring', 18),\n",
       " ('achieve', 18),\n",
       " ('apply', 18),\n",
       " ('play', 17),\n",
       " ('develop', 17),\n",
       " ('reduce', 17),\n",
       " ('see', 16),\n",
       " ('counter', 15)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in DOSSpeechesDF['POS-sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'secure', 'international', 'military', 'reliable', 'accessible', 'key', 'stable', 'mainstream', 'peaceful', 'domestic'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'cyberspace'\n",
    "NResults = set()\n",
    "for entry in DOSSpeechesDF['POS-sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My BA research is concerned with the changing discourse of cyber in government speeches. I will focusing on the concept of cyberspace. These findings, so far, are interesting in thinking how government officials are characterizing the ambiguous entity of cyberspace. The adjectives that stand out to me are 'accessible' 'mainstream' and both domestic and international. Given its ambiguous nature, I want to answer whether or not there is an attempt to territorialize cyberspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sophisticated', 'global', 'specific', 'sub-regional', 'first', 'states/International', 'joint', 'foreign', 'particular', 'international', 'recent', 'transnational', 'whole-of-government', 'other', 'specialized', 'major', 'malicious', 'regional', 'voluntary', 'practical', 'complicated', 'new', 'future', 'serious', 'bilateral', 'hostile', 'large-scale', 'potential', 'cross-cutting', 'coercive', 'reliable', 'big', 'fundamental', 'significant', 'offensive'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'cyber'\n",
    "NResults = set()\n",
    "for entry in DOSSpeechesDF['POS-sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at cyber more broadly, a few of the associated adjectives stand out. Specifically, adjectives such as \"new\" and \"first\" highlight the novelty of the phenomenon, possibly suggesting an attempt to better define the concept. I think it's interesting that offensive comes up, while defensive does not - this indicates that government officials are strategically thinking about cyber as an offensive tool. The international community is definately an important component of cyber, given the appearance of international and foreign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chain', 'computer', 'document', 'network', 'cyber', 'space', 'aviation'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'NN'\n",
    "Word = 'security'\n",
    "NResults = set()\n",
    "for entry in DOSSpeechesDF['POS-sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thinking more broadly about cyber's relation to security, I find it intersting that security is being discussed in relation to not only cyber, but also aviation, computer, and space. These nouns may provide further insights into the conceptualization of cyber in relation to them. I also find it interesting that aviation is present, given the ambiguity of outer space, which has similaritites to the ambiguity of cyberspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'international', 'regional', 'national', 'hemispheric', 'economic', 'private-sector', 'trilateral', 'multinational', 'shared', 'cyber', 'new', 'knowledgeable'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'security'\n",
    "NResults = set()\n",
    "for entry in DOSSpeechesDF['POS-sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hints at the various layers, and inevitably, actors within the broader field of security. However, I think that it is important to note the presence of cyber and new, given my project's focus on this phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is also a classification task, which identifies named objects. Included with Stanford NER are a 4 class model trained on the CoNLL 2003 eng.train, a 7 class model trained on the MUC 6 and MUC 7 training data sets, and a 3 class model trained on both data sets plus some additional data (including ACE 2002 and limited data in-house) on the intersection of those class sets. \n",
    "\n",
    "**3 class**:\tLocation, Person, Organization\n",
    "\n",
    "**4 class**:\tLocation, Person, Organization, Misc\n",
    "\n",
    "**7 class**:\tLocation, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "These models each use distributional similarity features, which provide some performance gain at the cost of increasing their size and runtime. Also available are the same models missing those features.\n",
    "\n",
    "(We note that the training data for the 3 class model does not include any material from the CoNLL eng.testa or eng.testb data sets, nor any of the MUC 6 or 7 test or devtest datasets, nor Alan Ritter's Twitter NER data, so all of these would be valid tests of its performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tag our first set of exemplary sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'O'), ('saw', 'O'), ('the', 'O'), ('elephant', 'O'), ('in', 'O'), ('my', 'O'), ('pajamas', 'O'), ('.', 'O')], [('The', 'O'), ('quick', 'O'), ('brown', 'O'), ('fox', 'O'), ('jumped', 'O'), ('over', 'O'), ('the', 'O'), ('lazy', 'O'), ('dog', 'O'), ('.', 'O')], [('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')], [('Trayvon', 'PERSON'), ('Benjamin', 'PERSON'), ('Martin', 'PERSON'), ('was', 'O'), ('an', 'O'), ('African', 'O'), ('American', 'O'), ('from', 'O'), ('Miami', 'LOCATION'), ('Gardens', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), (',', 'O'), ('who', 'O'), (',', 'O'), ('at', 'O'), ('17', 'O'), ('years', 'O'), ('old', 'O'), (',', 'O'), ('was', 'O'), ('fatally', 'O'), ('shot', 'O'), ('by', 'O'), ('George', 'PERSON'), ('Zimmerman', 'PERSON'), (',', 'O'), ('a', 'O'), ('neighborhood', 'O'), ('watch', 'O'), ('volunteer', 'O'), (',', 'O'), ('in', 'O'), ('Sanford', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), ('.', 'O')], [('Buffalo', 'LOCATION'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O'), ('buffalo', 'O'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "classified_sents = stanford.nerTagger.tag_sents(tokenized_text)\n",
    "print(classified_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run NER over our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, O), (year, O), (,, O), (Help, O), (De...\n",
       "8    [[(First, O), (post, O), (in, O), (quite, O), ...\n",
       "7    [[([, O), (Original, O), (Post, O), (], O), ((...\n",
       "6    [[(I, O), (witnessed, O), (this, O), (astoundi...\n",
       "5    [[(I, O), (work, O), (Helpdesk, ORGANIZATION),...\n",
       "4    [[(This, O), (just, O), (happened, O), (..., O...\n",
       "3    [[(Another, O), (tale, O), (from, O), (the, O)...\n",
       "2    [[([, O), (Part, O), (1, O), (], O), ((, O), (...\n",
       "1    [[(>, O), ($, O), (Me, O), (-, O), (Hello, O),...\n",
       "0    [[(So, O), (my, O), (story, O), (starts, O), (...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 401),\n",
       " ('I', 245),\n",
       " ('the', 226),\n",
       " (',', 205),\n",
       " ('to', 197),\n",
       " ('a', 143),\n",
       " ('and', 135),\n",
       " ('>', 106),\n",
       " ('you', 102),\n",
       " ('of', 97)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'Desk',\n",
       " 'busy',\n",
       " 'fix',\n",
       " 'received',\n",
       " 'couple',\n",
       " 'Windows',\n",
       " 'anymore',\n",
       " 'Sure',\n",
       " 'error',\n",
       " 'DVD',\n",
       " 'opened',\n",
       " 'There',\n",
       " 'upside',\n",
       " 'local',\n",
       " 'bane',\n",
       " 'existence',\n",
       " 'learn',\n",
       " 'sometimes',\n",
       " 'generic',\n",
       " 'Everyone',\n",
       " 'login',\n",
       " 'times',\n",
       " 'guy',\n",
       " 'asset',\n",
       " 'name',\n",
       " 'Computer',\n",
       " 'nothing',\n",
       " \"'P4ssword\",\n",
       " 'P',\n",
       " 'Everything',\n",
       " 'case',\n",
       " '*type',\n",
       " 'S',\n",
       " 'LOWERCASE',\n",
       " 'used',\n",
       " 'four',\n",
       " 'Original',\n",
       " 'cancer',\n",
       " 'month',\n",
       " 'live',\n",
       " 'brave',\n",
       " 'bitter',\n",
       " 'passed',\n",
       " 'ago',\n",
       " 'absolutely',\n",
       " 'ready',\n",
       " 'proud',\n",
       " 'above',\n",
       " 'completely',\n",
       " 'its',\n",
       " 'meant',\n",
       " 'both',\n",
       " 'sharing',\n",
       " 'making',\n",
       " '100',\n",
       " 'share',\n",
       " 'looking',\n",
       " 'ALL',\n",
       " 'whom',\n",
       " 'business',\n",
       " 'whose',\n",
       " 'stronger',\n",
       " 'bad',\n",
       " 'mess',\n",
       " 'turn',\n",
       " 'first',\n",
       " 'others',\n",
       " 'Here',\n",
       " 'suggested',\n",
       " 'videos',\n",
       " 'While',\n",
       " 'stand',\n",
       " 'certain',\n",
       " 'enjoy',\n",
       " 'well',\n",
       " 'drowned',\n",
       " 'soon',\n",
       " 'understand',\n",
       " 'risks',\n",
       " 'myself',\n",
       " 'point',\n",
       " 'future',\n",
       " 'avoid',\n",
       " 'thinking',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'site',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'discover',\n",
       " 'order',\n",
       " '5',\n",
       " 'slightly',\n",
       " 'spent',\n",
       " 'moment',\n",
       " 'arms',\n",
       " 'idea',\n",
       " 'food',\n",
       " 'party',\n",
       " 'played',\n",
       " 'family',\n",
       " 'allowed',\n",
       " 'cry',\n",
       " 'pretty',\n",
       " 'nice',\n",
       " 'loved',\n",
       " 'mind',\n",
       " 'favor',\n",
       " 'watched',\n",
       " 'Things',\n",
       " '17',\n",
       " 'small',\n",
       " 'lived',\n",
       " 'living',\n",
       " 'themselves',\n",
       " 'potential',\n",
       " 'happiness',\n",
       " 'sound',\n",
       " 'situation',\n",
       " 'believe',\n",
       " 'mistakes',\n",
       " 'same',\n",
       " 'scenario',\n",
       " 'difference',\n",
       " 'glad',\n",
       " 'flaws',\n",
       " 'stupid',\n",
       " 'yourself',\n",
       " 'ok',\n",
       " 'In',\n",
       " 'comments',\n",
       " 'SO',\n",
       " 'random',\n",
       " 'request',\n",
       " 'Give',\n",
       " 'THIS',\n",
       " 'response',\n",
       " 'Thanks',\n",
       " 'tears',\n",
       " 'helped',\n",
       " 'reply',\n",
       " 'large',\n",
       " 'academic',\n",
       " 'organization',\n",
       " 'CEO',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'mailboxes',\n",
       " 'Fail',\n",
       " '#',\n",
       " 'check',\n",
       " 'generate',\n",
       " '=',\n",
       " 'real',\n",
       " 'stopped',\n",
       " 'avalanche',\n",
       " 'died',\n",
       " 'systems',\n",
       " 'staff',\n",
       " 'brought',\n",
       " 'retail',\n",
       " 'store',\n",
       " 'plugged',\n",
       " 'hear',\n",
       " 'occasionally',\n",
       " 'operate',\n",
       " 'drawer*',\n",
       " 'try',\n",
       " 'hit',\n",
       " '*I',\n",
       " 'echo',\n",
       " 'heard',\n",
       " 'seconds',\n",
       " 'nose',\n",
       " 'working',\n",
       " 'BING',\n",
       " 'THE',\n",
       " '*Note',\n",
       " 'yes',\n",
       " 'different',\n",
       " 'search',\n",
       " 'immediately',\n",
       " 'connection',\n",
       " 'Turns',\n",
       " 'shortcut',\n",
       " 'okay',\n",
       " 'computering',\n",
       " 'taking',\n",
       " 'Steve',\n",
       " 'XYZ',\n",
       " 'however',\n",
       " 'forwarded',\n",
       " 'using',\n",
       " 'meet',\n",
       " 'ran',\n",
       " 'mouse',\n",
       " 'closed',\n",
       " 'window',\n",
       " 'revealing',\n",
       " 'me*',\n",
       " 'shaking',\n",
       " 'lunch',\n",
       " 'yelled',\n",
       " 'needed',\n",
       " 'key',\n",
       " 'week',\n",
       " 'pointed',\n",
       " 'door',\n",
       " 'blinked',\n",
       " 'Then',\n",
       " 'command',\n",
       " 'three',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'person',\n",
       " 'Whatever',\n",
       " 'um',\n",
       " 'wrong',\n",
       " 'mother',\n",
       " 'done',\n",
       " 'way',\n",
       " 'weeks',\n",
       " 'since',\n",
       " 'stuff',\n",
       " 'took',\n",
       " 'Wow',\n",
       " 'gildings',\n",
       " 'One',\n",
       " 'HR',\n",
       " 'select',\n",
       " '*Are',\n",
       " 'Ca',\n",
       " 'building',\n",
       " 'holiday',\n",
       " 'pay',\n",
       " 'gods',\n",
       " 'expire',\n",
       " '60',\n",
       " 'anyway',\n",
       " 'User',\n",
       " 'DeskMugPhonePencil1',\n",
       " 'Thursday',\n",
       " 'return',\n",
       " 'calls',\n",
       " 'often',\n",
       " 'issues',\n",
       " 'service',\n",
       " 'older',\n",
       " 'types',\n",
       " 'speak',\n",
       " 'box',\n",
       " 'properly',\n",
       " 'personally',\n",
       " 'ask',\n",
       " 'supervisor',\n",
       " 'earlier',\n",
       " 'visit',\n",
       " 'terrible',\n",
       " 'willing',\n",
       " 'nasty']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 17),\n",
       " ('Google', 6),\n",
       " ('Smith', 5),\n",
       " ('Steve', 2),\n",
       " ('Citrix', 1),\n",
       " ('Nono', 1),\n",
       " ('Reddit', 1),\n",
       " ('Helpdesk', 1),\n",
       " ('UK', 1),\n",
       " ('CMD', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 6), ('Citrix', 1), ('Helpdesk', 1), ('CMD', 1), ('GOOGLE', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, have much smaller counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed (using your own hand-codings as \"ground truth\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOSSpeechesDF['classified-sents'] = DOSSpeechesDF['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[(I, O), (would, O), (like, O), (to, O), (beg...\n",
       "1     [[(Good, O), (morning, O), (!, O)], [(Thank, O...\n",
       "2     [[(Introduction, O), (., O)], [(Chairman, O), ...\n",
       "3     [[(Chairman, O), (Donilon, PERSON), (,, O), (V...\n",
       "4     [[(So, O), (what, O), (do, O), (zombies, O), (...\n",
       "5     [[(Chairman, O), (Gardner, PERSON), (,, O), (R...\n",
       "6     [[(Good, O), (afternoon, O), (,, O), (and, O),...\n",
       "7     [[(Background, O), (:, O), (The, O), (Sixteent...\n",
       "8     [[(Good, O), (afternoon, O), (., O)], [(IÍm, O...\n",
       "9     [[(Thank, O), (you, O), (for, O), (the, O), (o...\n",
       "10    [[(Great, O), (., O)], [(Well, O), (,, O), (th...\n",
       "11    [[(ASSISTANT, O), (ATTORNEY, O), (GENERAL, O),...\n",
       "12    [[(Mr., O), (Chairman, O), (,, O), (Ranking, O...\n",
       "13    [[(ItÍs, O), (a, O), (pleasure, O), (to, O), (...\n",
       "14    [[(MODERATOR, O), (:, O), (Good, O), (afternoo...\n",
       "15    [[(Good, O), (morning, O), (,, O), (Minister, ...\n",
       "16    [[(MODERATOR, O), (:, O), (Good, O), (afternoo...\n",
       "17    [[(I, O), (am, O), (delighted, O), (to, O), (b...\n",
       "18    [[(Thank, O), (you, O), (,, O), (Colonel, O), ...\n",
       "19    [[(Good, O), (Morning, O), (., O)], [(Mr., O),...\n",
       "20    [[(I, O), (want, O), (to, O), (thank, O), (the...\n",
       "21    [[(Thank, O), (you, O), (for, O), (your, O), (...\n",
       "22    [[(MS., O), (FULTON, O), (:, O), (All, O), (ri...\n",
       "23    [[(MR., O), (PAINTER, O), (:, O), (Thanks, O),...\n",
       "24    [[(MODERATOR, O), (:, O), (Thank, O), (you, O)...\n",
       "25    [[(Well, O), (,, O), (thank, O), (you, O), (ve...\n",
       "Name: classified-sents, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOSSpeechesDF['classified-sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2460),\n",
       " ('the', 2280),\n",
       " ('and', 1771),\n",
       " ('.', 1709),\n",
       " ('to', 1695),\n",
       " ('of', 1553),\n",
       " ('in', 886),\n",
       " ('that', 759),\n",
       " ('a', 721),\n",
       " ('is', 469)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in DOSSpeechesDF['classified-sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTC',\n",
       " 'sessions',\n",
       " 'Terrorist',\n",
       " 'Iraq',\n",
       " 'Syria',\n",
       " 'expressed',\n",
       " 'offensive',\n",
       " 'techniques',\n",
       " 'hide',\n",
       " 'welfare',\n",
       " 'fail',\n",
       " 'aggressively',\n",
       " 'Constitution',\n",
       " 'violates',\n",
       " 'solicitation',\n",
       " 'decided',\n",
       " 'financing',\n",
       " 'planning',\n",
       " 'investigating',\n",
       " 'disrupting',\n",
       " 'academia',\n",
       " 'crucial',\n",
       " 'defining',\n",
       " 'extremist',\n",
       " 'prohibit',\n",
       " 'posting',\n",
       " 'suspend',\n",
       " 'reported',\n",
       " 'violating',\n",
       " 'TwitterÍs',\n",
       " 'middle',\n",
       " 'shift',\n",
       " 'type',\n",
       " 'discourage',\n",
       " 'suppression',\n",
       " 'extremistsÍ',\n",
       " 'contesting',\n",
       " 'worldviews',\n",
       " 'competing',\n",
       " 'messengers',\n",
       " 'voices',\n",
       " 'former',\n",
       " 'families',\n",
       " 'inform',\n",
       " 'recruitment',\n",
       " 'drivers',\n",
       " 'careful',\n",
       " 'association',\n",
       " 'handed',\n",
       " 'rhetoric',\n",
       " 'problematic',\n",
       " 'undermining',\n",
       " 'demonstrated',\n",
       " 'followers',\n",
       " 'interaction',\n",
       " 'ideologies',\n",
       " 'suppress',\n",
       " 'adapt',\n",
       " 'relying',\n",
       " 'Finally',\n",
       " 'countering',\n",
       " 'dissent',\n",
       " 'mind',\n",
       " 'profile',\n",
       " 'inviting',\n",
       " 'inaugural',\n",
       " 'worth',\n",
       " 'anniversary',\n",
       " 'city',\n",
       " 'Association',\n",
       " 'witness',\n",
       " 'advances',\n",
       " 'underscore',\n",
       " 'Digital',\n",
       " 'placed',\n",
       " 'doesnÍt',\n",
       " 'staff',\n",
       " 'nationÍs',\n",
       " 'cutting',\n",
       " 'allows',\n",
       " 'revolution',\n",
       " 'living',\n",
       " 'literally',\n",
       " 'kinds',\n",
       " 'young',\n",
       " 'state-sponsored',\n",
       " 'single',\n",
       " 'medical',\n",
       " 'approved',\n",
       " 'Presidential',\n",
       " 'Directive',\n",
       " '41',\n",
       " 'Incident',\n",
       " 'governs',\n",
       " 'assists',\n",
       " 'pillar',\n",
       " 'Pacific',\n",
       " 'political-military',\n",
       " 'establishing',\n",
       " 'date',\n",
       " 'Stop',\n",
       " 'Connect',\n",
       " 'difference',\n",
       " 'Gardner',\n",
       " 'Cardin',\n",
       " 'Members',\n",
       " 'Challenge',\n",
       " 'tests',\n",
       " 'repeatedly',\n",
       " 'frequent',\n",
       " 'delivered',\n",
       " 'announcement',\n",
       " 'deployment',\n",
       " 'Significant',\n",
       " 'funded',\n",
       " 'well-being',\n",
       " 'Comprehensive',\n",
       " '2005',\n",
       " 'grounded',\n",
       " 'negotiating',\n",
       " 'table',\n",
       " 'complete',\n",
       " 'verifiable',\n",
       " 'posture',\n",
       " 'publicly',\n",
       " 'Alliance',\n",
       " 'sustained',\n",
       " 'fund',\n",
       " 'reputation',\n",
       " 'rigorous',\n",
       " 'Resolution',\n",
       " 'visits',\n",
       " 'sever',\n",
       " 'D.P.R.K.Ís',\n",
       " 'caused',\n",
       " 'continually',\n",
       " 'examining',\n",
       " 'relative',\n",
       " 'Iran',\n",
       " 'planet',\n",
       " 'solely',\n",
       " 'conjunction',\n",
       " 'universally',\n",
       " 'imposed',\n",
       " 'mandatory',\n",
       " 'requirement',\n",
       " 'CouncilÍs',\n",
       " 'outreach',\n",
       " 'gaps',\n",
       " 'U.S',\n",
       " 'wake',\n",
       " 'operate',\n",
       " 'Enhancement',\n",
       " 'vigorously',\n",
       " '15',\n",
       " 'EO',\n",
       " '13722',\n",
       " '12',\n",
       " 'additions',\n",
       " 'SDN',\n",
       " 'designates',\n",
       " 'publishes',\n",
       " 'mass',\n",
       " 'leverage',\n",
       " 'Company',\n",
       " 'shut',\n",
       " 'reduced',\n",
       " 'Several',\n",
       " 'visa',\n",
       " 'restrictions',\n",
       " 'source',\n",
       " 'currency',\n",
       " 'workers',\n",
       " 'Challenges',\n",
       " 'exports',\n",
       " 'generate',\n",
       " 'children',\n",
       " 'satisfied',\n",
       " 'says',\n",
       " 'press',\n",
       " 'crisis',\n",
       " 'away',\n",
       " 'unilateral',\n",
       " 'enforce',\n",
       " 'price',\n",
       " 'Enhancing',\n",
       " 'collective',\n",
       " 'consult',\n",
       " 'reinforce',\n",
       " 'Developments',\n",
       " 'Field',\n",
       " 'Telecommunications',\n",
       " 'Context',\n",
       " 'expert-level',\n",
       " 'captured',\n",
       " 'settings',\n",
       " 'Turkey',\n",
       " 'Perhaps',\n",
       " 'JinpingÍs',\n",
       " 'inter',\n",
       " 'alia',\n",
       " 'competitive',\n",
       " 'sectors.î',\n",
       " 'Capacity',\n",
       " 'African',\n",
       " 'Symantec',\n",
       " 'delivery',\n",
       " 'counterterrorism',\n",
       " 'investigators',\n",
       " 'handling',\n",
       " 'Responding',\n",
       " 'Over',\n",
       " 'high-profile',\n",
       " 'cyberattacks',\n",
       " 'corresponding',\n",
       " 'frameworks',\n",
       " 'maturity',\n",
       " 'CommissionÍs',\n",
       " 'oversight',\n",
       " 'bad',\n",
       " 'guys',\n",
       " 'banks',\n",
       " 'asking',\n",
       " 'Let',\n",
       " 'got',\n",
       " 'wild',\n",
       " 'diplomat',\n",
       " 'Forbin',\n",
       " 'arsenal',\n",
       " 'perfect',\n",
       " 'fifth',\n",
       " 'warfare',\n",
       " 'hopeful',\n",
       " 'terribly',\n",
       " 'Atari',\n",
       " '800',\n",
       " 'though',\n",
       " 'Project',\n",
       " 'Where',\n",
       " 'nation-states',\n",
       " 'separate',\n",
       " 'grew',\n",
       " 'Second',\n",
       " 'Certainly',\n",
       " 'voice',\n",
       " 'wall',\n",
       " 'Third',\n",
       " 'identity',\n",
       " 'fear',\n",
       " 'prospect',\n",
       " 'stay',\n",
       " 'heard',\n",
       " 'coalitions',\n",
       " 'solve',\n",
       " 'agreements',\n",
       " 'job',\n",
       " 'frank',\n",
       " 'stakes',\n",
       " 'cyberspace.î',\n",
       " 'existence',\n",
       " 'linked',\n",
       " 'fragmented',\n",
       " 'altogether',\n",
       " 'Departments',\n",
       " 'dynamic',\n",
       " 'specifically',\n",
       " 'discussing',\n",
       " '2014-2015',\n",
       " 'conflictÍs',\n",
       " 'humanity',\n",
       " 'Norms',\n",
       " 'observed',\n",
       " 'self-restraint',\n",
       " 'reportÍs',\n",
       " 'recommendation',\n",
       " 'proposed',\n",
       " 'audience',\n",
       " 'advancing',\n",
       " 'reflect',\n",
       " 'territories',\n",
       " 'hosted',\n",
       " 'affirm',\n",
       " 'Dialogue',\n",
       " 'scheduled',\n",
       " 'communique',\n",
       " 'Among',\n",
       " 'ñno',\n",
       " 'represents',\n",
       " 'remarkable',\n",
       " '57',\n",
       " 'announced',\n",
       " 'stateÍs',\n",
       " 'consultation',\n",
       " 'focusing',\n",
       " 'integration',\n",
       " 'serve',\n",
       " 'governmental',\n",
       " 'extensively',\n",
       " 'delegations',\n",
       " 'previously',\n",
       " 'drives',\n",
       " 'external',\n",
       " 'expansive',\n",
       " 'ñcyber',\n",
       " 'ñinformation',\n",
       " 'authority',\n",
       " 'Code',\n",
       " 'non-binding',\n",
       " 'Preventing',\n",
       " 'sophistication',\n",
       " 'IT',\n",
       " 'unauthorized',\n",
       " 'hundreds',\n",
       " 'disruption',\n",
       " 'perceived',\n",
       " 'reporting',\n",
       " 'incurred',\n",
       " 'motivation',\n",
       " 'accomplished',\n",
       " 'combination',\n",
       " 'ñdeterrence',\n",
       " 'deny',\n",
       " 'carrying',\n",
       " 'inflict',\n",
       " 'one-size-fits-all',\n",
       " 'denial',\n",
       " 'resiliency',\n",
       " 'imposition',\n",
       " 'term',\n",
       " 'provocative',\n",
       " 'authorizes',\n",
       " 'employing',\n",
       " 'forces',\n",
       " 'attributing',\n",
       " '85',\n",
       " 'courses',\n",
       " 'Looking',\n",
       " 'devices',\n",
       " 'gathering',\n",
       " 'professionals',\n",
       " 'knowledgeable',\n",
       " 'RSA',\n",
       " 'belief',\n",
       " 'technological',\n",
       " 'forging',\n",
       " 'lasting',\n",
       " 'humans',\n",
       " 'securing',\n",
       " 'exciting',\n",
       " 'transformation',\n",
       " 'efficient',\n",
       " 'thieves',\n",
       " 'reminded',\n",
       " 'ideals',\n",
       " 'fronts',\n",
       " 'designations',\n",
       " 'FBIÍs',\n",
       " 'clock',\n",
       " 'Investigative',\n",
       " 'dangers',\n",
       " 'Each',\n",
       " 'corporations',\n",
       " 'Criminal',\n",
       " 'formed',\n",
       " 'experienced',\n",
       " 'trafficking',\n",
       " 'edge',\n",
       " 'UnionÍs',\n",
       " 'coordinating',\n",
       " 'market',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'fake',\n",
       " 'Darkode',\n",
       " 'sold',\n",
       " 'software',\n",
       " 'botnets',\n",
       " 'intrusion',\n",
       " 'search',\n",
       " 'root',\n",
       " 'Beyond',\n",
       " 'Recently',\n",
       " 'conflicting',\n",
       " 'prevents',\n",
       " 'puts',\n",
       " 'refuse',\n",
       " 'qualify',\n",
       " 'release',\n",
       " 'closest',\n",
       " 'adequately',\n",
       " 'obtain',\n",
       " 'arrangement',\n",
       " 'standing',\n",
       " 'tech',\n",
       " 'takedown',\n",
       " 'demonstrate',\n",
       " 'history',\n",
       " 'did',\n",
       " 'consultations',\n",
       " 'whenever',\n",
       " 'pressing',\n",
       " 'endeavor',\n",
       " 'illusions',\n",
       " 'surely',\n",
       " 'learn',\n",
       " 'prosperous',\n",
       " 'benefited',\n",
       " 'funding',\n",
       " 'expert',\n",
       " 'legislative',\n",
       " 'exploitation',\n",
       " 'cyber-',\n",
       " 'interfering',\n",
       " 'ñlikelihood',\n",
       " 'sourcesî',\n",
       " 'ñimpose',\n",
       " 'security.î',\n",
       " 'caretakers',\n",
       " 'multi',\n",
       " 's',\n",
       " 'ñvalue-addî',\n",
       " 'well-established',\n",
       " 'extends',\n",
       " 'realized',\n",
       " 'Computer',\n",
       " 'Response',\n",
       " 'Teams',\n",
       " 'staying',\n",
       " 'hosting',\n",
       " '2004',\n",
       " 'confronting',\n",
       " 'immense',\n",
       " 'possibility',\n",
       " 'viewed',\n",
       " 'difficulty',\n",
       " 'notes',\n",
       " 'policymakers',\n",
       " 'normal',\n",
       " 'predictable',\n",
       " 'Otherwise',\n",
       " 'reactions',\n",
       " 'misattribution',\n",
       " 'coupled',\n",
       " 'achieving',\n",
       " 'progressive',\n",
       " 'feel',\n",
       " 'warrant',\n",
       " 'vilify',\n",
       " 'genocide',\n",
       " 'divide',\n",
       " 'resistance',\n",
       " 'kill',\n",
       " 'comparable',\n",
       " 'utilizing',\n",
       " 'enemy',\n",
       " 'impasse',\n",
       " 'serving',\n",
       " 'instrument',\n",
       " 'unfettered',\n",
       " 'overreactions',\n",
       " 'overreaction',\n",
       " 'surveillance',\n",
       " 'monitoring',\n",
       " 'terrorism-related',\n",
       " 'request',\n",
       " 'platformsÍ',\n",
       " 'prohibited',\n",
       " 'harassment',\n",
       " 'usually',\n",
       " 'identification',\n",
       " 'child',\n",
       " 'pornography',\n",
       " 'filtering',\n",
       " 'tweets',\n",
       " 'theyÍve',\n",
       " 'tweet',\n",
       " 'checking',\n",
       " 'audio',\n",
       " 'video',\n",
       " 'obstacle',\n",
       " 'app',\n",
       " 'apps',\n",
       " 'extent',\n",
       " 'occurring',\n",
       " 'anyone',\n",
       " 'elaborate',\n",
       " 'respective',\n",
       " 'briefly',\n",
       " 'Role',\n",
       " 'norm',\n",
       " 'servers',\n",
       " 'easily',\n",
       " 'Asia-Pacific',\n",
       " 'Communications',\n",
       " 'aim',\n",
       " 'incredibly',\n",
       " 'depends',\n",
       " 'self-assessment',\n",
       " 'An',\n",
       " 'formal',\n",
       " 'recovery',\n",
       " 'upcoming',\n",
       " 'Poland',\n",
       " 'coast',\n",
       " 'todayÍs',\n",
       " 'tied',\n",
       " 'IÍll',\n",
       " 'presence',\n",
       " 'Central',\n",
       " 'sorry',\n",
       " 'improvements',\n",
       " 'compatible',\n",
       " 'requirements',\n",
       " 'ASSISTANT',\n",
       " 'announce',\n",
       " 'alleged',\n",
       " '500,000',\n",
       " 'losses',\n",
       " 'inactive',\n",
       " 'enterprise',\n",
       " 'grows',\n",
       " 'shrink',\n",
       " 'importantly',\n",
       " 'Leslie',\n",
       " 'legislation',\n",
       " 'TOC',\n",
       " 'ladies',\n",
       " 'finance',\n",
       " '88',\n",
       " 'announcing',\n",
       " 'innocent',\n",
       " 'somewhere',\n",
       " 'gained',\n",
       " 'Joe',\n",
       " 'division',\n",
       " 'offering',\n",
       " 'Ms.',\n",
       " 'dollars',\n",
       " 'scheme',\n",
       " 'affected',\n",
       " 'design',\n",
       " 'uncovered',\n",
       " 'fraudulent',\n",
       " 'transfer',\n",
       " 'initiated',\n",
       " 'Switzerland',\n",
       " 'employee',\n",
       " 'dairy',\n",
       " 'fell',\n",
       " 'phishing',\n",
       " 'subsequently',\n",
       " 'suffered',\n",
       " 'disguised',\n",
       " 'Upon',\n",
       " 'Both',\n",
       " 'infections',\n",
       " 'almost',\n",
       " 'Fox-IT',\n",
       " 'More',\n",
       " 'High-Tech',\n",
       " 'Unit',\n",
       " 'ministry',\n",
       " 'reside',\n",
       " 'Obviously',\n",
       " 'countless',\n",
       " 'ph',\n",
       " 'Michael',\n",
       " 'officers',\n",
       " 'ambassador',\n",
       " 'demonstration',\n",
       " 'defeat',\n",
       " 'threaten',\n",
       " 'court',\n",
       " 'judge',\n",
       " 'subsequent',\n",
       " 'choosing',\n",
       " 'designated',\n",
       " 'board',\n",
       " 'sharpen',\n",
       " 'abandon',\n",
       " 'Sadly',\n",
       " 'Its',\n",
       " 'fourth',\n",
       " 'systematic',\n",
       " 'documented',\n",
       " 'Inquiry',\n",
       " 'Diplomacy',\n",
       " 'improved',\n",
       " 'door',\n",
       " 'Unfortunately',\n",
       " 'responded',\n",
       " 'ñdeadî',\n",
       " 'ensured',\n",
       " 'unity',\n",
       " 'accepted',\n",
       " 'Instead',\n",
       " 'ROK',\n",
       " 'trip',\n",
       " 'expect',\n",
       " 'relations',\n",
       " 'Similarly',\n",
       " 'comes',\n",
       " 'prominently',\n",
       " 'aggression',\n",
       " 'invited',\n",
       " 'tough',\n",
       " 'until',\n",
       " 'cease',\n",
       " 'successfully',\n",
       " 'revenues',\n",
       " 'funnel',\n",
       " 'proportionate',\n",
       " 'misbehavior',\n",
       " 'representatives',\n",
       " 'eyes',\n",
       " 'designate',\n",
       " 'sectors',\n",
       " 'left',\n",
       " 'Chiefs',\n",
       " 'maintaining',\n",
       " 'honor',\n",
       " 'By',\n",
       " 'commits',\n",
       " 'fortunate',\n",
       " 'thoughtful',\n",
       " 'celebrated',\n",
       " 'constituted',\n",
       " 'articulates',\n",
       " 'peoples',\n",
       " 'sovereign',\n",
       " 'disclosures',\n",
       " 'changes',\n",
       " 'acceptable',\n",
       " 'state-on-state',\n",
       " 'multinational',\n",
       " 'front',\n",
       " 'leaving',\n",
       " 'non-governmental',\n",
       " 'Any',\n",
       " 'involving',\n",
       " 'total',\n",
       " 'top-down',\n",
       " 'written',\n",
       " 'billions',\n",
       " 'accession',\n",
       " 'Colombia',\n",
       " 'trainings',\n",
       " 'bridge',\n",
       " 'articulated',\n",
       " 'signing',\n",
       " 'project',\n",
       " 'experience',\n",
       " 'Multi-stakeholder',\n",
       " 'sub-themes',\n",
       " 'added',\n",
       " 'pertinent',\n",
       " 'complexity',\n",
       " 'regards',\n",
       " 'elsewhere',\n",
       " 'strive',\n",
       " '``',\n",
       " \"''\",\n",
       " 'contexts',\n",
       " 'high-level',\n",
       " 'capacity-building',\n",
       " 'accessible',\n",
       " 'subjects',\n",
       " 'debates',\n",
       " 'express',\n",
       " 'thoughts',\n",
       " 'sustainable',\n",
       " 'Press',\n",
       " 'floor',\n",
       " 'holiday',\n",
       " 'season',\n",
       " 'defraud',\n",
       " 'goods',\n",
       " 'presented',\n",
       " 'attacking',\n",
       " 'representing',\n",
       " 'Nike',\n",
       " 'Tiffany',\n",
       " 'sale',\n",
       " 'sides',\n",
       " 'Atlantic',\n",
       " 'seizures',\n",
       " '.eu',\n",
       " '.de',\n",
       " 'City',\n",
       " 'purses',\n",
       " 'trademark',\n",
       " 'copyright',\n",
       " 'Remember',\n",
       " 'purchases',\n",
       " 'numbers',\n",
       " 'quick',\n",
       " 'Were',\n",
       " 'instances',\n",
       " 'dupe',\n",
       " 'rip',\n",
       " 'finish',\n",
       " 'youÍll',\n",
       " 'holidays',\n",
       " 'Know',\n",
       " 'seller',\n",
       " 'Not',\n",
       " 'buy',\n",
       " 'deliver',\n",
       " 'harness',\n",
       " 'invitation',\n",
       " 'Everyone',\n",
       " 'accounting',\n",
       " 'struggled',\n",
       " 'deals',\n",
       " 'novel',\n",
       " 'entirely',\n",
       " 'Is',\n",
       " 'contemplates',\n",
       " 'commentators',\n",
       " 'looks',\n",
       " 'death',\n",
       " 'recognizing',\n",
       " 'bomb',\n",
       " 'exercising',\n",
       " 'imminent',\n",
       " 'Must',\n",
       " 'distinguish',\n",
       " 'significance',\n",
       " '8',\n",
       " 'legally',\n",
       " 'instructions',\n",
       " 'putatively',\n",
       " 'exercises',\n",
       " 'ten',\n",
       " 'produce',\n",
       " 'code',\n",
       " 'attackî',\n",
       " 'concerning',\n",
       " 'derived',\n",
       " 'contrary',\n",
       " 'bigger',\n",
       " 'pervasive',\n",
       " 'broadcasting',\n",
       " 'box',\n",
       " 'remarkably',\n",
       " 'anticipating',\n",
       " 'opinions',\n",
       " 'powerî',\n",
       " 'frees',\n",
       " 'legitimately',\n",
       " 'Compliance',\n",
       " 'parcel',\n",
       " 'constraint',\n",
       " 'restrict',\n",
       " 'earn',\n",
       " 'adherence',\n",
       " 'resonate',\n",
       " 'openly',\n",
       " 'repeat',\n",
       " 'smart',\n",
       " 'contributed',\n",
       " 'mobile',\n",
       " 'OASCICTE',\n",
       " 'Again',\n",
       " 'sorts',\n",
       " 'allowed',\n",
       " 'interactions',\n",
       " 'Caribbean',\n",
       " 'Mexico',\n",
       " 'Lima',\n",
       " 'telecommunications',\n",
       " 'telecommunication',\n",
       " 'recognizes',\n",
       " 'long-standing',\n",
       " 'safely',\n",
       " 'visions',\n",
       " 'promise',\n",
       " 'accelerate',\n",
       " 'sharpening',\n",
       " 'disputes',\n",
       " 'spoke',\n",
       " 'gave',\n",
       " 'disparate',\n",
       " 'man-made',\n",
       " 'congested',\n",
       " 'contested',\n",
       " 'Debris',\n",
       " 'Mitigation',\n",
       " 'pieces',\n",
       " 'minimize',\n",
       " 'U.N.',\n",
       " 'tracking',\n",
       " 'hazardous',\n",
       " 'notification',\n",
       " 'hazards',\n",
       " 'via',\n",
       " 'Infrastructure',\n",
       " 'Protection',\n",
       " 'spacecraft',\n",
       " 'environmental',\n",
       " 'avoidance',\n",
       " 'conducts',\n",
       " 'foundational',\n",
       " 'guidelines',\n",
       " 'ñCode',\n",
       " 'sweep',\n",
       " 'folks',\n",
       " 'internetÍs',\n",
       " 'SheÍs',\n",
       " 'PAINTER',\n",
       " 'pyramid',\n",
       " 'transformative',\n",
       " 'strands',\n",
       " 'upholding',\n",
       " 'happened',\n",
       " 'Painter',\n",
       " 'fashion',\n",
       " 'uniquely',\n",
       " 'destination',\n",
       " 'creates']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('United', 146),\n",
       " ('States', 138),\n",
       " ('U.S.', 99),\n",
       " ('Department', 92),\n",
       " ('State', 70),\n",
       " ('Korea', 70),\n",
       " ('of', 69),\n",
       " ('North', 59),\n",
       " ('UN', 49),\n",
       " ('China', 40),\n",
       " ('DPRK', 29),\n",
       " ('Security', 26),\n",
       " ('Council', 20),\n",
       " ('Cyberspace', 17),\n",
       " ('Russia', 17)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in DOSSpeechesDF['classified-sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Department', 92),\n",
       " ('State', 70),\n",
       " ('of', 64),\n",
       " ('UN', 49),\n",
       " ('DPRK', 29),\n",
       " ('Security', 26),\n",
       " ('Council', 20),\n",
       " ('United', 16),\n",
       " ('Nations', 15),\n",
       " ('Sony', 15),\n",
       " ('Congress', 14),\n",
       " ('National', 12),\n",
       " ('OAS', 12),\n",
       " ('International', 11),\n",
       " ('FBI', 11)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in DOSSpeechesDF['classified-sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bogachev', 15),\n",
       " ('Obama', 9),\n",
       " ('Kim', 8),\n",
       " ('Clinton', 8),\n",
       " ('Kerry', 6),\n",
       " ('Evgeniy', 5),\n",
       " ('John', 4),\n",
       " ('Morton', 4),\n",
       " ('Jong', 3),\n",
       " ('Un', 3),\n",
       " ('MR.', 3),\n",
       " ('Caldwell', 3),\n",
       " ('Paet', 3),\n",
       " ('KERRY', 3),\n",
       " ('Hague', 3)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PersonCounts = {}\n",
    "for entry in DOSSpeechesDF['classified-sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'PERSON':\n",
    "                continue\n",
    "            elif ent in PersonCounts:\n",
    "                PersonCounts[ent] += 1\n",
    "            else:\n",
    "                PersonCounts[ent] = 1\n",
    "sortedPerson = sorted(PersonCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedPerson[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('States', 134),\n",
       " ('United', 130),\n",
       " ('U.S.', 89),\n",
       " ('Korea', 69),\n",
       " ('North', 57),\n",
       " ('China', 40),\n",
       " ('Russia', 17),\n",
       " ('Japan', 15),\n",
       " ('Cyberspace', 13),\n",
       " ('South', 12),\n",
       " ('Seoul', 10),\n",
       " ('Pyongyang', 8),\n",
       " ('Europe', 8),\n",
       " ('Singapore', 7),\n",
       " ('Asia', 7)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LocCounts = {}\n",
    "for entry in DOSSpeechesDF['classified-sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'LOCATION':\n",
    "                continue\n",
    "            elif ent in LocCounts:\n",
    "                LocCounts[ent] += 1\n",
    "            else:\n",
    "                LocCounts[ent] = 1\n",
    "sortedLocs = sorted(LocCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedLocs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized-text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>POS-sents</th>\n",
       "      <th>classified-sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/1/16</td>\n",
       "      <td>Preventing the Exploitation of Information and...</td>\n",
       "      <td>I would like to begin by thanking the UN Count...</td>\n",
       "      <td>[I, would, like, to, begin, by, thanking, the,...</td>\n",
       "      <td>[[I, would, like, to, begin, by, thanking, the...</td>\n",
       "      <td>[[(I, PRP), (would, MD), (like, VB), (to, TO),...</td>\n",
       "      <td>[[(I, O), (would, O), (like, O), (to, O), (beg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/16</td>\n",
       "      <td>Keynote Address at the Singapore International...</td>\n",
       "      <td>Good morning! Thank you to Prime Minister Lee ...</td>\n",
       "      <td>[Good, morning, !, Thank, you, to, Prime, Mini...</td>\n",
       "      <td>[[Good, morning, !], [Thank, you, to, Prime, M...</td>\n",
       "      <td>[[(Good, JJ), (morning, NN), (!, .)], [(Thank,...</td>\n",
       "      <td>[[(Good, O), (morning, O), (!, O)], [(Thank, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/28/16</td>\n",
       "      <td>The Persistent Threat of North Korea and Devel...</td>\n",
       "      <td>Introduction. Chairman Gardner, Ranking Member...</td>\n",
       "      <td>[Introduction, ., Chairman, Gardner, ,, Rankin...</td>\n",
       "      <td>[[Introduction, .], [Chairman, Gardner, ,, Ran...</td>\n",
       "      <td>[[(Introduction, NN), (., .)], [(Chairman, NNP...</td>\n",
       "      <td>[[(Introduction, O), (., O)], [(Chairman, O), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/19/16</td>\n",
       "      <td>Statement Before the Presidential Commission o...</td>\n",
       "      <td>Chairman Donilon, Vice Chairman Palmisano, and...</td>\n",
       "      <td>[Chairman, Donilon, ,, Vice, Chairman, Palmisa...</td>\n",
       "      <td>[[Chairman, Donilon, ,, Vice, Chairman, Palmis...</td>\n",
       "      <td>[[(Chairman, NNP), (Donilon, NNP), (,, ,), (Vi...</td>\n",
       "      <td>[[(Chairman, O), (Donilon, PERSON), (,, O), (V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/3/16</td>\n",
       "      <td>TEDx Tysons Talk</td>\n",
       "      <td>So what do zombies have to do with diplomacy? ...</td>\n",
       "      <td>[So, what, do, zombies, have, to, do, with, di...</td>\n",
       "      <td>[[So, what, do, zombies, have, to, do, with, d...</td>\n",
       "      <td>[[(So, RB), (what, WP), (do, VBP), (zombies, N...</td>\n",
       "      <td>[[(So, O), (what, O), (do, O), (zombies, O), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5/25/16</td>\n",
       "      <td>International Cybersecurity Strategy: Deterrin...</td>\n",
       "      <td>Chairman Gardner, Ranking Member Cardin, membe...</td>\n",
       "      <td>[Chairman, Gardner, ,, Ranking, Member, Cardin...</td>\n",
       "      <td>[[Chairman, Gardner, ,, Ranking, Member, Cardi...</td>\n",
       "      <td>[[(Chairman, NNP), (Gardner, NNP), (,, ,), (Ra...</td>\n",
       "      <td>[[(Chairman, O), (Gardner, PERSON), (,, O), (R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3/1/16</td>\n",
       "      <td>Remarks by Attorney General Loretta E. Lynch a...</td>\n",
       "      <td>Good afternoon, and thank you for that warm we...</td>\n",
       "      <td>[Good, afternoon, ,, and, thank, you, for, tha...</td>\n",
       "      <td>[[Good, afternoon, ,, and, thank, you, for, th...</td>\n",
       "      <td>[[(Good, JJ), (afternoon, NN), (,, ,), (and, C...</td>\n",
       "      <td>[[(Good, O), (afternoon, O), (,, O), (and, O),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2/26/16</td>\n",
       "      <td>Inter-American Committee Against Terrorism (CI...</td>\n",
       "      <td>Background: The Sixteenth Regular Session of t...</td>\n",
       "      <td>[Background, :, The, Sixteenth, Regular, Sessi...</td>\n",
       "      <td>[[Background, :, The, Sixteenth, Regular, Sess...</td>\n",
       "      <td>[[(Background, NN), (:, :), (The, DT), (Sixtee...</td>\n",
       "      <td>[[(Background, O), (:, O), (The, O), (Sixteent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2/11/16</td>\n",
       "      <td>The New Face of Terrorism: Countering Violent ...</td>\n",
       "      <td>Good afternoon. IÍm looking forward to our dis...</td>\n",
       "      <td>[Good, afternoon, ., IÍm, looking, forward, to...</td>\n",
       "      <td>[[Good, afternoon, .], [IÍm, looking, forward,...</td>\n",
       "      <td>[[(Good, JJ), (afternoon, NN), (., .)], [(IÍm,...</td>\n",
       "      <td>[[(Good, O), (afternoon, O), (., O)], [(IÍm, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7/29/15</td>\n",
       "      <td>Remarks for Panel Session \"Development of Cybe...</td>\n",
       "      <td>Thank you for the opportunity to discuss devel...</td>\n",
       "      <td>[Thank, you, for, the, opportunity, to, discus...</td>\n",
       "      <td>[[Thank, you, for, the, opportunity, to, discu...</td>\n",
       "      <td>[[(Thank, VB), (you, PRP), (for, IN), (the, DT...</td>\n",
       "      <td>[[(Thank, O), (you, O), (for, O), (the, O), (o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5/4/15</td>\n",
       "      <td>Department of Commerce Cybersecurity Trade Mis...</td>\n",
       "      <td>Great. Well, thank you, and first of all good ...</td>\n",
       "      <td>[Great, ., Well, ,, thank, you, ,, and, first,...</td>\n",
       "      <td>[[Great, .], [Well, ,, thank, you, ,, and, fir...</td>\n",
       "      <td>[[(Great, JJ), (., .)], [(Well, RB), (,, ,), (...</td>\n",
       "      <td>[[(Great, O), (., O)], [(Well, O), (,, O), (th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2/24/15</td>\n",
       "      <td>New Reward for Cyber Fugitive</td>\n",
       "      <td>ASSISTANT ATTORNEY GENERAL CALDWELL: Thank you...</td>\n",
       "      <td>[ASSISTANT, ATTORNEY, GENERAL, CALDWELL, :, Th...</td>\n",
       "      <td>[[ASSISTANT, ATTORNEY, GENERAL, CALDWELL, :, T...</td>\n",
       "      <td>[[(ASSISTANT, NNP), (ATTORNEY, NNP), (GENERAL,...</td>\n",
       "      <td>[[(ASSISTANT, O), (ATTORNEY, O), (GENERAL, O),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/13/15</td>\n",
       "      <td>The North Korean Threat: Nuclear, Missiles and...</td>\n",
       "      <td>Mr. Chairman, Ranking Member Engel, and Member...</td>\n",
       "      <td>[Mr., Chairman, ,, Ranking, Member, Engel, ,, ...</td>\n",
       "      <td>[[Mr., Chairman, ,, Ranking, Member, Engel, ,,...</td>\n",
       "      <td>[[(Mr., NNP), (Chairman, NNP), (,, ,), (Rankin...</td>\n",
       "      <td>[[(Mr., O), (Chairman, O), (,, O), (Ranking, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3/4/14</td>\n",
       "      <td>As Prepared Remarks at Georgetown University I...</td>\n",
       "      <td>ItÍs a pleasure to be here today. I want to be...</td>\n",
       "      <td>[ItÍs, a, pleasure, to, be, here, today, ., I,...</td>\n",
       "      <td>[[ItÍs, a, pleasure, to, be, here, today, .], ...</td>\n",
       "      <td>[[(ItÍs, VBZ), (a, DT), (pleasure, NN), (to, T...</td>\n",
       "      <td>[[(ItÍs, O), (a, O), (pleasure, O), (to, O), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12/3/13</td>\n",
       "      <td>Remarks at a Cyber Partnership Agreement Signi...</td>\n",
       "      <td>MODERATOR: Good afternoon, and thank you all f...</td>\n",
       "      <td>[MODERATOR, :, Good, afternoon, ,, and, thank,...</td>\n",
       "      <td>[[MODERATOR, :, Good, afternoon, ,, and, thank...</td>\n",
       "      <td>[[(MODERATOR, NN), (:, :), (Good, JJ), (aftern...</td>\n",
       "      <td>[[(MODERATOR, O), (:, O), (Good, O), (afternoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10/21/13</td>\n",
       "      <td>Remarks at High Level Leader's Meeting</td>\n",
       "      <td>Good morning, Minister, Chairman Sasongko, Mr....</td>\n",
       "      <td>[Good, morning, ,, Minister, ,, Chairman, Saso...</td>\n",
       "      <td>[[Good, morning, ,, Minister, ,, Chairman, Sas...</td>\n",
       "      <td>[[(Good, JJ), (morning, NN), (,, ,), (Minister...</td>\n",
       "      <td>[[(Good, O), (morning, O), (,, O), (Minister, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11/26/12</td>\n",
       "      <td>Cyber Security: Project Cyber Monday 3 and Pro...</td>\n",
       "      <td>MODERATOR: Good afternoon, everyone. The Forei...</td>\n",
       "      <td>[MODERATOR, :, Good, afternoon, ,, everyone, ....</td>\n",
       "      <td>[[MODERATOR, :, Good, afternoon, ,, everyone, ...</td>\n",
       "      <td>[[(MODERATOR, NN), (:, :), (Good, JJ), (aftern...</td>\n",
       "      <td>[[(MODERATOR, O), (:, O), (Good, O), (afternoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10/5/12</td>\n",
       "      <td>Video Remarks for the Budapest Cyber Conference</td>\n",
       "      <td>I am delighted to be able to send greetings to...</td>\n",
       "      <td>[I, am, delighted, to, be, able, to, send, gre...</td>\n",
       "      <td>[[I, am, delighted, to, be, able, to, send, gr...</td>\n",
       "      <td>[[(I, PRP), (am, VBP), (delighted, JJ), (to, T...</td>\n",
       "      <td>[[(I, O), (am, O), (delighted, O), (to, O), (b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9/18/12</td>\n",
       "      <td>International Law in Cyberspace</td>\n",
       "      <td>Thank you, Colonel Brown, for your kind invita...</td>\n",
       "      <td>[Thank, you, ,, Colonel, Brown, ,, for, your, ...</td>\n",
       "      <td>[[Thank, you, ,, Colonel, Brown, ,, for, your,...</td>\n",
       "      <td>[[(Thank, VB), (you, PRP), (,, ,), (Colonel, N...</td>\n",
       "      <td>[[(Thank, O), (you, O), (,, O), (Colonel, O), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8/31/12</td>\n",
       "      <td>The Organization of American States' Inter-Ame...</td>\n",
       "      <td>Good Morning. Mr. Assistant Secretary General ...</td>\n",
       "      <td>[Good, Morning, ., Mr., Assistant, Secretary, ...</td>\n",
       "      <td>[[Good, Morning, .], [Mr., Assistant, Secretar...</td>\n",
       "      <td>[[(Good, JJ), (Morning, NN), (., .)], [(Mr., N...</td>\n",
       "      <td>[[(Good, O), (Morning, O), (., O)], [(Mr., O),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3/7/12</td>\n",
       "      <td>Strengthening Cyber Security in the Americas</td>\n",
       "      <td>I want to thank the Committee for the opportun...</td>\n",
       "      <td>[I, want, to, thank, the, Committee, for, the,...</td>\n",
       "      <td>[[I, want, to, thank, the, Committee, for, the...</td>\n",
       "      <td>[[(I, PRP), (want, VBP), (to, TO), (thank, VB)...</td>\n",
       "      <td>[[(I, O), (want, O), (to, O), (thank, O), (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11/17/11</td>\n",
       "      <td>Leading With Diplomacy to Strengthen Stability...</td>\n",
       "      <td>Thank you for your kind introduction. I am hon...</td>\n",
       "      <td>[Thank, you, for, your, kind, introduction, .,...</td>\n",
       "      <td>[[Thank, you, for, your, kind, introduction, ....</td>\n",
       "      <td>[[(Thank, VB), (you, PRP), (for, IN), (your, P...</td>\n",
       "      <td>[[(Thank, O), (you, O), (for, O), (your, O), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10/31/11</td>\n",
       "      <td>Background Briefing: Secretary Clinton's Parti...</td>\n",
       "      <td>MS. FULTON: All right. Thank you. And thanks e...</td>\n",
       "      <td>[MS., FULTON, :, All, right, ., Thank, you, .,...</td>\n",
       "      <td>[[MS., FULTON, :, All, right, .], [Thank, you,...</td>\n",
       "      <td>[[(MS., NNP), (FULTON, NNP), (:, :), (All, DT)...</td>\n",
       "      <td>[[(MS., O), (FULTON, O), (:, O), (All, O), (ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10/18/11</td>\n",
       "      <td>Cybersecurity Update</td>\n",
       "      <td>MR. PAINTER: Thanks, and thanks for coming her...</td>\n",
       "      <td>[MR., PAINTER, :, Thanks, ,, and, thanks, for,...</td>\n",
       "      <td>[[MR., PAINTER, :, Thanks, ,, and, thanks, for...</td>\n",
       "      <td>[[(MR., NNP), (PAINTER, NNP), (:, :), (Thanks,...</td>\n",
       "      <td>[[(MR., O), (PAINTER, O), (:, O), (Thanks, O),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5/18/11</td>\n",
       "      <td>Release of the Obama Administration's Internat...</td>\n",
       "      <td>MODERATOR: Thank you, everyone, for coming to ...</td>\n",
       "      <td>[MODERATOR, :, Thank, you, ,, everyone, ,, for...</td>\n",
       "      <td>[[MODERATOR, :, Thank, you, ,, everyone, ,, fo...</td>\n",
       "      <td>[[(MODERATOR, NN), (:, :), (Thank, VB), (you, ...</td>\n",
       "      <td>[[(MODERATOR, O), (:, O), (Thank, O), (you, O)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5/16/11</td>\n",
       "      <td>Remarks on the Release of President Obama Admi...</td>\n",
       "      <td>Well, thank you very much. As you can guess fr...</td>\n",
       "      <td>[Well, ,, thank, you, very, much, ., As, you, ...</td>\n",
       "      <td>[[Well, ,, thank, you, very, much, .], [As, yo...</td>\n",
       "      <td>[[(Well, RB), (,, ,), (thank, VB), (you, PRP),...</td>\n",
       "      <td>[[(Well, O), (,, O), (thank, O), (you, O), (ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0    12/1/16  Preventing the Exploitation of Information and...   \n",
       "1   10/10/16  Keynote Address at the Singapore International...   \n",
       "2    9/28/16  The Persistent Threat of North Korea and Devel...   \n",
       "3    9/19/16  Statement Before the Presidential Commission o...   \n",
       "4     6/3/16                                   TEDx Tysons Talk   \n",
       "5    5/25/16  International Cybersecurity Strategy: Deterrin...   \n",
       "6     3/1/16  Remarks by Attorney General Loretta E. Lynch a...   \n",
       "7    2/26/16  Inter-American Committee Against Terrorism (CI...   \n",
       "8    2/11/16  The New Face of Terrorism: Countering Violent ...   \n",
       "9    7/29/15  Remarks for Panel Session \"Development of Cybe...   \n",
       "10    5/4/15  Department of Commerce Cybersecurity Trade Mis...   \n",
       "11   2/24/15                      New Reward for Cyber Fugitive   \n",
       "12   1/13/15  The North Korean Threat: Nuclear, Missiles and...   \n",
       "13    3/4/14  As Prepared Remarks at Georgetown University I...   \n",
       "14   12/3/13  Remarks at a Cyber Partnership Agreement Signi...   \n",
       "15  10/21/13             Remarks at High Level Leader's Meeting   \n",
       "16  11/26/12  Cyber Security: Project Cyber Monday 3 and Pro...   \n",
       "17   10/5/12    Video Remarks for the Budapest Cyber Conference   \n",
       "18   9/18/12                    International Law in Cyberspace   \n",
       "19   8/31/12  The Organization of American States' Inter-Ame...   \n",
       "20    3/7/12       Strengthening Cyber Security in the Americas   \n",
       "21  11/17/11  Leading With Diplomacy to Strengthen Stability...   \n",
       "22  10/31/11  Background Briefing: Secretary Clinton's Parti...   \n",
       "23  10/18/11                               Cybersecurity Update   \n",
       "24   5/18/11  Release of the Obama Administration's Internat...   \n",
       "25   5/16/11  Remarks on the Release of President Obama Admi...   \n",
       "\n",
       "                                                 text  \\\n",
       "0   I would like to begin by thanking the UN Count...   \n",
       "1   Good morning! Thank you to Prime Minister Lee ...   \n",
       "2   Introduction. Chairman Gardner, Ranking Member...   \n",
       "3   Chairman Donilon, Vice Chairman Palmisano, and...   \n",
       "4   So what do zombies have to do with diplomacy? ...   \n",
       "5   Chairman Gardner, Ranking Member Cardin, membe...   \n",
       "6   Good afternoon, and thank you for that warm we...   \n",
       "7   Background: The Sixteenth Regular Session of t...   \n",
       "8   Good afternoon. IÍm looking forward to our dis...   \n",
       "9   Thank you for the opportunity to discuss devel...   \n",
       "10  Great. Well, thank you, and first of all good ...   \n",
       "11  ASSISTANT ATTORNEY GENERAL CALDWELL: Thank you...   \n",
       "12  Mr. Chairman, Ranking Member Engel, and Member...   \n",
       "13  ItÍs a pleasure to be here today. I want to be...   \n",
       "14  MODERATOR: Good afternoon, and thank you all f...   \n",
       "15  Good morning, Minister, Chairman Sasongko, Mr....   \n",
       "16  MODERATOR: Good afternoon, everyone. The Forei...   \n",
       "17  I am delighted to be able to send greetings to...   \n",
       "18  Thank you, Colonel Brown, for your kind invita...   \n",
       "19  Good Morning. Mr. Assistant Secretary General ...   \n",
       "20  I want to thank the Committee for the opportun...   \n",
       "21  Thank you for your kind introduction. I am hon...   \n",
       "22  MS. FULTON: All right. Thank you. And thanks e...   \n",
       "23  MR. PAINTER: Thanks, and thanks for coming her...   \n",
       "24  MODERATOR: Thank you, everyone, for coming to ...   \n",
       "25  Well, thank you very much. As you can guess fr...   \n",
       "\n",
       "                                       tokenized-text  \\\n",
       "0   [I, would, like, to, begin, by, thanking, the,...   \n",
       "1   [Good, morning, !, Thank, you, to, Prime, Mini...   \n",
       "2   [Introduction, ., Chairman, Gardner, ,, Rankin...   \n",
       "3   [Chairman, Donilon, ,, Vice, Chairman, Palmisa...   \n",
       "4   [So, what, do, zombies, have, to, do, with, di...   \n",
       "5   [Chairman, Gardner, ,, Ranking, Member, Cardin...   \n",
       "6   [Good, afternoon, ,, and, thank, you, for, tha...   \n",
       "7   [Background, :, The, Sixteenth, Regular, Sessi...   \n",
       "8   [Good, afternoon, ., IÍm, looking, forward, to...   \n",
       "9   [Thank, you, for, the, opportunity, to, discus...   \n",
       "10  [Great, ., Well, ,, thank, you, ,, and, first,...   \n",
       "11  [ASSISTANT, ATTORNEY, GENERAL, CALDWELL, :, Th...   \n",
       "12  [Mr., Chairman, ,, Ranking, Member, Engel, ,, ...   \n",
       "13  [ItÍs, a, pleasure, to, be, here, today, ., I,...   \n",
       "14  [MODERATOR, :, Good, afternoon, ,, and, thank,...   \n",
       "15  [Good, morning, ,, Minister, ,, Chairman, Saso...   \n",
       "16  [MODERATOR, :, Good, afternoon, ,, everyone, ....   \n",
       "17  [I, am, delighted, to, be, able, to, send, gre...   \n",
       "18  [Thank, you, ,, Colonel, Brown, ,, for, your, ...   \n",
       "19  [Good, Morning, ., Mr., Assistant, Secretary, ...   \n",
       "20  [I, want, to, thank, the, Committee, for, the,...   \n",
       "21  [Thank, you, for, your, kind, introduction, .,...   \n",
       "22  [MS., FULTON, :, All, right, ., Thank, you, .,...   \n",
       "23  [MR., PAINTER, :, Thanks, ,, and, thanks, for,...   \n",
       "24  [MODERATOR, :, Thank, you, ,, everyone, ,, for...   \n",
       "25  [Well, ,, thank, you, very, much, ., As, you, ...   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   [[I, would, like, to, begin, by, thanking, the...   \n",
       "1   [[Good, morning, !], [Thank, you, to, Prime, M...   \n",
       "2   [[Introduction, .], [Chairman, Gardner, ,, Ran...   \n",
       "3   [[Chairman, Donilon, ,, Vice, Chairman, Palmis...   \n",
       "4   [[So, what, do, zombies, have, to, do, with, d...   \n",
       "5   [[Chairman, Gardner, ,, Ranking, Member, Cardi...   \n",
       "6   [[Good, afternoon, ,, and, thank, you, for, th...   \n",
       "7   [[Background, :, The, Sixteenth, Regular, Sess...   \n",
       "8   [[Good, afternoon, .], [IÍm, looking, forward,...   \n",
       "9   [[Thank, you, for, the, opportunity, to, discu...   \n",
       "10  [[Great, .], [Well, ,, thank, you, ,, and, fir...   \n",
       "11  [[ASSISTANT, ATTORNEY, GENERAL, CALDWELL, :, T...   \n",
       "12  [[Mr., Chairman, ,, Ranking, Member, Engel, ,,...   \n",
       "13  [[ItÍs, a, pleasure, to, be, here, today, .], ...   \n",
       "14  [[MODERATOR, :, Good, afternoon, ,, and, thank...   \n",
       "15  [[Good, morning, ,, Minister, ,, Chairman, Sas...   \n",
       "16  [[MODERATOR, :, Good, afternoon, ,, everyone, ...   \n",
       "17  [[I, am, delighted, to, be, able, to, send, gr...   \n",
       "18  [[Thank, you, ,, Colonel, Brown, ,, for, your,...   \n",
       "19  [[Good, Morning, .], [Mr., Assistant, Secretar...   \n",
       "20  [[I, want, to, thank, the, Committee, for, the...   \n",
       "21  [[Thank, you, for, your, kind, introduction, ....   \n",
       "22  [[MS., FULTON, :, All, right, .], [Thank, you,...   \n",
       "23  [[MR., PAINTER, :, Thanks, ,, and, thanks, for...   \n",
       "24  [[MODERATOR, :, Thank, you, ,, everyone, ,, fo...   \n",
       "25  [[Well, ,, thank, you, very, much, .], [As, yo...   \n",
       "\n",
       "                                            POS-sents  \\\n",
       "0   [[(I, PRP), (would, MD), (like, VB), (to, TO),...   \n",
       "1   [[(Good, JJ), (morning, NN), (!, .)], [(Thank,...   \n",
       "2   [[(Introduction, NN), (., .)], [(Chairman, NNP...   \n",
       "3   [[(Chairman, NNP), (Donilon, NNP), (,, ,), (Vi...   \n",
       "4   [[(So, RB), (what, WP), (do, VBP), (zombies, N...   \n",
       "5   [[(Chairman, NNP), (Gardner, NNP), (,, ,), (Ra...   \n",
       "6   [[(Good, JJ), (afternoon, NN), (,, ,), (and, C...   \n",
       "7   [[(Background, NN), (:, :), (The, DT), (Sixtee...   \n",
       "8   [[(Good, JJ), (afternoon, NN), (., .)], [(IÍm,...   \n",
       "9   [[(Thank, VB), (you, PRP), (for, IN), (the, DT...   \n",
       "10  [[(Great, JJ), (., .)], [(Well, RB), (,, ,), (...   \n",
       "11  [[(ASSISTANT, NNP), (ATTORNEY, NNP), (GENERAL,...   \n",
       "12  [[(Mr., NNP), (Chairman, NNP), (,, ,), (Rankin...   \n",
       "13  [[(ItÍs, VBZ), (a, DT), (pleasure, NN), (to, T...   \n",
       "14  [[(MODERATOR, NN), (:, :), (Good, JJ), (aftern...   \n",
       "15  [[(Good, JJ), (morning, NN), (,, ,), (Minister...   \n",
       "16  [[(MODERATOR, NN), (:, :), (Good, JJ), (aftern...   \n",
       "17  [[(I, PRP), (am, VBP), (delighted, JJ), (to, T...   \n",
       "18  [[(Thank, VB), (you, PRP), (,, ,), (Colonel, N...   \n",
       "19  [[(Good, JJ), (Morning, NN), (., .)], [(Mr., N...   \n",
       "20  [[(I, PRP), (want, VBP), (to, TO), (thank, VB)...   \n",
       "21  [[(Thank, VB), (you, PRP), (for, IN), (your, P...   \n",
       "22  [[(MS., NNP), (FULTON, NNP), (:, :), (All, DT)...   \n",
       "23  [[(MR., NNP), (PAINTER, NNP), (:, :), (Thanks,...   \n",
       "24  [[(MODERATOR, NN), (:, :), (Thank, VB), (you, ...   \n",
       "25  [[(Well, RB), (,, ,), (thank, VB), (you, PRP),...   \n",
       "\n",
       "                                     classified-sents  \n",
       "0   [[(I, O), (would, O), (like, O), (to, O), (beg...  \n",
       "1   [[(Good, O), (morning, O), (!, O)], [(Thank, O...  \n",
       "2   [[(Introduction, O), (., O)], [(Chairman, O), ...  \n",
       "3   [[(Chairman, O), (Donilon, PERSON), (,, O), (V...  \n",
       "4   [[(So, O), (what, O), (do, O), (zombies, O), (...  \n",
       "5   [[(Chairman, O), (Gardner, PERSON), (,, O), (R...  \n",
       "6   [[(Good, O), (afternoon, O), (,, O), (and, O),...  \n",
       "7   [[(Background, O), (:, O), (The, O), (Sixteent...  \n",
       "8   [[(Good, O), (afternoon, O), (., O)], [(IÍm, O...  \n",
       "9   [[(Thank, O), (you, O), (for, O), (the, O), (o...  \n",
       "10  [[(Great, O), (., O)], [(Well, O), (,, O), (th...  \n",
       "11  [[(ASSISTANT, O), (ATTORNEY, O), (GENERAL, O),...  \n",
       "12  [[(Mr., O), (Chairman, O), (,, O), (Ranking, O...  \n",
       "13  [[(ItÍs, O), (a, O), (pleasure, O), (to, O), (...  \n",
       "14  [[(MODERATOR, O), (:, O), (Good, O), (afternoo...  \n",
       "15  [[(Good, O), (morning, O), (,, O), (Minister, ...  \n",
       "16  [[(MODERATOR, O), (:, O), (Good, O), (afternoo...  \n",
       "17  [[(I, O), (am, O), (delighted, O), (to, O), (b...  \n",
       "18  [[(Thank, O), (you, O), (,, O), (Colonel, O), ...  \n",
       "19  [[(Good, O), (Morning, O), (., O)], [(Mr., O),...  \n",
       "20  [[(I, O), (want, O), (to, O), (thank, O), (the...  \n",
       "21  [[(Thank, O), (you, O), (for, O), (your, O), (...  \n",
       "22  [[(MS., O), (FULTON, O), (:, O), (All, O), (ri...  \n",
       "23  [[(MR., O), (PAINTER, O), (:, O), (Thanks, O),...  \n",
       "24  [[(MODERATOR, O), (:, O), (Thank, O), (you, O)...  \n",
       "25  [[(Well, O), (,, O), (thank, O), (you, O), (ve...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOSSpeechesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Handcoding Portion - Given the time constraint, I did as much as possible\n",
    "#I'm aware that the idea was to create our hand code, then put it in a dataframe,\n",
    "#and run similar code to last week comparing the computer's code to our handcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODERATOR: Good afternoon, everyone. The Foreign Press Center is pleased to host a briefing on recent cyber security operations with Mr. John Morton. Mr. Morton has served as director of U.S. Immigration and Customs Enforcement since May of 2009. ICE is the principal investigative arm of the Department of Homeland Security and the second largest investigative agency in the federal government. Following the briefing, weÍll have time for a few questions, just a few. For those with me here, I would ask you to wait for the microphone before asking your question and identify yourselves as well as your outlet before you ask your question. And weÍll be also fielding some questions potentially from New York, by email, as well as overseas. With that, I will turn the floor to Director Morton. MR. MORTON: All right. Well, good day and welcome. I am John Morton, the Director of Immigration and Customs Enforcement. And today IÍm pleased to announce the results of a coordinated effort to attack online counterfeiting and consumer fraud on this Cyber Monday. Last year Cyber Monday generated a record $1.25 billion in sales, and we anticipate a similarly high level this year. The purpose of our efforts is to confront consumer fraud online. Organized criminals are using the internet and the holiday shopping season to defraud consumers on a grand scale. A wide variety of consumer goods are presented on bogus websites as being genuine, when in fact they are counterfeits, illegally produced and shipped from overseas. In this, our third year of attacking online counterfeiters on Cyber Monday, we have seized 101 websites here in the United States that were selling counterfeit and falsely labeled goods to consumers during the holiday season. These websites sold products representing 41 different rights holders of U.S. companies, everything from Ergo baby carriers to New Era hats, Nike sneakers, Tiffany jewelry, Oakley sunglasses, and NFL jerseys just to name a few. Even counterfeit Adobe software was for sale. This year we went a step further, taking this operation to both sides of the Atlantic. The National Intellectual Property Rights Coordination Center, a component of ICEÍs Homeland Security investigations, partnered with our friends at Europol to coordinate seizures of counterfeit websites operating through top-level domain names in Europe such as .eu and .de. So today, Europol and its partner countries through Project Transatlantic seized an additional 31 domain names. Law enforcement agencies from Belgium and the United Kingdom took part in this coordinated effort, and other Europol partners will be participating in the coming days with additional websites being addressed. For the specifics, you have the Belgian Federal Police obtaining warrants to seize 12 .eu domain names and one .de domain name and the City of London Police redirecting the domain names of an additional 18 websites. We are, in short, organizing a transnational law enforcement effort to defeat transnational criminal organizations, a united front against a common foe. Domain names such as hermesborse.eu and nikeatalon.eu were selling everything from counterfeit Hermes purses, Christian Louboutin shoes, and various Nike apparel, all of it fake, all of it substandard. Between both of these operations on both sides of the Atlantic, we have seized a total of 132 websites. These websites were stealing from businesses, trademark and copyright holders, and the people who make the legitimate products and who sell them, including companies here and abroad. This is a global problem. It affects everyone. When IP rights are violated, jobs are lost, businesses are stolen, and ultimately consumers are cheated. Remember, counterfeiters care about making money and only about making money. They donÍt pay healthcare, they donÍt pay pensions, they donÍt pay taxes, they donÍt care about the people that work for them, and they donÍt, frankly, care about the consumers who purchase the products. Recognizing the Cyber Monday is the busiest day for online purchases in the United States, weÍve conducted specific operations around this day to try to bring more awareness to the public about the potential dangers of shopping online. Shoppers all over the world are continuing to make purchases online in increasing numbers. ThatÍs a good thing. The internet is a good thing. Sales over the internet are a good thing. But with an increase in online shopping comes an increase, sadly, in online predators, organized criminals looking to take advantage of unsuspecting consumers to make a quick buck. Unfortunately, counterfeiters have expanded from selling DVDs and purses to products that affect the health and safety of consumers, everything from counterfeit pharmaceuticals, counterfeit heart medicine, for example, to counterfeit airbags. For example, in this operation, counterfeiters were offering Ergo baby carriers for sale on many websites. Were these legitimate baby carriers? No. They were 100 percent knockoffs having nothing to do with Ergo Baby, having nothing to do with thoughtful design. Were they safe? Absolutely not. This is a purchase that expectant parents should not have to worry about. And in many instances, the websites were designed to mimic the real Ergo baby carrier site to further dupe consumers. If you go to our Facebook page, you can see an example of that and the increasing sophistication that counterfeiters are using to attract consumers to dupe them, to rip them off. This is a fraud from start to finish. If you go on the Facebook site, I invite you, you try to see if you can determine the difference between the real sites and the legitimate sites, and youÍll find youÍll have a very, very hard time doing it. Visitors to the domain names that we have taken will now find a banner that notifies them of the seizure and educates them about the federal crime of trademark and copyright infringement. And these seizures underscore the lessons of online shopping for all of us at this time of the holidays. Remember, if the price seems too good to be true, it probably is. Know your supplier, know your seller, research the seller or the website from which you intend to make a purchase, look at all of the web pages. If you see misspellings, if you see addresses on a site that pretends to be about Tiffany, but the return policy addresses is used some place in China, well youÍre on notice that trouble is afoot. Educate yourself on the exact logo, the hardware, and the stitching of whatever youÍre considering to make sure that youÍre getting what you think youÍre getting. DonÍt conduct business with an anonymous user. IÍll tell you, a number of these websites are frauds through and through. Not only are they looking to defraud you in terms of what you buy, often providing them your credit card number is not a good idea either, and there are numerous instances of more than one crime going on these websites. At the end of the day, trust your instincts. You know what youÍre looking for, you know the product, you know when a site is likely legitimate. This is the best line of defense for you, so donÍt reason away your intuition. And I just think, in closing, people need to realize that obviously the holidays are a time of great excitement for all of us, and when you go out there and you try to buy the real thing for your relative or your family member, you want it to be legitimate, youÍre paying for it, and unfortunately there is a steady group of organized criminals that are trying to rip you off, literally, from start to finish. And thatÍs what this enforcement action was all about, and for the first time weÍre doing it together with the Europeans, the right answer. This is a global problem; this isnÍt a U.S. problem, itÍs not a European problem, itÍs not a Mexican problem. ItÍs a shared problem that we need to handle all together.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(DOSSpeechesDF['text'][16])\n",
    "HCsample = DOSSpeechesDF['classified-sents'][16]\n",
    "HCsample = HCsample[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('MODERATOR', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('Good', 'O'),\n",
       "  ('afternoon', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('everyone', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('The', 'O'),\n",
       "  ('Foreign', 'ORGANIZATION'),\n",
       "  ('Press', 'ORGANIZATION'),\n",
       "  ('Center', 'ORGANIZATION'),\n",
       "  ('is', 'O'),\n",
       "  ('pleased', 'O'),\n",
       "  ('to', 'O'),\n",
       "  ('host', 'O'),\n",
       "  ('a', 'O'),\n",
       "  ('briefing', 'O'),\n",
       "  ('on', 'O'),\n",
       "  ('recent', 'O'),\n",
       "  ('cyber', 'O'),\n",
       "  ('security', 'O'),\n",
       "  ('operations', 'O'),\n",
       "  ('with', 'O'),\n",
       "  ('Mr.', 'O'),\n",
       "  ('John', 'PERSON'),\n",
       "  ('Morton', 'PERSON'),\n",
       "  ('.', 'O')],\n",
       " [('Mr.', 'O'),\n",
       "  ('Morton', 'PERSON'),\n",
       "  ('has', 'O'),\n",
       "  ('served', 'O'),\n",
       "  ('as', 'O'),\n",
       "  ('director', 'O'),\n",
       "  ('of', 'O'),\n",
       "  ('U.S.', 'LOCATION'),\n",
       "  ('Immigration', 'O'),\n",
       "  ('and', 'O'),\n",
       "  ('Customs', 'ORGANIZATION'),\n",
       "  ('Enforcement', 'ORGANIZATION'),\n",
       "  ('since', 'O'),\n",
       "  ('May', 'O'),\n",
       "  ('of', 'O'),\n",
       "  ('2009', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('ICE', 'O'),\n",
       "  ('is', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('principal', 'O'),\n",
       "  ('investigative', 'O'),\n",
       "  ('arm', 'O'),\n",
       "  ('of', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('Department', 'ORGANIZATION'),\n",
       "  ('of', 'ORGANIZATION'),\n",
       "  ('Homeland', 'ORGANIZATION'),\n",
       "  ('Security', 'ORGANIZATION'),\n",
       "  ('and', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('second', 'O'),\n",
       "  ('largest', 'O'),\n",
       "  ('investigative', 'O'),\n",
       "  ('agency', 'O'),\n",
       "  ('in', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('federal', 'O'),\n",
       "  ('government', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Following', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('briefing', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('weÍll', 'O'),\n",
       "  ('have', 'O'),\n",
       "  ('time', 'O'),\n",
       "  ('for', 'O'),\n",
       "  ('a', 'O'),\n",
       "  ('few', 'O'),\n",
       "  ('questions', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('just', 'O'),\n",
       "  ('a', 'O'),\n",
       "  ('few', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('For', 'O'),\n",
       "  ('those', 'O'),\n",
       "  ('with', 'O'),\n",
       "  ('me', 'O'),\n",
       "  ('here', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('I', 'O'),\n",
       "  ('would', 'O'),\n",
       "  ('ask', 'O'),\n",
       "  ('you', 'O'),\n",
       "  ('to', 'O'),\n",
       "  ('wait', 'O'),\n",
       "  ('for', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('microphone', 'O'),\n",
       "  ('before', 'O'),\n",
       "  ('asking', 'O'),\n",
       "  ('your', 'O'),\n",
       "  ('question', 'O'),\n",
       "  ('and', 'O'),\n",
       "  ('identify', 'O'),\n",
       "  ('yourselves', 'O'),\n",
       "  ('as', 'O'),\n",
       "  ('well', 'O'),\n",
       "  ('as', 'O'),\n",
       "  ('your', 'O'),\n",
       "  ('outlet', 'O'),\n",
       "  ('before', 'O'),\n",
       "  ('you', 'O'),\n",
       "  ('ask', 'O'),\n",
       "  ('your', 'O'),\n",
       "  ('question', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('And', 'O'),\n",
       "  ('weÍll', 'O'),\n",
       "  ('be', 'O'),\n",
       "  ('also', 'O'),\n",
       "  ('fielding', 'O'),\n",
       "  ('some', 'O'),\n",
       "  ('questions', 'O'),\n",
       "  ('potentially', 'O'),\n",
       "  ('from', 'O'),\n",
       "  ('New', 'LOCATION'),\n",
       "  ('York', 'LOCATION'),\n",
       "  (',', 'O'),\n",
       "  ('by', 'O'),\n",
       "  ('email', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('as', 'O'),\n",
       "  ('well', 'O'),\n",
       "  ('as', 'O'),\n",
       "  ('overseas', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('With', 'O'),\n",
       "  ('that', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('I', 'O'),\n",
       "  ('will', 'O'),\n",
       "  ('turn', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('floor', 'O'),\n",
       "  ('to', 'O'),\n",
       "  ('Director', 'O'),\n",
       "  ('Morton', 'PERSON'),\n",
       "  ('.', 'O')],\n",
       " [('MR.', 'PERSON'),\n",
       "  ('MORTON', 'PERSON'),\n",
       "  (':', 'O'),\n",
       "  ('All', 'O'),\n",
       "  ('right', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Well', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('good', 'O'),\n",
       "  ('day', 'O'),\n",
       "  ('and', 'O'),\n",
       "  ('welcome', 'O'),\n",
       "  ('.', 'O')]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved to csv file and then upload handcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   classified-sents  machine-code  handcode\n",
      "0               ('MODERATOR', 'O'),         False      True\n",
      "1                       (':', 'O'),         False     False\n",
      "2                    ('Good', 'O'),         False     False\n",
      "3               ('afternoon', 'O'),         False     False\n",
      "4                       (',', 'O'),         False     False\n",
      "5                ('everyone', 'O'),         False     False\n",
      "6                      ('.', 'O')],         False     False\n",
      "7                    [('The', 'O'),         False     False\n",
      "8      ('Foreign', 'ORGANIZATION'),         False     False\n",
      "9        ('Press', 'ORGANIZATION'),         False     False\n",
      "10      ('Center', 'ORGANIZATION'),         False     False\n",
      "11                     ('is', 'O'),         False     False\n",
      "12                ('pleased', 'O'),         False     False\n",
      "13                     ('to', 'O'),         False     False\n",
      "14                   ('host', 'O'),         False     False\n",
      "15                      ('a', 'O'),         False     False\n",
      "16               ('briefing', 'O'),         False     False\n",
      "17                     ('on', 'O'),         False     False\n",
      "18                 ('recent', 'O'),         False     False\n",
      "19                  ('cyber', 'O'),         False     False\n",
      "20               ('security', 'O'),         False     False\n",
      "21             ('operations', 'O'),         False     False\n",
      "22                   ('with', 'O'),         False     False\n",
      "23                    ('Mr.', 'O'),         False      True\n",
      "24              ('John', 'PERSON'),          True      True\n",
      "25            ('Morton', 'PERSON'),          True      True\n",
      "26                     ('.', 'O')],         False     False\n",
      "27                   [('Mr.', 'O'),         False      True\n",
      "28            ('Morton', 'PERSON'),          True      True\n",
      "29                    ('has', 'O'),         False     False\n",
      "..                              ...           ...       ...\n",
      "131                    ('as', 'O'),         False     False\n",
      "132                  ('well', 'O'),         False     False\n",
      "133                    ('as', 'O'),         False     False\n",
      "134              ('overseas', 'O'),         False     False\n",
      "135                    ('.', 'O')],         False     False\n",
      "136                 [('With', 'O'),         False     False\n",
      "137                  ('that', 'O'),         False     False\n",
      "138                     (',', 'O'),         False     False\n",
      "139                     ('I', 'O'),         False     False\n",
      "140                  ('will', 'O'),         False     False\n",
      "141                  ('turn', 'O'),         False     False\n",
      "142                   ('the', 'O'),         False     False\n",
      "143                 ('floor', 'O'),         False     False\n",
      "144                    ('to', 'O'),         False     False\n",
      "145              ('Director', 'O'),         False      True\n",
      "146           ('Morton', 'PERSON'),          True      True\n",
      "147                    ('.', 'O')],         False     False\n",
      "148             [('MR.', 'PERSON'),          True      True\n",
      "149           ('MORTON', 'PERSON'),          True      True\n",
      "150                     (':', 'O'),         False     False\n",
      "151                   ('All', 'O'),         False     False\n",
      "152                 ('right', 'O'),         False     False\n",
      "153                    ('.', 'O')],         False     False\n",
      "154                 [('Well', 'O'),         False     False\n",
      "155                     (',', 'O'),         False     False\n",
      "156                  ('good', 'O'),         False     False\n",
      "157                   ('day', 'O'),         False     False\n",
      "158                   ('and', 'O'),         False     False\n",
      "159               ('welcome', 'O'),         False     False\n",
      "160                    ('.', 'O')]]         False     False\n",
      "\n",
      "[161 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "handcode = '/Users/Enya/Desktop/content-analysis-2018/7-Information-Extraction/handcode.csv'\n",
    "handcodeDF = pandas.read_csv(handcode, encoding='Latin-1')\n",
    "print (handcodeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision\n",
    "sklearn.metrics.precision_score(handcodeDF['machine-code'], handcodeDF['handcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recall\n",
    "sklearn.metrics.recall_score(handcodeDF['machine-code'], handcodeDF['handcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499999999999999"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F1\n",
    "sklearn.metrics.f1_score(handcodeDF['machine-code'], handcodeDF['handcode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Here we will introduce the Stanford Parser by feeding it tokenized text from our initial example sentences. The parser is a dependency parser, but this initial program outputs a simple, self-explanatory phrase-structure representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Trayvon']), Tree('NNP', ['Benjamin']), Tree('NNP', ['Martin'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('DT', ['an']), Tree('NNP', ['African']), Tree('NNP', ['American'])]), Tree('PP', [Tree('IN', ['from']), Tree('NP', [Tree('NP', [Tree('NNP', ['Miami']), Tree('NNPS', ['Gardens'])]), Tree(',', [',']), Tree('NP', [Tree('NNP', ['Florida'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree(',', [',']), Tree('PP', [Tree('IN', ['at']), Tree('ADJP', [Tree('NP', [Tree('CD', ['17']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])])]), Tree(',', [',']), Tree('VP', [Tree('VBD', ['was']), Tree('ADVP', [Tree('RB', ['fatally'])]), Tree('VP', [Tree('VBN', ['shot']), Tree('PP', [Tree('IN', ['by']), Tree('NP', [Tree('NP', [Tree('NNP', ['George']), Tree('NNP', ['Zimmerman'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['neighborhood']), Tree('NN', ['watch']), Tree('NN', ['volunteer'])]), Tree(',', [','])])]), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('NNP', ['Sanford']), Tree(',', [',']), Tree('NNP', ['Florida'])])])])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parses = list(stanford.parser.parse_sents(tokenized_text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "fourthSentParseTree = list(parses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(fourthSentParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are a common data structure and there are a large number of things to do with them. What we are intetered in is the relationship between different types of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeSubRelation(parsetree, relationTypeScope, relationTypeTarget, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retSet = set()\n",
    "        for subT in parsetree.subtrees():\n",
    "            if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                if subT.label() == relationTypeScope:\n",
    "                    for subsub in subT.subtrees():\n",
    "                        if subsub.label()==relationTypeTarget:\n",
    "                            retSet.add(' '.join(subsub.leaves()))\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   'an African American from Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')],\n",
       " [('NP',\n",
       "   'Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'NP', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Florida occurs twice in two different nested noun phrases in the sentence. \n",
    "\n",
    "We can also find all of the verbs within the noun phrase defined by one or more target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shot'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeSubRelation(fourthSentParseTree, 'NP', 'VBN', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to to look at the whole tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   ROOT                                                                                                                       \n",
      "                                                                                                                    |                                                                                                                          \n",
      "                                                                                                                    S                                                                                                                         \n",
      "            ________________________________________________________________________________________________________|_______________________________________________________________________________________________________________________   \n",
      "           |                       VP                                                                                                                                                                                                       | \n",
      "           |              _________|______________                                                                                                                                                                                          |  \n",
      "           |             |                        NP                                                                                                                                                                                        | \n",
      "           |             |          ______________|________________                                                                                                                                                                         |  \n",
      "           |             |         |                               PP                                                                                                                                                                       | \n",
      "           |             |         |               ________________|___________                                                                                                                                                             |  \n",
      "           |             |         |              |                            NP                                                                                                                                                           | \n",
      "           |             |         |              |           _________________|____________________________________                                                                                                                        |  \n",
      "           |             |         |              |          |           |     |     |                             SBAR                                                                                                                     | \n",
      "           |             |         |              |          |           |     |     |    __________________________|______________________________                                                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |                                                         S                                                                                        | \n",
      "           |             |         |              |          |           |     |     |   |     ____________________________________________________|_______________________                                                                 |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |                                                 VP                                                               | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |    _____________________________________________|_______                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |                                               VP                                                       | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |      _________________________________________|________________________________________                |  \n",
      "           |             |         |              |          |           |     |     |   |    |           PP             |   |     |     |                      PP                                                          |               | \n",
      "           |             |         |              |          |           |     |     |   |    |    _______|____          |   |     |     |     _________________|__________                                                 |               |  \n",
      "           |             |         |              |          |           |     |     |   |    |   |           ADJP       |   |     |     |    |                            NP                                               PP              | \n",
      "           |             |         |              |          |           |     |     |   |    |   |        ____|____     |   |     |     |    |           _________________|________________________________     ___________|___            |  \n",
      "           NP            |         NP             |          NP          |     NP    |  WHNP  |   |       NP        |    |   |    ADVP   |    |          NP            |           NP                       |   |               NP          | \n",
      "    _______|_______      |    _____|_______       |      ____|_____      |     |     |   |    |   |    ___|____     |    |   |     |     |    |     _____|______       |    _______|_________________       |   |      _________|_____      |  \n",
      "  NNP     NNP     NNP   VBD  DT   NNP     NNP     IN   NNP        NNPS   ,    NNP    ,   WP   ,   IN  CD      NNS   JJ   ,  VBD    RB   VBN   IN  NNP          NNP     ,   DT      NN        NN      NN     ,   IN   NNP        ,    NNP    . \n",
      "   |       |       |     |   |     |       |      |     |          |     |     |     |   |    |   |   |        |    |    |   |     |     |    |    |            |      |   |       |         |       |      |   |     |         |     |     |  \n",
      "Trayvon Benjamin Martin was  an African American from Miami     Gardens  ,  Florida  ,  who   ,   at  17     years old   ,  was fatally shot  by George     Zimmerman  ,   a  neighborhood watch volunteer  ,   in Sanford      ,  Florida  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-3ee1a5163f1d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-3ee1a5163f1d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    list(parses[1])[].pretty_print()\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list(parses[1])[].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing and graph representations\n",
    "\n",
    "Dependency parsing was developed to robustly capture linguistic dependencies from text. The complex tags associated with these parses are detailed [here]('http://universaldependencies.org/u/overview/syntax.html'). When parsing with the dependency parser, we will work directly from the untokenized text. Note that no *processing* takes place before parsing sentences--we do not remove so-called stop words or anything that plays a syntactic role in the sentence, although anaphora resolution and related normalization may be performed before or after parsing to enhance the value of information extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x119a4e730>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [7]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Trayvon'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Benjamin'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'compound': [1, 2]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Martin'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'cop',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'was'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'an'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'African'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'compound': [6],\n",
      "                                      'cop': [4],\n",
      "                                      'det': [5],\n",
      "                                      'nmod': [10],\n",
      "                                      'nsubj': [3]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'American'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 10,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'from'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 10,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Miami'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'NNPS',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'acl:relcl': [23],\n",
      "                                       'appos': [12],\n",
      "                                       'case': [8],\n",
      "                                       'compound': [9]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 7,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNPS',\n",
      "                  'word': 'Gardens'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 10,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'appos',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Florida'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'WP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nsubjpass',\n",
      "                  'tag': 'WP',\n",
      "                  'word': 'who'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 19,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'at'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'CD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nummod',\n",
      "                  'tag': 'CD',\n",
      "                  'word': '17'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': 'NNS',\n",
      "                  'deps': defaultdict(<class 'list'>, {'nummod': [17]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 19,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod:npmod',\n",
      "                  'tag': 'NNS',\n",
      "                  'word': 'years'},\n",
      "             19: {'address': 19,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'case': [16],\n",
      "                                       'nmod:npmod': [18]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advcl',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'old'},\n",
      "             21: {'address': 21,\n",
      "                  'ctag': 'VBD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'auxpass',\n",
      "                  'tag': 'VBD',\n",
      "                  'word': 'was'},\n",
      "             22: {'address': 22,\n",
      "                  'ctag': 'RB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advmod',\n",
      "                  'tag': 'RB',\n",
      "                  'word': 'fatally'},\n",
      "             23: {'address': 23,\n",
      "                  'ctag': 'VBN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advcl': [19],\n",
      "                                       'advmod': [22],\n",
      "                                       'auxpass': [21],\n",
      "                                       'nmod': [26, 36],\n",
      "                                       'nsubjpass': [14]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 10,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'acl:relcl',\n",
      "                  'tag': 'VBN',\n",
      "                  'word': 'shot'},\n",
      "             24: {'address': 24,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 26,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'by'},\n",
      "             25: {'address': 25,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 26,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'George'},\n",
      "             26: {'address': 26,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'appos': [31],\n",
      "                                       'case': [24],\n",
      "                                       'compound': [25]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Zimmerman'},\n",
      "             28: {'address': 28,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 31,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'det',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             29: {'address': 29,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 31,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'neighborhood'},\n",
      "             30: {'address': 30,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 31,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'watch'},\n",
      "             31: {'address': 31,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'compound': [29, 30],\n",
      "                                       'det': [28]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 26,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'appos',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'volunteer'},\n",
      "             33: {'address': 33,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'in'},\n",
      "             34: {'address': 34,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Sanford'},\n",
      "             36: {'address': 36,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'case': [33],\n",
      "                                       'compound': [34]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Florida'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "secondSentDepParseTree = list(depParses[1])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph and we can convert it to a dot file and use that to visulize it. Try traversing the tree and extracting elements that are nearby one another. We note that unless you have the graphviz successfully installed on your computer (which is not necessary to complete this homework), the following graphviz call will trigger an error. If you are interested in installing graphviz and working on a Mac, consider installing through [homebrew](https://brew.sh), a package manager (i.e., with the command \"brew install graphviz\", once brew is installed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"467pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 467.37 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-298 463.3657,-298 463.3657,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (jumped)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M193.7949,-257.7616C193.7949,-246.3597 193.7949,-231.4342 193.7949,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"197.295,-218.2121 193.7949,-208.2121 190.295,-218.2121 197.295,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"205.0708\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"152.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M184.9506,-171.6975C182.2192,-166.0286 179.2067,-159.7594 176.457,-154 172.9867,-146.731 169.2503,-138.8578 165.7948,-131.5568\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.7462,-129.6107 161.3081,-122.0658 162.4177,-132.6024 168.7462,-129.6107\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.9639\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"316.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (dog)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.5798,-171.9716C237.9481,-159.1286 262.8214,-141.7376 282.8098,-127.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"284.8541,-130.6033 291.044,-122.0047 280.843,-124.8665 284.8541,-130.6033\"/>\n",
       "<text text-anchor=\"middle\" x=\"278.7397\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"108.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (quick)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"195.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (brown)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M126.8005,-85.9716C108.2827,-73.1286 83.2073,-55.7376 63.0563,-41.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.967,-38.8277 54.7552,-36.0047 60.9777,-44.5797 64.967,-38.8277\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4637,-85.7616C137.4551,-74.0176 129.534,-58.5355 122.7802,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.7834,-43.5204 118.1127,-36.2121 119.5517,-46.7088 125.7834,-43.5204\"/>\n",
       "<text text-anchor=\"middle\" x=\"149.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M161.9141,-85.7616C167.7861,-74.0176 175.5272,-58.5355 182.1275,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.3472,-46.7216 186.6889,-36.2121 179.0862,-43.5911 185.3472,-46.7216\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"279.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (over)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>9&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M308.9482,-85.7616C303.9446,-74.1316 297.3638,-58.8357 291.721,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"294.7976,-44.0148 287.6304,-36.2121 288.3674,-46.7813 294.7976,-44.0148\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.8398\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"354.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (the)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M324.8537,-85.7616C329.9926,-74.1316 336.7512,-58.8357 342.5466,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"345.9074,-46.7736 346.7477,-36.2121 339.5046,-43.9444 345.9074,-46.7736\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"429.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (lazy)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M340.4834,-85.9716C357.2078,-73.2433 379.8019,-56.0478 398.0799,-42.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"400.2997,-44.8461 406.1376,-36.0047 396.0604,-39.2758 400.2997,-44.8461\"/>\n",
       "<text text-anchor=\"middle\" x=\"396.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x119a6f9b0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    secondSentGraph = graphviz.Source(secondSentDepParseTree.to_dot())\n",
    "except:\n",
    "    secondSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "secondSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"997pt\" height=\"560pt\"\n",
       " viewBox=\"0.00 0.00 996.96 560.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 556)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-556 992.9575,-556 992.9575,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-515.7616C235.3833,-504.3597 235.3833,-489.4342 235.3833,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-476.2121 235.3833,-466.2121 231.8834,-476.2121 238.8834,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"246.6592\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"77.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5259,-429.8504C183.9641,-424.3453 171.399,-418.1273 160.0454,-412 144.9107,-403.8322 128.6384,-394.1858 114.5678,-385.5556\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.0198,-382.3384 105.6735,-380.0569 112.3388,-388.2925 116.0198,-382.3384\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5522\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"161.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.6898,-429.7616C209.1921,-417.5615 195.2231,-401.3273 183.5899,-387.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.2297,-385.5094 177.0542,-380.2121 180.9236,-390.0751 186.2297,-385.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.4902\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-429.7616C235.3833,-418.3597 235.3833,-403.4342 235.3833,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-390.2121 235.3833,-380.2121 231.8834,-390.2121 238.8834,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"319.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M253.1975,-429.7616C265.2253,-417.4475 281.2673,-401.0235 294.5454,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.1111,-389.8115 301.5947,-380.2121 292.1035,-384.9203 297.1111,-389.8115\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"421.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M281.035,-436.267C301.523,-430.2297 325.6866,-422.0184 346.3833,-412 361.1468,-404.8536 376.3934,-395.1776 389.2526,-386.2414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.6132,-388.8578 397.7385,-380.213 387.5592,-383.1511 391.6132,-388.8578\"/>\n",
       "<text text-anchor=\"middle\" x=\"386.3281\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"146.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.7486,-343.7616C64.8803,-332.1316 58.4773,-316.8357 52.987,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.097,-302.085 49.007,-294.2121 49.6399,-304.788 56.097,-302.085\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.7905,-343.9005C112.4839,-338.7035 119.1771,-332.628 124.3833,-326 129.5733,-319.3927 133.8059,-311.3704 137.1222,-303.7037\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.4118,-304.9025 140.8359,-294.3161 133.9027,-302.3275 140.4118,-304.9025\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"274.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M376.5212,-344.6694C364.1537,-339.2688 350.9417,-332.9006 339.2935,-326 326.8103,-318.6047 313.97,-309.165 303.0389,-300.4839\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"305.0617,-297.6182 295.0895,-294.0398 300.6536,-303.0559 305.0617,-297.6182\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.4282\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"360.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M391.2568,-343.6347C384.9071,-338.5898 378.7774,-332.6501 374.2798,-326 369.9075,-319.5352 366.903,-311.7204 364.8413,-304.2096\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"368.2186,-303.2775 362.5744,-294.3112 361.3952,-304.8402 368.2186,-303.2775\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"456.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M428.8059,-343.7616C433.539,-332.1316 439.7641,-316.8357 445.1019,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"448.4436,-304.7938 448.9714,-294.2121 441.96,-302.1551 448.4436,-304.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.7144\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"549.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M450.562,-343.9652C459.5457,-338.3073 469.4264,-331.9769 478.3833,-326 490.7955,-317.7174 504.223,-308.3654 516.0254,-300.0089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"518.1726,-302.7768 524.2944,-294.1295 514.1163,-297.0718 518.1726,-302.7768\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.9214\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"341.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1602,-260.9769C513.2216,-259.8878 510.2679,-258.8756 507.3833,-258 465.6572,-245.3346 451.0323,-257.1374 410.9351,-240 396.1578,-233.6843 381.463,-223.7913 369.4142,-214.4665\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"371.5209,-211.6696 361.5269,-208.1526 367.1463,-217.1344 371.5209,-211.6696\"/>\n",
       "<text text-anchor=\"middle\" x=\"438.6074\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"423.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1838,-257.9822C506.6151,-252.48 496.2767,-246.2305 487.0659,-240 475.5821,-232.2319 463.494,-223.0275 452.972,-214.6466\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.892,-211.6986 444.9108,-208.1451 450.4974,-217.1473 454.892,-211.6986\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.542\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"504.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.2032,-257.8722C534.8316,-252.2105 531.1955,-245.9014 528.0591,-240 524.2785,-232.8864 520.4337,-225.0669 516.9726,-217.7676\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.9328,-215.8341 512.5329,-208.2517 513.5892,-218.7937 519.9328,-215.8341\"/>\n",
       "<text text-anchor=\"middle\" x=\"550.5454\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"594.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M561.88,-257.643C565.455,-252.0813 569.2329,-245.8848 572.3833,-240 576.1375,-232.9872 579.7933,-225.2024 583.0121,-217.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"586.3752,-218.9423 587.0989,-208.3722 579.9415,-216.1839 586.3752,-218.9423\"/>\n",
       "<text text-anchor=\"middle\" x=\"601.9351\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"707.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.5441,-262.0972C596.8621,-255.7864 613.6686,-247.961 628.3833,-240 643.1389,-232.0169 658.8574,-222.3295 672.3559,-213.6153\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"674.4115,-216.4533 680.8734,-208.0572 670.586,-210.5911 674.4115,-216.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.6464,-267.6451C610.6488,-260.5344 651.722,-249.924 687.3833,-240 726.7862,-229.0348 771.1175,-215.8883 804.4873,-205.8282\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"805.9136,-209.0536 814.474,-202.811 803.889,-202.3527 805.9136,-209.0536\"/>\n",
       "<text text-anchor=\"middle\" x=\"750.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"391.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M472.3833,-85.7616C472.3833,-74.3597 472.3833,-59.4342 472.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.8834,-46.2121 472.3833,-36.2121 468.8834,-46.2121 475.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.2729\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M416.5969,-171.7616C412.2695,-160.1316 406.578,-144.8357 401.6977,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"404.9276,-130.3637 398.1599,-122.2121 398.367,-132.8049 404.9276,-130.3637\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M433.7749,-171.7616C440.5313,-159.9036 449.459,-144.2345 457.0275,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"460.0971,-132.6334 462.0066,-122.2121 454.0151,-129.1681 460.0971,-132.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.7178\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"567.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M668.1869,-171.9496C657.328,-166.5419 645.6763,-160.3501 635.2935,-154 622.6297,-146.2549 609.3446,-136.8017 597.9168,-128.2084\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.0333,-125.4208 589.9593,-122.1398 595.7884,-130.9869 600.0333,-125.4208\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"655.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M684.2392,-171.951C678.6851,-166.6739 673.2261,-160.5473 669.2798,-154 665.2509,-147.3157 662.3575,-139.4301 660.2923,-131.9215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.6687,-130.9902 657.9608,-122.0643 656.8567,-132.6015 663.6687,-130.9902\"/>\n",
       "<text text-anchor=\"middle\" x=\"698.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M719.2595,-171.7616C726.981,-159.9036 737.1841,-144.2345 745.8338,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"749.0005,-132.502 751.5243,-122.2121 743.1345,-128.6822 749.0005,-132.502\"/>\n",
       "<text text-anchor=\"middle\" x=\"755.7144\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M856.3833,-171.7616C856.3833,-160.3597 856.3833,-145.4342 856.3833,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.8834,-132.2121 856.3833,-122.2121 852.8834,-132.2121 859.8834,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"945.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M875.2579,-171.7616C888.0016,-159.4475 904.9985,-143.0235 919.067,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"921.7767,-131.6779 926.5359,-122.2121 916.9125,-126.644 921.7767,-131.6779\"/>\n",
       "<text text-anchor=\"middle\" x=\"934.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"658.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M741.3719,-85.9716C725.9715,-73.358 705.2148,-56.3573 688.313,-42.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"690.3198,-39.6334 680.3658,-36.0047 685.8843,-45.0488 690.3198,-39.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"725.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M763.3833,-85.7616C763.3833,-74.3597 763.3833,-59.4342 763.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"766.8834,-46.2121 763.3833,-36.2121 759.8834,-46.2121 766.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"879.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M798.3154,-85.9914C807.713,-80.629 817.6789,-74.4499 826.3833,-68 836.2881,-60.6606 846.3323,-51.6634 854.9744,-43.3341\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"857.5027,-45.7565 862.1694,-36.2448 852.5896,-40.7703 857.5027,-45.7565\"/>\n",
       "<text text-anchor=\"middle\" x=\"873.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11302a278>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(depParses[3])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a dependency parse on the reddit sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topPostDepParse = list(stanford.depParser.parse_sents(redditTopScores['sentences'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a few seconds, but now lets look at the parse tree from one of the processed sentences.\n",
    "\n",
    "The sentence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So anyway , I get a call from an older gentleman who 's quite bitter and mean right off the bat ( does n't like that I asked for his address / telephone number to verify the account , hates that he has to speak with a machine before reaching an agent , etc . ) .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 7\n",
    "print(' '.join(redditTopScores['sentences'][0][targetSentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leads to a very rich dependancy tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1113pt\" height=\"818pt\"\n",
       " viewBox=\"0.00 0.00 1112.57 818.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 814)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-814 1108.5708,-814 1108.5708,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-787.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-701.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (get)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M197,-773.7616C197,-762.3597 197,-747.4342 197,-734.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.5001,-734.2121 197,-724.2121 193.5001,-734.2121 200.5001,-734.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.2759\" y=\"-744.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (So)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.8559,-700.0143C147.0611,-694.3052 114.1086,-684.4249 87.8965,-670 75.603,-663.2347 63.384,-653.8632 53.1537,-645.0603\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"55.2127,-642.2079 45.4121,-638.1831 50.5637,-647.4412 55.2127,-642.2079\"/>\n",
       "<text text-anchor=\"middle\" x=\"110.5518\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (anyway)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.7699,-688.3776C162.2887,-682.9299 154.4412,-676.6103 147.8965,-670 140.9396,-662.9735 134.3222,-654.4941 128.7076,-646.5301\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"131.5534,-644.4907 123.0402,-638.1838 125.7623,-648.423 131.5534,-644.4907\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5518\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (I)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M197,-687.7616C197,-676.3597 197,-661.4342 197,-648.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.5001,-648.2121 197,-638.2121 193.5001,-648.2121 200.5001,-648.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"212.1689\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (call)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M214.6382,-687.9935C220.0064,-682.3367 225.8468,-675.9998 231,-670 237.4265,-662.5177 244.1316,-654.1485 250.1331,-646.4296\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"253.0006,-648.4423 256.3228,-638.3818 247.4519,-644.1747 253.0006,-648.4423\"/>\n",
       "<text text-anchor=\"middle\" x=\"255.4448\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"643\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (number)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;34 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M224.2279,-700.7498C296.5806,-686.7984 494.6077,-648.6138 590.1104,-630.1985\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"590.9741,-633.5965 600.1305,-628.2663 589.6487,-626.7231 590.9741,-633.5965\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.1069\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"175\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (a)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M249.8529,-601.7616C236.1242,-589.3335 217.771,-572.719 202.6742,-559.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"204.8805,-556.3286 195.118,-552.2121 200.1826,-561.518 204.8805,-556.3286\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.5518\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (gentleman)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;11 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M270,-601.7616C270,-590.3597 270,-575.4342 270,-562.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.5001,-562.2121 270,-552.2121 266.5001,-562.2121 273.5001,-562.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.9448\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"569\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (like)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;25 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>34&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M627.3065,-601.7616C616.8088,-589.5615 602.8398,-573.3273 591.2066,-559.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"593.8464,-557.5094 584.6709,-552.2121 588.5403,-562.0751 593.8464,-557.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"621.1069\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"668\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (telephone)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;33 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>34&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M648.3019,-601.7616C651.6495,-590.2456 656.0421,-575.1353 659.8295,-562.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.2752,-562.7916 662.7058,-552.2121 656.5534,-560.8376 663.2752,-562.7916\"/>\n",
       "<text text-anchor=\"middle\" x=\"686.5518\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"780\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (verify)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;36 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>34&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M685.5984,-602.4626C697.0839,-597.1021 709.2884,-590.8061 720,-584 731.5591,-576.6554 743.3366,-567.3235 753.361,-558.7155\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"755.7821,-561.2474 760.9868,-552.019 751.1633,-555.9874 755.7821,-561.2474\"/>\n",
       "<text text-anchor=\"middle\" x=\"748.1587\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>11&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M231.7907,-515.847C220.4724,-510.291 208.113,-504.0465 196.9102,-498 181.4511,-489.6562 164.732,-479.9612 150.251,-471.3415\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"151.9329,-468.2691 141.5551,-466.1352 148.3371,-474.275 151.9329,-468.2691\"/>\n",
       "<text text-anchor=\"middle\" x=\"209.0449\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (an)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M252.822,-515.7616C241.3312,-503.5615 226.0409,-487.3273 213.3072,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.5574,-471.0919 206.1533,-466.2121 210.4617,-475.8913 215.5574,-471.0919\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.5518\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (older)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M270,-515.7616C270,-504.3597 270,-489.4342 270,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.5001,-476.2121 270,-466.2121 266.5001,-476.2121 273.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.5518\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"360\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (bitter)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M289.0867,-515.7616C301.9735,-503.4475 319.1614,-487.0235 333.388,-473.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"336.1289,-475.6512 340.9408,-466.2121 331.2929,-470.5902 336.1289,-475.6512\"/>\n",
       "<text text-anchor=\"middle\" x=\"343.5381\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"160\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (who)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>15&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M323.7668,-433.1476C320.814,-432.047 317.8649,-430.9839 315,-430 287.7259,-420.6334 279.3566,-422.9093 252.6621,-412 234.2444,-404.4732 214.7375,-394.3082 198.3969,-385.109\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.0902,-382.0455 189.6716,-380.1206 196.6158,-388.1224 200.0902,-382.0455\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.1689\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"238\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>15&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M332.2421,-429.9468C323.6973,-424.2881 314.3008,-417.9619 305.7861,-412 294.0448,-403.7789 281.3575,-394.5234 270.1697,-386.2309\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"272.0587,-383.2738 261.9472,-380.1103 267.8789,-388.8889 272.0587,-383.2738\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.1069\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"318\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (quite)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>15&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M346.5824,-429.8451C342.8269,-424.2891 338.9433,-418.045 335.8965,-412 332.3589,-404.9812 329.1928,-397.0923 326.543,-389.6912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"329.7918,-388.373 323.2634,-380.0286 323.1632,-390.6229 329.7918,-388.373\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.5518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"403\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (and)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M371.0917,-429.9088C374.4154,-424.2486 377.9776,-417.9311 381,-412 384.6116,-404.9127 388.2182,-397.1023 391.4354,-389.8033\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"394.7951,-390.8514 395.5451,-380.2831 388.3684,-388.0771 394.7951,-390.8514\"/>\n",
       "<text text-anchor=\"middle\" x=\"394.2139\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"489\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (mean)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M387.0426,-429.9716C406.393,-417.0713 432.6266,-399.5822 453.6376,-385.575\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.6139,-388.4639 461.993,-380.0047 451.731,-382.6395 455.6139,-388.4639\"/>\n",
       "<text text-anchor=\"middle\" x=\"445.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"451\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (right)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M470.6407,-343.8974C466.1437,-338.5346 461.7952,-332.3788 458.8965,-326 455.8322,-319.2569 453.9297,-311.5343 452.7561,-304.2081\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"456.2273,-303.7587 451.5373,-294.2585 449.2792,-304.6099 456.2273,-303.7587\"/>\n",
       "<text text-anchor=\"middle\" x=\"481.5518\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"534\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (bat)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M498.5433,-343.7616C504.6885,-332.0176 512.7896,-316.5355 519.6969,-303.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"522.9353,-304.6951 524.4704,-294.2121 516.733,-301.4497 522.9353,-304.6951\"/>\n",
       "<text text-anchor=\"middle\" x=\"530.9448\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"478\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (off)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>21&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M522.1238,-257.7616C514.4023,-245.9036 504.1992,-230.2345 495.5495,-216.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"498.2488,-214.6822 489.859,-208.2121 492.3828,-218.502 498.2488,-214.6822\"/>\n",
       "<text text-anchor=\"middle\" x=\"522.0449\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"555\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">20 (the)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;20 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>21&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.4536,-257.7616C541.2656,-246.2456 544.9553,-231.1353 548.1368,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"551.5807,-218.757 550.5529,-208.2121 544.7805,-217.0964 551.5807,-218.757\"/>\n",
       "<text text-anchor=\"middle\" x=\"554.5518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"489\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (does)</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"569\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (n&#39;t)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>25&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M552.0341,-515.7616C540.6851,-503.5615 525.5836,-487.3273 513.0071,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"515.3152,-471.1501 505.9415,-466.2121 510.1899,-475.9179 515.3152,-471.1501\"/>\n",
       "<text text-anchor=\"middle\" x=\"544.1069\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;24 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>25&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M569,-515.7616C569,-504.3597 569,-489.4342 569,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"572.5001,-476.2121 569,-466.2121 565.5001,-476.2121 572.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"579.1069\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">neg</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"652\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (asked)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;28 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>25&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M586.6022,-515.7616C598.4867,-503.4475 614.3377,-487.0235 627.4578,-473.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"629.9971,-475.8382 634.4232,-466.2121 624.9603,-470.977 629.9971,-475.8382\"/>\n",
       "<text text-anchor=\"middle\" x=\"634.6587\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"575\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (that)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M635.2812,-429.7312C630.1192,-424.0637 624.4271,-417.7868 619.2344,-412 612.1517,-404.1071 604.4811,-395.4688 597.5476,-387.6264\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.1509,-385.2867 590.9089,-380.1063 594.9032,-389.9194 600.1509,-385.2867\"/>\n",
       "<text text-anchor=\"middle\" x=\"634.3828\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"652\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">27 (I)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M652,-429.7616C652,-418.3597 652,-403.4342 652,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"655.5001,-390.2121 652,-380.2121 648.5001,-390.2121 655.5001,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.1689\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"739\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (address)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;31 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>28&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M670.4505,-429.7616C682.9077,-417.4475 699.5227,-401.0235 713.275,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"715.9248,-389.7314 720.5761,-380.2121 711.0037,-384.7531 715.9248,-389.7314\"/>\n",
       "<text text-anchor=\"middle\" x=\"716.9448\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"662\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (for)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M722.6703,-343.7616C711.747,-331.5615 697.2117,-315.3273 685.1068,-301.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"687.5843,-299.3276 678.3062,-294.2121 682.3691,-303.9969 687.5843,-299.3276\"/>\n",
       "<text text-anchor=\"middle\" x=\"718.0449\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"739\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (his)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M739,-343.7616C739,-332.3597 739,-317.4342 739,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"742.5001,-304.2121 739,-294.2121 735.5001,-304.2121 742.5001,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"769.3379\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"780\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">35 (to)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;35 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>36&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M780,-515.7616C780,-504.3597 780,-489.4342 780,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"783.5001,-476.2121 780,-466.2121 776.5001,-476.2121 783.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"794.3828\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"883\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">38 (account)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M801.8437,-515.7616C816.8651,-503.2195 836.9927,-486.4138 853.445,-472.677\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"855.7549,-475.3079 861.1878,-466.2121 851.2685,-469.9347 855.7549,-475.3079\"/>\n",
       "<text text-anchor=\"middle\" x=\"850.4448\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>37</title>\n",
       "<text text-anchor=\"middle\" x=\"829\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">37 (the)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;37 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>38&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M871.548,-429.7616C864.1023,-417.9036 854.2635,-402.2345 845.9227,-388.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"848.7174,-386.8198 840.4355,-380.2121 842.7891,-390.5422 848.7174,-386.8198\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.5518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"913\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">40 (hates)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;40 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>38&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M889.3622,-429.7616C893.4192,-418.1316 898.755,-402.8357 903.3302,-389.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"906.6579,-390.8069 906.6469,-380.2121 900.0485,-388.5013 906.6579,-390.8069\"/>\n",
       "<text text-anchor=\"middle\" x=\"912.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"996\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">54 (etc)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;54 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>38&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M906.6885,-429.9716C923.4129,-417.2433 946.007,-400.0478 964.285,-386.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"966.5048,-388.8461 972.3427,-380.0047 962.2654,-383.2758 966.5048,-388.8461\"/>\n",
       "<text text-anchor=\"middle\" x=\"959.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"817\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">43 (has)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;43 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>40&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M892.6409,-343.7616C878.7676,-331.3335 860.2213,-314.719 844.9655,-301.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"847.1135,-298.2777 837.3298,-294.2121 842.4428,-303.4915 847.1135,-298.2777\"/>\n",
       "<text text-anchor=\"middle\" x=\"889.6587\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>50</title>\n",
       "<text text-anchor=\"middle\" x=\"974\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">50 (reaching)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;50 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>40&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M925.9365,-343.7616C934.4283,-331.7896 945.6756,-315.9328 955.1549,-302.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"958.1514,-304.3935 961.0821,-294.2121 952.4419,-300.3437 958.1514,-304.3935\"/>\n",
       "<text text-anchor=\"middle\" x=\"963.1587\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>41</title>\n",
       "<text text-anchor=\"middle\" x=\"718\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">41 (that)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;41 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>43&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M794.5653,-257.8876C787.6662,-252.2266 780.0866,-245.9139 773.2344,-240 763.9189,-231.9602 753.8992,-222.991 744.9852,-214.8934\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"747.228,-212.2018 737.4818,-208.047 742.5098,-217.3728 747.228,-212.2018\"/>\n",
       "<text text-anchor=\"middle\" x=\"788.3828\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>42</title>\n",
       "<text text-anchor=\"middle\" x=\"796\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">42 (he)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;42 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>43&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M812.5464,-257.7616C809.7344,-246.2456 806.0447,-231.1353 802.8632,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"806.2195,-217.0964 800.4471,-208.2121 799.4193,-218.757 806.2195,-217.0964\"/>\n",
       "<text text-anchor=\"middle\" x=\"823.1689\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"879\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">45 (speak)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;45 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>43&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M830.1486,-257.7616C838.7796,-245.7896 850.2112,-229.9328 859.8459,-216.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"862.8614,-218.3707 865.8703,-208.2121 857.1832,-214.2771 862.8614,-218.3707\"/>\n",
       "<text text-anchor=\"middle\" x=\"871.0518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"974\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">49 (before)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;49 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>50&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M974,-257.7616C974,-246.3597 974,-231.4342 974,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"977.5001,-218.2121 974,-208.2121 970.5001,-218.2121 977.5001,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"988.3828\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"1068\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">52 (agent)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M993.935,-257.7616C1007.5192,-245.3335 1025.6792,-228.719 1040.6172,-215.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1043.0782,-217.5446 1048.0938,-208.2121 1038.3531,-212.3799 1043.0782,-217.5446\"/>\n",
       "<text text-anchor=\"middle\" x=\"1039.4448\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"844\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">44 (to)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;44 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>45&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M871.4717,-171.6927C869.1458,-166.0237 866.5793,-159.7555 864.2344,-154 861.2709,-146.7261 858.0772,-138.8513 855.1223,-131.5502\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"858.2784,-130.0188 851.2849,-122.06 851.7888,-132.6429 858.2784,-130.0188\"/>\n",
       "<text text-anchor=\"middle\" x=\"879.3828\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"934\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">48 (machine)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;48 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>45&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M890.6641,-171.7616C898.2477,-159.9036 908.2686,-144.2345 916.7639,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"919.9135,-132.5223 922.3527,-122.2121 914.0163,-128.7509 919.9135,-132.5223\"/>\n",
       "<text text-anchor=\"middle\" x=\"925.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>46</title>\n",
       "<text text-anchor=\"middle\" x=\"894\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">46 (with)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;46 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>48&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M925.517,-85.7616C920.0547,-74.0176 912.8537,-58.5355 906.7139,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"909.8616,-43.8032 902.4707,-36.2121 903.5146,-46.7554 909.8616,-43.8032\"/>\n",
       "<text text-anchor=\"middle\" x=\"929.0449\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"973\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">47 (a)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;47 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>48&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M942.2709,-85.7616C947.5967,-74.0176 954.6176,-58.5355 960.604,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"963.7985,-46.7649 964.741,-36.2121 957.4234,-43.8739 963.7985,-46.7649\"/>\n",
       "<text text-anchor=\"middle\" x=\"964.5518\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"1068\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">51 (an)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;51 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>52&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1068,-171.7616C1068,-160.3597 1068,-145.4342 1068,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1071.5001,-132.2121 1068,-122.2121 1064.5001,-132.2121 1071.5001,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"1076.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11318b668>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(topPostDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('JJ', ['Terrorist']), Tree('NNS', ['groups'])]), Tree(',', [',']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('RB', ['especially']), Tree('DT', ['the']), Tree('JJ', ['Islamic']), Tree('NN', ['State'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('NNP', ['Iraq'])])])]), Tree('CC', ['and']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['Levant'])]), Tree('PRN', [Tree('-LRB-', [Tree('', []), Tree('NP', [Tree('NNP', ['ISIL'])]), Tree('-RRB-', [])])])]), Tree('VP', [Tree('ADVP', [Tree('RB', ['also'])]), Tree('VBN', ['known']), Tree('PP', [Tree('IN', ['as']), Tree('NP', [Tree('NNP', ['ñDaÍeshî'])])])])])]), Tree(',', [','])]), Tree('VP', [Tree('VBP', ['have']), Tree('VP', [Tree('VBN', ['become']), Tree('S', [Tree('ADJP', [Tree('RB', ['increasingly']), Tree('JJ', ['adept']), Tree('PP', [Tree('IN', ['at']), Tree('S', [Tree('VP', [Tree('VBG', ['using']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['Internet'])]), Tree('CC', ['and']), Tree('NP', [Tree('NP', [Tree('NN', ['information']), Tree('CC', ['and']), Tree('NN', ['communications']), Tree('NNS', ['technologies'])]), Tree('PRN', [Tree('-LRB-', [Tree('', []), Tree('NP', [Tree('NNP', ['ICT'])]), Tree('-RRB-', [])])])]), Tree(',', [',']), Tree('ADVP', [Tree('IN', ['along']), Tree('PP', [Tree('IN', ['with']), Tree('NP', [Tree('JJ', ['other']), Tree('NNS', ['tools'])])])])])])])])])]), Tree(',', [',']), Tree('S', [Tree('VP', [Tree('TO', ['to']), Tree('VP', [Tree('VP', [Tree('VB', ['raise']), Tree('NP', [Tree('NN', ['money'])])]), Tree(',', [',']), Tree('VP', [Tree('VB', ['propagate']), Tree('NP', [Tree('PRP$', ['their']), Tree('NNS', ['messages'])])]), Tree(',', [',']), Tree('VP', [Tree('VB', ['recruit']), Tree('NP', [Tree('NNS', ['individuals'])])]), Tree(',', [',']), Tree('CC', ['and']), Tree('VP', [Tree('VB', ['inspire']), Tree('NP', [Tree('JJ', ['violent']), Tree('NNS', ['acts'])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "DOSparses = list(stanford.parser.parse_sents(DOSSpeechesDF['sentences'][0])) #Converting the iterator to a list so we can call by index. They are still \n",
    "DOSfourthSentParseTree = list(DOSparses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(DOSfourthSentParseTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeSubRelation(parsetree, relationTypeScope, relationTypeTarget, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retSet = set()\n",
    "        for subT in parsetree.subtrees():\n",
    "            if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                if subT.label() == relationTypeScope:\n",
    "                    for subsub in subT.subtrees():\n",
    "                        if subsub.label()==relationTypeTarget:\n",
    "                            retSet.add(' '.join(subsub.leaves()))\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeSubRelation(DOSfourthSentParseTree, 'Terrorist', 'Islamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I had a lot of trouble finding words that worked for both the treeRelation and treeSubRelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                 ROOT                                                                                                                                                                                                                          \n",
      "                                                                                                                                                                                                  |                                                                                                                                                                                                                             \n",
      "                                                                                                                                                                                                  S                                                                                                                                                                                                                            \n",
      "                                                                     _____________________________________________________________________________________________________________________________|__________________________________________________________________________________________________________________________________________________________________________________________________________________________   \n",
      "                                                                    |                                                                                                                                      VP                                                                                                                                                                                                                | \n",
      "                                                                    |                                                                   ___________________________________________________________________|_________________________________________________________________                                                                                                                                                |  \n",
      "                                                                    |                                                                  |                                                                                                                                     VP                                                                                                                                              | \n",
      "                                                                    |                                                                  |      _______________________________________________________________________________________________________________________________|_________________________________________________                                                                                              |  \n",
      "                                                                    |                                                                  |     |                        S                                                                                                                                |                       |                                                                                             | \n",
      "                                                                    |                                                                  |     |                        |                                                                                                                                |                       |                                                                                             |  \n",
      "                                                                    |                                                                  |     |                       ADJP                                                                                                                              |                       |                                                                                             | \n",
      "                                                                    |                                                                  |     |          ______________|___________________________                                                                                                     |                       |                                                                                             |  \n",
      "                                                                    |                                                                  |     |         |         |                                PP                                                                                                   |                       |                                                                                             | \n",
      "                                                                    |                                                                  |     |         |         |     ___________________________|_______________________________________                                                             |                       |                                                                                             |  \n",
      "                                                                    NP                                                                 |     |         |         |    |                                                                   S                                                            |                       |                                                                                             | \n",
      "            ________________________________________________________|______________________________________________________________    |     |         |         |    |                                                                   |                                                            |                       |                                                                                             |  \n",
      "           |          |                                                 NP                                                         |   |     |         |         |    |                                                                   VP                                                           |                       |                                                                                             | \n",
      "           |          |                     ____________________________|_________________________                                 |   |     |         |         |    |      _____________________________________________________________|________                                                    |                       |                                                                                             |  \n",
      "           |          |                    |                        |                             NP                               |   |     |         |         |    |     |                                                                      NP                                                  |                       S                                                                                             | \n",
      "           |          |                    |                        |                   __________|__________                      |   |     |         |         |    |     |         _____________________________________________________________|______________________________                     |                       |                                                                                             |  \n",
      "           |          |                    |                        |                  NP                    |                     |   |     |         |         |    |     |        |            |                                       NP                       |              |                    |                       VP                                                                                            | \n",
      "           |          |                    |                        |        __________|____                 |                     |   |     |         |         |    |     |        |            |                 ______________________|_____________           |              |                    |    ___________________|________________________________                                                             |  \n",
      "           |          |                    NP                       |       |              PRN               VP                    |   |     |         |         |    |     |        |            |                |                                   PRN         |             ADVP                  |   |                                                    VP                                                           | \n",
      "           |          |               _____|_______________         |       |               |           _____|________             |   |     |         |         |    |     |        |            |                |                                    |          |     _________|_____               |   |          __________________________________________|_____________________________________________               |  \n",
      "           |          |              |                     PP       |       |             -LRB-        |     |        PP           |   |     |         |         |    |     |        |            |                |                                  -LRB-        |    |               PP             |   |         VP        |              VP                |           VP              |   |             VP             | \n",
      "           |          |              |                  ___|___     |       |           ____|_____     |     |     ___|_____       |   |     |         |         |    |     |        |            |                |                                ____|_____     |    |     __________|____          |   |     ____|____     |       _______|____             |      _____|_______        |   |      _______|_____         |  \n",
      "           NP         |              NP                |       NP   |       NP         |    NP    |   ADVP   |    |         NP     |   |     |         |         |    |     |        NP           |                NP                              |    NP    |    |    |    |               NP        |   |    |         NP   |      |            NP           |     |             NP      |   |     |             NP       | \n",
      "     ______|____      |       _______|____________     |       |    |    ___|____      |    |     |    |     |    |         |      |   |     |         |         |    |     |     ___|_____       |         _______|______________________         |    |     |    |    |    |           ____|____     |   |    |         |    |      |        ____|_____       |     |             |       |   |     |        _____|___     |  \n",
      "    JJ         NNS    ,      RB      DT    JJ     NN   IN     NNP   CC  DT       NN        NNP  -RRB-  RB   VBN   IN       NNP     ,  VBP   VBN        RB        JJ   IN   VBG   DT        NN     CC       NN      CC       NN           NNS           NNP  -RRB-  ,    IN   IN         JJ       NNS   ,   TO   VB        NN   ,      VB     PRP$       NNS     ,     VB           NNS      ,   CC    VB      JJ       NNS   . \n",
      "    |           |     |      |       |     |      |    |       |    |   |        |     |    |     |    |     |    |         |      |   |     |         |         |    |     |    |         |      |        |       |        |             |        |    |     |    |    |    |          |         |    |   |    |         |    |      |       |          |      |     |             |       |   |     |       |         |    |  \n",
      "Terrorist     groups  ,  especially the Islamic State  of     Iraq and the     Levant ...  ISIL  ...  also known  as     ñDaÍeshî  ,  have become increasingly adept  at  using the     Internet and  information and communications technologies ...  ICT   ...   ,  along with      other     tools  ,   to raise     money  ,  propagate their     messages  ,  recruit     individuals  ,  and inspire violent     acts  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOSfourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            ROOT                                                    \n",
      "                                             |                                                       \n",
      "                                             S                                                      \n",
      "  ___________________________________________|____________________________________________________   \n",
      " |                                           VP                                                   | \n",
      " |    _______________________________________|__________________________________________          |  \n",
      " |   |                  ADJP                                                            |         | \n",
      " |   |      _____________|___________________                                           |         |  \n",
      " |   |     |                                 S                                          |         | \n",
      " |   |     |                                 |                                          |         |  \n",
      " |   |     |                                 VP                                         |         | \n",
      " |   |     |      ___________________________|_______                                   |         |  \n",
      " |   |     |     |                                   VP                                 |         | \n",
      " |   |     |     |        ___________________________|_______________                   |         |  \n",
      " |   |     |     |       |           PP                              PP                 |         | \n",
      " |   |     |     |       |        ___|_______                     ___|____              |         |  \n",
      " NP  |     |     |       |       |           NP                  |        NP           ADVP       | \n",
      " |   |     |     |       |       |    _______|_____________      |    ____|____      ___|____     |  \n",
      "PRP VBD    JJ    TO      VB      IN  JJ     NNP     NNP    NN    IN  DT        NN   RB       RB   . \n",
      " |   |     |     |       |       |   |       |       |     |     |   |         |    |        |    |  \n",
      " I  was pleased  to participate  in last DecemberÍs CTC meeting  on this     issue  as      well  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(DOSparses[1])[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dependency parsing and graph representations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrorist groups , especially the Islamic State of Iraq and the Levant ( ISIL ) also known as ñDaÍeshî , have become increasingly adept at using the Internet and information and communications technologies ( ICT ) , along with other tools , to raise money , propagate their messages , recruit individuals , and inspire violent acts .\n"
     ]
    }
   ],
   "source": [
    "DOSdepParses = list(stanford.depParser.parse_sents(DOSSpeechesDF['sentences'][0])) #Converting the iterator to a list so we can call by index. They are still \n",
    "targetSentence = 3\n",
    "print(' '.join(DOSSpeechesDF['sentences'][0][targetSentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1357pt\" height=\"646pt\"\n",
       " viewBox=\"0.00 0.00 1357.42 646.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 642)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-642 1353.4175,-642 1353.4175,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"579.8984\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"579.8984\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (become)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;22 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M579.8984,-601.7616C579.8984,-590.3597 579.8984,-575.4342 579.8984,-562.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"583.3985,-562.2121 579.8984,-552.2121 576.3985,-562.2121 583.3985,-562.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"591.1743\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"306.8984\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (groups)</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;2 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>22&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M536.6408,-520.373C486.8847,-504.699 405.4668,-479.0508 354.1044,-462.8707\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"354.8405,-459.4331 344.251,-459.7667 352.7373,-466.1097 354.8405,-459.4331\"/>\n",
       "<text text-anchor=\"middle\" x=\"475.0674\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"558.8984\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (have)</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>22&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M575.4449,-515.7616C572.6328,-504.2456 568.9431,-489.1353 565.7616,-476.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"569.1179,-475.0964 563.3456,-466.2121 562.3177,-476.757 569.1179,-475.0964\"/>\n",
       "<text text-anchor=\"middle\" x=\"581.0054\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"647.8984\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (adept)</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;24 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>22&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M594.3195,-515.7616C603.8759,-503.6756 616.5628,-487.6304 627.1921,-474.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"630.0412,-476.2271 633.4982,-466.2121 624.5503,-471.8854 630.0412,-476.2271\"/>\n",
       "<text text-anchor=\"middle\" x=\"637.9502\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"1022.8984\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">44 (raise)</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;44 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>22&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M623.1817,-525.5974C707.4937,-509.2298 892.9367,-473.2296 978.2935,-456.6592\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"979.0435,-460.079 988.1932,-454.7374 977.7095,-453.2073 979.0435,-460.079\"/>\n",
       "<text text-anchor=\"middle\" x=\"847.9502\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"214.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Terrorist)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M287.3876,-429.7616C274.0924,-417.3335 256.3188,-400.719 241.6987,-387.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.0765,-384.4841 234.3811,-380.2121 239.2963,-389.5978 244.0765,-384.4841\"/>\n",
       "<text text-anchor=\"middle\" x=\"282.4502\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"306.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (State)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;7 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M306.8984,-429.7616C306.8984,-418.3597 306.8984,-403.4342 306.8984,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.3985,-390.2121 306.8984,-380.2121 303.3985,-390.2121 310.3985,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"323.2295\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"45.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (especially)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M274.9462,-347.1008C271.9227,-345.9563 268.8752,-344.8989 265.8984,-344 221.3676,-330.553 207.3892,-339.2352 162.7949,-326 138.5555,-318.806 112.6036,-308.0391 91.3248,-298.3372\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"92.751,-295.1406 82.2059,-294.115 89.8098,-301.4927 92.751,-295.1406\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.4502\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"136.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (the)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M274.9227,-345.8241C245.897,-331.1405 203.1365,-309.5087 172.9161,-294.2207\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.4722,-291.0856 163.969,-289.6945 171.3123,-297.3318 174.4722,-291.0856\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.4502\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"220.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (Islamic)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M288.66,-343.7616C276.3459,-331.4475 259.922,-315.0235 246.3277,-301.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"248.6565,-298.8083 239.1105,-294.2121 243.7067,-303.7581 248.6565,-298.8083\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.4502\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"306.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Iraq)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M306.8984,-343.7616C306.8984,-332.3597 306.8984,-317.4342 306.8984,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.3985,-304.2121 306.8984,-294.2121 303.3985,-304.2121 310.3985,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"322.8433\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"385.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (and)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M325.4834,-343.9468C331.1637,-338.2881 337.3684,-331.9619 342.8984,-326 349.8742,-318.4794 357.2341,-310.0988 363.8547,-302.3811\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"366.8822,-304.2238 370.6956,-294.339 361.5503,-299.6883 366.8822,-304.2238\"/>\n",
       "<text text-anchor=\"middle\" x=\"362.1123\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"475.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (Levant)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;12 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M339.0838,-345.6216C365.1817,-332.3411 402.2558,-313.4749 431.2291,-298.7311\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"433.0022,-301.756 440.3272,-294.1013 429.8274,-295.5173 433.0022,-301.756\"/>\n",
       "<text text-anchor=\"middle\" x=\"413.9502\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"306.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (of)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M306.8984,-257.7616C306.8984,-246.3597 306.8984,-231.4342 306.8984,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.3985,-218.2121 306.8984,-208.2121 303.3985,-218.2121 310.3985,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.9434\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"387.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (the)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>12&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M457.2359,-257.7616C444.6354,-245.4475 427.8295,-229.0235 413.9191,-215.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"416.1322,-212.6983 406.5341,-208.2121 411.2397,-217.7046 416.1322,-212.6983\"/>\n",
       "<text text-anchor=\"middle\" x=\"446.4502\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"469.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (ISIL)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M474.626,-257.7616C473.8305,-246.3597 472.7892,-231.4342 471.8864,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.3566,-217.9442 471.169,-208.2121 468.3736,-218.4315 475.3566,-217.9442\"/>\n",
       "<text text-anchor=\"middle\" x=\"490.2295\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"562.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (known)</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M494.3489,-257.7616C506.8062,-245.4475 523.4211,-229.0235 537.1735,-215.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"539.8233,-217.7314 544.4746,-208.2121 534.9022,-212.7531 539.8233,-217.7314\"/>\n",
       "<text text-anchor=\"middle\" x=\"533.0571\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"478.8984\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (also)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>17&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M539.3713,-171.7157C532.8101,-166.2575 525.8307,-160.098 519.7949,-154 512.5346,-146.6648 505.209,-138.1416 498.8034,-130.2327\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.4588,-127.9483 492.5024,-122.2885 495.9745,-132.2983 501.4588,-127.9483\"/>\n",
       "<text text-anchor=\"middle\" x=\"542.4502\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"575.8984\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (ñDaÍeshî)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;19 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>17&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M565.6554,-171.7616C567.3962,-160.2456 569.6803,-145.1353 571.6498,-132.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"575.1114,-132.6229 573.1454,-122.2121 568.19,-131.5766 575.1114,-132.6229\"/>\n",
       "<text text-anchor=\"middle\" x=\"586.8433\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"575.8984\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (as)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M575.8984,-85.7616C575.8984,-74.3597 575.8984,-59.4342 575.8984,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"579.3985,-46.2121 575.8984,-36.2121 572.3985,-46.2121 579.3985,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"587.9434\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"602.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (increasingly)</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;23 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>24&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M635.3339,-429.6521C631.7425,-424.0907 627.9503,-417.892 624.7949,-412 621.0425,-404.9931 617.3985,-397.2104 614.1946,-389.9123\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"617.2716,-388.2051 610.1296,-380.3793 610.8325,-390.9508 617.2716,-388.2051\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.4502\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"712.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (using)</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;26 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>24&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M661.6833,-429.7616C670.7319,-417.7896 682.7167,-401.9328 692.8176,-388.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"695.896,-390.3002 699.1335,-380.2121 690.3116,-386.0794 695.896,-390.3002\"/>\n",
       "<text text-anchor=\"middle\" x=\"700.0571\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"830.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">43 (to)</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;43 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>44&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M988.1751,-435.2078C970.8818,-428.6275 949.7445,-420.2673 931.1328,-412 904.4891,-400.1648 896.7289,-395.1297 867.0189,-380.0891\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"868.5298,-376.9314 858.0231,-375.5676 865.3861,-383.1858 868.5298,-376.9314\"/>\n",
       "<text text-anchor=\"middle\" x=\"946.2813\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"915.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">45 (money)</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;45 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>44&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1000.4677,-429.9716C984.7741,-417.358 963.622,-400.3573 946.3982,-386.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"948.2867,-383.5414 938.2996,-380.0047 943.9014,-388.9975 948.2867,-383.5414\"/>\n",
       "<text text-anchor=\"middle\" x=\"988.3433\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"1022.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">47 (propagate)</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;47 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>44&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1022.8984,-429.7616C1022.8984,-418.3597 1022.8984,-403.4342 1022.8984,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1026.3985,-390.2121 1022.8984,-380.2121 1019.3985,-390.2121 1026.3985,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"1034.9502\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"1129.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">51 (recruit)</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;51 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>44&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1045.3291,-429.9716C1061.0228,-417.358 1082.1749,-400.3573 1099.3986,-386.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1101.8954,-388.9975 1107.4973,-380.0047 1097.5101,-383.5414 1101.8954,-388.9975\"/>\n",
       "<text text-anchor=\"middle\" x=\"1094.9502\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 55 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>55</title>\n",
       "<text text-anchor=\"middle\" x=\"1227.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">55 (inspire)</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;55 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>44&#45;&gt;55</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1057.6402,-433.9325C1073.8485,-427.3323 1093.3854,-419.3238 1110.8984,-412 1132.9893,-402.7618 1157.3497,-392.3824 1178.0968,-383.4845\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1179.5459,-386.6714 1187.3539,-379.51 1176.7842,-380.2392 1179.5459,-386.6714\"/>\n",
       "<text text-anchor=\"middle\" x=\"1155.9502\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"1317.8984\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">54 (and)</text>\n",
       "</g>\n",
       "<!-- 44&#45;&gt;54 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>44&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1057.6309,-440.8594C1107.1441,-430.1979 1200.4869,-408.3048 1276.8984,-380 1276.9941,-379.9646 1277.0899,-379.929 1277.1858,-379.8933\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1278.2367,-383.2409 1286.2655,-376.3279 1275.6781,-376.7252 1278.2367,-383.2409\"/>\n",
       "<text text-anchor=\"middle\" x=\"1229.1123\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"694.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (at)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M709.0811,-343.7616C706.6708,-332.2456 703.5082,-317.1353 700.7812,-304.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"704.1848,-303.283 698.7103,-294.2121 697.3332,-304.7171 704.1848,-303.283\"/>\n",
       "<text text-anchor=\"middle\" x=\"720.2813\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"782.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (Internet)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;28 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>26&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M727.7436,-343.7616C737.5811,-331.6756 750.6411,-315.6304 761.5831,-302.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"764.4764,-304.1772 768.0746,-294.2121 759.0474,-299.7583 764.4764,-304.1772\"/>\n",
       "<text text-anchor=\"middle\" x=\"765.3433\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"650.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">27 (the)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M755.2269,-257.9716C735.4265,-245.0713 708.5828,-227.5822 687.0833,-213.575\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"688.8228,-210.531 678.5335,-208.0047 685.0016,-216.3961 688.8228,-210.531\"/>\n",
       "<text text-anchor=\"middle\" x=\"733.4502\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"729.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (and)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>28&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M771.6585,-257.7616C764.3506,-245.9036 754.6941,-230.2345 746.5078,-216.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"749.3483,-214.889 741.1222,-208.2121 743.3891,-218.5616 749.3483,-214.889\"/>\n",
       "<text text-anchor=\"middle\" x=\"766.1123\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"835.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (technologies)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;33 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>28&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M794.1384,-257.7616C801.4462,-245.9036 811.1028,-230.2345 819.2891,-216.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"822.4078,-218.5616 824.6747,-208.2121 816.4485,-214.889 822.4078,-218.5616\"/>\n",
       "<text text-anchor=\"middle\" x=\"824.9502\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"947.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">38 (along)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;38 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>28&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M817.4878,-257.9716C842.7884,-244.7846 877.2887,-226.8027 904.449,-212.6464\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"906.1045,-215.7304 913.3546,-208.0047 902.8691,-209.523 906.1045,-215.7304\"/>\n",
       "<text text-anchor=\"middle\" x=\"898.4502\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"743.8984\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (information)</text>\n",
       "</g>\n",
       "<!-- 33&#45;&gt;30 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>33&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M802.2538,-171.9092C794.1048,-166.7121 785.7475,-160.6343 778.7949,-154 771.7085,-147.2379 765.1857,-138.8319 759.745,-130.8537\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"762.6751,-128.9392 754.2904,-122.4625 756.8061,-132.7543 762.6751,-128.9392\"/>\n",
       "<text text-anchor=\"middle\" x=\"808.4502\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"848.8984\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">35 (ICT)</text>\n",
       "</g>\n",
       "<!-- 33&#45;&gt;35 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>33&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M838.6554,-171.7616C840.3962,-160.2456 842.6803,-145.1353 844.6498,-132.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"848.1114,-132.6229 846.1454,-122.2121 841.19,-131.5766 848.1114,-132.6229\"/>\n",
       "<text text-anchor=\"middle\" x=\"860.2295\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>41</title>\n",
       "<text text-anchor=\"middle\" x=\"947.8984\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">41 (tools)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;41 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>38&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M947.8984,-171.7616C947.8984,-160.3597 947.8984,-145.4342 947.8984,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"951.3985,-132.2121 947.8984,-122.2121 944.3985,-132.2121 951.3985,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"963.8433\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"687.8984\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (and)</text>\n",
       "</g>\n",
       "<!-- 30&#45;&gt;31 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>30&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M732.0223,-85.7616C724.3008,-73.9036 714.0976,-58.2345 705.448,-44.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"708.1473,-42.6822 699.7575,-36.2121 702.2813,-46.502 708.1473,-42.6822\"/>\n",
       "<text text-anchor=\"middle\" x=\"726.1123\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 32 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>32</title>\n",
       "<text text-anchor=\"middle\" x=\"804.8984\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">32 (communications)</text>\n",
       "</g>\n",
       "<!-- 30&#45;&gt;32 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>30&#45;&gt;32</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M756.835,-85.7616C765.3268,-73.7896 776.574,-57.9328 786.0533,-44.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"789.0499,-46.3935 791.9806,-36.2121 783.3403,-42.3437 789.0499,-46.3935\"/>\n",
       "<text text-anchor=\"middle\" x=\"790.9502\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 39 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>39</title>\n",
       "<text text-anchor=\"middle\" x=\"942.8984\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">39 (with)</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;39 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>41&#45;&gt;39</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M946.8381,-85.7616C946.1752,-74.3597 945.3074,-59.4342 944.5551,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"948.0319,-45.9921 943.9573,-36.2121 941.0437,-46.3984 948.0319,-45.9921\"/>\n",
       "<text text-anchor=\"middle\" x=\"957.9434\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"1030.8984\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">40 (other)</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;40 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>41&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M965.5006,-85.7616C977.3851,-73.4475 993.2362,-57.0235 1006.3562,-43.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1008.8956,-45.8382 1013.3216,-36.2121 1003.8587,-40.977 1008.8956,-45.8382\"/>\n",
       "<text text-anchor=\"middle\" x=\"1010.4502\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"1014.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">49 (messages)</text>\n",
       "</g>\n",
       "<!-- 47&#45;&gt;49 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>47&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1021.2018,-343.7616C1020.1412,-332.3597 1018.7528,-317.4342 1017.549,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1021.0039,-303.8449 1016.5926,-294.2121 1014.034,-304.4933 1021.0039,-303.8449\"/>\n",
       "<text text-anchor=\"middle\" x=\"1032.3433\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"1132.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">52 (individuals)</text>\n",
       "</g>\n",
       "<!-- 51&#45;&gt;52 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>51&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1130.5347,-343.7616C1130.9324,-332.3597 1131.4531,-317.4342 1131.9045,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1135.4123,-304.3281 1132.2631,-294.2121 1128.4165,-304.084 1135.4123,-304.3281\"/>\n",
       "<text text-anchor=\"middle\" x=\"1144.3433\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 57 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>57</title>\n",
       "<text text-anchor=\"middle\" x=\"1235.8984\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">57 (acts)</text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;57 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>55&#45;&gt;57</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1229.595,-343.7616C1230.6557,-332.3597 1232.0441,-317.4342 1233.2478,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1236.7629,-304.4933 1234.2043,-294.2121 1229.793,-303.8449 1236.7629,-304.4933\"/>\n",
       "<text text-anchor=\"middle\" x=\"1245.3433\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"1036.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">48 (their)</text>\n",
       "</g>\n",
       "<!-- 49&#45;&gt;48 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>49&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1019.5641,-257.7616C1022.51,-246.2456 1026.3755,-231.1353 1029.7084,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1033.1519,-218.7676 1032.2395,-208.2121 1026.3703,-217.0327 1033.1519,-218.7676\"/>\n",
       "<text text-anchor=\"middle\" x=\"1058.2363\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 56 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>56</title>\n",
       "<text text-anchor=\"middle\" x=\"1235.8984\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">56 (violent)</text>\n",
       "</g>\n",
       "<!-- 57&#45;&gt;56 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>57&#45;&gt;56</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1235.8984,-257.7616C1235.8984,-246.3597 1235.8984,-231.4342 1235.8984,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1239.3985,-218.2121 1235.8984,-208.2121 1232.3985,-218.2121 1239.3985,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"1251.4502\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11886a320>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(DOSdepParses[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbs, and the noun 'State' both are central to the parsing, and dependency parsing of my sentences. It makes sense that the noun 'State' would be so significant, given that international relations revolves around States. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "\n",
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 26.963 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [29.2 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0410 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/f8/0jh4kzb145vf0215db6q0_bh0000gn/T/tmpko76w2rl\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIE()` prints everything stanford core produces and we can see from looking at it that initializing the dependency parser takes most of the time, so calling the function will always take at least 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>elephant</td>\n",
       "      <td>is in</td>\n",
       "      <td>my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant in my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed stimulus efforts in</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview with Wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "      <td>is in</td>\n",
       "      <td>recent interview with Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview with Wall Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>recent interview</td>\n",
       "      <td>is with</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was African American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                      subject                           verb  \\\n",
       "0         1.0                     elephant                          is in   \n",
       "1         1.0                            I                            saw   \n",
       "2         1.0                            I                            saw   \n",
       "3         1.0              quick brown fox                    jumped over   \n",
       "4         1.0              quick brown fox                    jumped over   \n",
       "5         1.0                    quick fox                    jumped over   \n",
       "6         1.0                          fox                    jumped over   \n",
       "7         1.0                    brown fox                    jumped over   \n",
       "8         1.0                    brown fox                    jumped over   \n",
       "9         1.0                    quick fox                    jumped over   \n",
       "10        1.0                          fox                    jumped over   \n",
       "11        1.0            Christine Lagarde                      discussed   \n",
       "12        1.0            Christine Lagarde                      discussed   \n",
       "13        1.0            Christine Lagarde                      discussed   \n",
       "14        1.0            Christine Lagarde  discussed stimulus efforts in   \n",
       "15        1.0            Christine Lagarde                      discussed   \n",
       "16        1.0            Christine Lagarde                      discussed   \n",
       "17        1.0            Christine Lagarde                      discussed   \n",
       "18        1.0  short-term stimulus efforts                          is in   \n",
       "19        1.0            Christine Lagarde                      discussed   \n",
       "20        1.0            Christine Lagarde                      discussed   \n",
       "21        1.0            Christine Lagarde                      discussed   \n",
       "22        1.0            Christine Lagarde                      discussed   \n",
       "23        1.0             recent interview                        is with   \n",
       "24        1.0                       Martin                            was   \n",
       "25        1.0      Trayvon Benjamin Martin      was African American from   \n",
       "26        1.0      Trayvon Benjamin Martin              was American from   \n",
       "27        1.0      Trayvon Benjamin Martin                            was   \n",
       "28        1.0      Trayvon Benjamin Martin                            was   \n",
       "\n",
       "                                               object  \n",
       "0                                          my pajamas  \n",
       "1                              elephant in my pajamas  \n",
       "2                                            elephant  \n",
       "3                                            lazy dog  \n",
       "4                                                 dog  \n",
       "5                                                 dog  \n",
       "6                                                 dog  \n",
       "7                                            lazy dog  \n",
       "8                                                 dog  \n",
       "9                                            lazy dog  \n",
       "10                                           lazy dog  \n",
       "11  short-term stimulus efforts in interview with ...  \n",
       "12                      stimulus efforts in interview  \n",
       "13               stimulus efforts in recent interview  \n",
       "14                                             France  \n",
       "15  short-term stimulus efforts in recent intervie...  \n",
       "16  stimulus efforts in recent interview with Wall...  \n",
       "17    short-term stimulus efforts in recent interview  \n",
       "18          recent interview with Wall Street Journal  \n",
       "19  stimulus efforts in interview with Wall Street...  \n",
       "20                                   stimulus efforts  \n",
       "21                        short-term stimulus efforts  \n",
       "22           short-term stimulus efforts in interview  \n",
       "23                                Wall Street Journal  \n",
       "24                                            African  \n",
       "25                                            Florida  \n",
       "26                                            Florida  \n",
       "27                                           American  \n",
       "28                                   African American  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No buffalos (because there were no verbs), but the rest is somewhat promising. Note, however, that it abandoned the key theme of the sentence about the tragic Trayvon Martin death (\"fatally shot\"), likely because it was buried so deeply within the complex phrase structure. This is obviously a challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?) How could these approaches inform your research project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x119a4e620>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [7]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Trayvon'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Benjamin'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'compound': [1, 2]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Martin'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'cop',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'was'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'an'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 7,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'African'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'compound': [6],\n",
      "                                      'cop': [4],\n",
      "                                      'det': [5],\n",
      "                                      'nmod': [10],\n",
      "                                      'nsubj': [3]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'American'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 10,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'from'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 10,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Miami'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'NNPS',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'acl:relcl': [23],\n",
      "                                       'appos': [12],\n",
      "                                       'case': [8],\n",
      "                                       'compound': [9]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 7,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNPS',\n",
      "                  'word': 'Gardens'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 10,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'appos',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Florida'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'WP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nsubjpass',\n",
      "                  'tag': 'WP',\n",
      "                  'word': 'who'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 19,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'at'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'CD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 18,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nummod',\n",
      "                  'tag': 'CD',\n",
      "                  'word': '17'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': 'NNS',\n",
      "                  'deps': defaultdict(<class 'list'>, {'nummod': [17]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 19,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod:npmod',\n",
      "                  'tag': 'NNS',\n",
      "                  'word': 'years'},\n",
      "             19: {'address': 19,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'case': [16],\n",
      "                                       'nmod:npmod': [18]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advcl',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'old'},\n",
      "             21: {'address': 21,\n",
      "                  'ctag': 'VBD',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'auxpass',\n",
      "                  'tag': 'VBD',\n",
      "                  'word': 'was'},\n",
      "             22: {'address': 22,\n",
      "                  'ctag': 'RB',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advmod',\n",
      "                  'tag': 'RB',\n",
      "                  'word': 'fatally'},\n",
      "             23: {'address': 23,\n",
      "                  'ctag': 'VBN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advcl': [19],\n",
      "                                       'advmod': [22],\n",
      "                                       'auxpass': [21],\n",
      "                                       'nmod': [26, 36],\n",
      "                                       'nsubjpass': [14]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 10,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'acl:relcl',\n",
      "                  'tag': 'VBN',\n",
      "                  'word': 'shot'},\n",
      "             24: {'address': 24,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 26,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'by'},\n",
      "             25: {'address': 25,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 26,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'George'},\n",
      "             26: {'address': 26,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'appos': [31],\n",
      "                                       'case': [24],\n",
      "                                       'compound': [25]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Zimmerman'},\n",
      "             28: {'address': 28,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 31,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'det',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'a'},\n",
      "             29: {'address': 29,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 31,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'neighborhood'},\n",
      "             30: {'address': 30,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 31,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'watch'},\n",
      "             31: {'address': 31,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'compound': [29, 30],\n",
      "                                       'det': [28]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 26,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'appos',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'volunteer'},\n",
      "             33: {'address': 33,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'in'},\n",
      "             34: {'address': 34,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Sanford'},\n",
      "             36: {'address': 36,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'case': [33],\n",
      "                                       'compound': [34]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 23,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Florida'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "TMDepParseTree = list(depParses[3])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(TMDepParseTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"997pt\" height=\"560pt\"\n",
       " viewBox=\"0.00 0.00 996.96 560.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 556)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-556 992.9575,-556 992.9575,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-515.7616C235.3833,-504.3597 235.3833,-489.4342 235.3833,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-476.2121 235.3833,-466.2121 231.8834,-476.2121 238.8834,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"246.6592\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"77.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5259,-429.8504C183.9641,-424.3453 171.399,-418.1273 160.0454,-412 144.9107,-403.8322 128.6384,-394.1858 114.5678,-385.5556\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.0198,-382.3384 105.6735,-380.0569 112.3388,-388.2925 116.0198,-382.3384\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5522\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"161.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.6898,-429.7616C209.1921,-417.5615 195.2231,-401.3273 183.5899,-387.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.2297,-385.5094 177.0542,-380.2121 180.9236,-390.0751 186.2297,-385.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.4902\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-429.7616C235.3833,-418.3597 235.3833,-403.4342 235.3833,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-390.2121 235.3833,-380.2121 231.8834,-390.2121 238.8834,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"319.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M253.1975,-429.7616C265.2253,-417.4475 281.2673,-401.0235 294.5454,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.1111,-389.8115 301.5947,-380.2121 292.1035,-384.9203 297.1111,-389.8115\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"421.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M281.035,-436.267C301.523,-430.2297 325.6866,-422.0184 346.3833,-412 361.1468,-404.8536 376.3934,-395.1776 389.2526,-386.2414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.6132,-388.8578 397.7385,-380.213 387.5592,-383.1511 391.6132,-388.8578\"/>\n",
       "<text text-anchor=\"middle\" x=\"386.3281\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"146.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.7486,-343.7616C64.8803,-332.1316 58.4773,-316.8357 52.987,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.097,-302.085 49.007,-294.2121 49.6399,-304.788 56.097,-302.085\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.7905,-343.9005C112.4839,-338.7035 119.1771,-332.628 124.3833,-326 129.5733,-319.3927 133.8059,-311.3704 137.1222,-303.7037\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.4118,-304.9025 140.8359,-294.3161 133.9027,-302.3275 140.4118,-304.9025\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"274.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M376.5212,-344.6694C364.1537,-339.2688 350.9417,-332.9006 339.2935,-326 326.8103,-318.6047 313.97,-309.165 303.0389,-300.4839\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"305.0617,-297.6182 295.0895,-294.0398 300.6536,-303.0559 305.0617,-297.6182\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.4282\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"360.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M391.2568,-343.6347C384.9071,-338.5898 378.7774,-332.6501 374.2798,-326 369.9075,-319.5352 366.903,-311.7204 364.8413,-304.2096\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"368.2186,-303.2775 362.5744,-294.3112 361.3952,-304.8402 368.2186,-303.2775\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"456.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M428.8059,-343.7616C433.539,-332.1316 439.7641,-316.8357 445.1019,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"448.4436,-304.7938 448.9714,-294.2121 441.96,-302.1551 448.4436,-304.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.7144\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"549.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M450.562,-343.9652C459.5457,-338.3073 469.4264,-331.9769 478.3833,-326 490.7955,-317.7174 504.223,-308.3654 516.0254,-300.0089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"518.1726,-302.7768 524.2944,-294.1295 514.1163,-297.0718 518.1726,-302.7768\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.9214\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"341.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1602,-260.9769C513.2216,-259.8878 510.2679,-258.8756 507.3833,-258 465.6572,-245.3346 451.0323,-257.1374 410.9351,-240 396.1578,-233.6843 381.463,-223.7913 369.4142,-214.4665\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"371.5209,-211.6696 361.5269,-208.1526 367.1463,-217.1344 371.5209,-211.6696\"/>\n",
       "<text text-anchor=\"middle\" x=\"438.6074\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"423.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1838,-257.9822C506.6151,-252.48 496.2767,-246.2305 487.0659,-240 475.5821,-232.2319 463.494,-223.0275 452.972,-214.6466\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.892,-211.6986 444.9108,-208.1451 450.4974,-217.1473 454.892,-211.6986\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.542\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"504.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.2032,-257.8722C534.8316,-252.2105 531.1955,-245.9014 528.0591,-240 524.2785,-232.8864 520.4337,-225.0669 516.9726,-217.7676\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.9328,-215.8341 512.5329,-208.2517 513.5892,-218.7937 519.9328,-215.8341\"/>\n",
       "<text text-anchor=\"middle\" x=\"550.5454\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"594.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M561.88,-257.643C565.455,-252.0813 569.2329,-245.8848 572.3833,-240 576.1375,-232.9872 579.7933,-225.2024 583.0121,-217.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"586.3752,-218.9423 587.0989,-208.3722 579.9415,-216.1839 586.3752,-218.9423\"/>\n",
       "<text text-anchor=\"middle\" x=\"601.9351\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"707.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.5441,-262.0972C596.8621,-255.7864 613.6686,-247.961 628.3833,-240 643.1389,-232.0169 658.8574,-222.3295 672.3559,-213.6153\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"674.4115,-216.4533 680.8734,-208.0572 670.586,-210.5911 674.4115,-216.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.6464,-267.6451C610.6488,-260.5344 651.722,-249.924 687.3833,-240 726.7862,-229.0348 771.1175,-215.8883 804.4873,-205.8282\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"805.9136,-209.0536 814.474,-202.811 803.889,-202.3527 805.9136,-209.0536\"/>\n",
       "<text text-anchor=\"middle\" x=\"750.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"391.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M472.3833,-85.7616C472.3833,-74.3597 472.3833,-59.4342 472.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.8834,-46.2121 472.3833,-36.2121 468.8834,-46.2121 475.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.2729\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M416.5969,-171.7616C412.2695,-160.1316 406.578,-144.8357 401.6977,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"404.9276,-130.3637 398.1599,-122.2121 398.367,-132.8049 404.9276,-130.3637\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M433.7749,-171.7616C440.5313,-159.9036 449.459,-144.2345 457.0275,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"460.0971,-132.6334 462.0066,-122.2121 454.0151,-129.1681 460.0971,-132.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.7178\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"567.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M668.1869,-171.9496C657.328,-166.5419 645.6763,-160.3501 635.2935,-154 622.6297,-146.2549 609.3446,-136.8017 597.9168,-128.2084\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.0333,-125.4208 589.9593,-122.1398 595.7884,-130.9869 600.0333,-125.4208\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"655.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M684.2392,-171.951C678.6851,-166.6739 673.2261,-160.5473 669.2798,-154 665.2509,-147.3157 662.3575,-139.4301 660.2923,-131.9215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.6687,-130.9902 657.9608,-122.0643 656.8567,-132.6015 663.6687,-130.9902\"/>\n",
       "<text text-anchor=\"middle\" x=\"698.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M719.2595,-171.7616C726.981,-159.9036 737.1841,-144.2345 745.8338,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"749.0005,-132.502 751.5243,-122.2121 743.1345,-128.6822 749.0005,-132.502\"/>\n",
       "<text text-anchor=\"middle\" x=\"755.7144\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M856.3833,-171.7616C856.3833,-160.3597 856.3833,-145.4342 856.3833,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.8834,-132.2121 856.3833,-122.2121 852.8834,-132.2121 859.8834,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"945.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M875.2579,-171.7616C888.0016,-159.4475 904.9985,-143.0235 919.067,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"921.7767,-131.6779 926.5359,-122.2121 916.9125,-126.644 921.7767,-131.6779\"/>\n",
       "<text text-anchor=\"middle\" x=\"934.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"658.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M741.3719,-85.9716C725.9715,-73.358 705.2148,-56.3573 688.313,-42.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"690.3198,-39.6334 680.3658,-36.0047 685.8843,-45.0488 690.3198,-39.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"725.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M763.3833,-85.7616C763.3833,-74.3597 763.3833,-59.4342 763.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"766.8834,-46.2121 763.3833,-36.2121 759.8834,-46.2121 766.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"879.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M798.3154,-85.9914C807.713,-80.629 817.6789,-74.4499 826.3833,-68 836.2881,-60.6606 846.3323,-51.6634 854.9744,-43.3341\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"857.5027,-45.7565 862.1694,-36.2448 852.5896,-40.7703 857.5027,-45.7565\"/>\n",
       "<text text-anchor=\"middle\" x=\"873.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1158fb7f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    TMSentGraph = graphviz.Source(TMDepParseTree.to_dot())\n",
    "except:\n",
    "    TMSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "TMSentGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server on http://localhost:16432 , please wait a few seconds\n",
      "click Kernel -> Then Interupt to stop              (* ﾟДﾟ)            \n",
      "Exiting (ノ≧▽≦)ノ\n"
     ]
    }
   ],
   "source": [
    "stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I used the Core Server above, and found it interesting that 'Trayvon Martin' was not linked to 'shot', even though he is, in grammatical terms, the object of the sentence upon which the act took place. Instead, 'shot' is linked to the noun who took the action, 'George Zimmerman', as well as 'African American', which describes Trayvon Martin. The punctuation in the sentence appears to have an influence on this, but I may be wrong. Overall, 'shot' appears to be the epicenter of this sentence, and 'Trayvon Martin' is the most isolated/ the least linked to the rest of the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also look for subject, object, target triples in one of the reddit stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.9 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 19.925 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [21.8 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0281 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/f8/0jh4kzb145vf0215db6q0_bh0000gn/T/tmpj6fg0r68\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(redditTopScores['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>Quite often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>many homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our digital equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>repeat offenders</td>\n",
       "      <td>is with</td>\n",
       "      <td>long ticket histories of types of issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>speak with</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>So anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak with machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>however was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.780294</td>\n",
       "      <td>handling</td>\n",
       "      <td>types of</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy again for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our front entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put to</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>was</td>\n",
       "      <td>framed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>just going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty                subject                            verb  \\\n",
       "0     1.000000                     we                         'll get   \n",
       "1     1.000000                     we             Quite often 'll get   \n",
       "2     1.000000                     we                   often 'll get   \n",
       "3     0.831036                     we                            coax   \n",
       "4     0.774359  straight analog cable                            coax   \n",
       "5     0.774359           analog cable                            coax   \n",
       "6     0.774359  straight analog cable                            coax   \n",
       "7     1.000000                     we    would supply analog cable to   \n",
       "8     0.831036                     we                            coax   \n",
       "9     0.774359           analog cable                            coax   \n",
       "10    0.831036                     we                            coax   \n",
       "11    0.774359  straight analog cable                            coax   \n",
       "12    0.774359  straight analog cable                            coax   \n",
       "13    0.831036                     we                            coax   \n",
       "14    0.774359           analog cable                            coax   \n",
       "15    1.000000                     we    would supply analog cable to   \n",
       "16    0.774359           analog cable                            coax   \n",
       "17    1.000000          our equipment                         receive   \n",
       "18    1.000000  our digital equipment                         receive   \n",
       "19    1.000000       repeat offenders                         is with   \n",
       "20    1.000000                      I                      anyway get   \n",
       "21    1.000000                      I                             get   \n",
       "22    1.000000                     he                      speak with   \n",
       "23    1.000000                      I                   So anyway get   \n",
       "24    1.000000                     he                             has   \n",
       "25    1.000000                     he                             has   \n",
       "26    1.000000                   call               however was going   \n",
       "27    1.000000                   call                       was going   \n",
       "28    0.780294               handling                        types of   \n",
       "29    1.000000                   call                       was going   \n",
       "..         ...                    ...                             ...   \n",
       "161   1.000000                     it                            made   \n",
       "162   1.000000                     it                            made   \n",
       "163   1.000000                     it                     really made   \n",
       "164   1.000000                    man                       happy for   \n",
       "165   1.000000                old man                 happy again for   \n",
       "166   1.000000                    man                 happy again for   \n",
       "167   1.000000                     it                     really made   \n",
       "168   1.000000                old man                 happy again for   \n",
       "169   1.000000                     it                     really made   \n",
       "170   1.000000                    man                       happy for   \n",
       "171   1.000000                     it                            made   \n",
       "172   1.000000                     it                            made   \n",
       "173   1.000000                     it                     really made   \n",
       "174   1.000000                     it                     really made   \n",
       "175   1.000000                old man                       happy for   \n",
       "176   1.000000                     it                     really made   \n",
       "177   1.000000                     it                            made   \n",
       "178   1.000000                     it                            made   \n",
       "179   1.000000                     it                     really made   \n",
       "180   1.000000                     it                            made   \n",
       "181   1.000000                 letter                          put on   \n",
       "182   1.000000                 letter                          put to   \n",
       "183   1.000000                 letter                          put on   \n",
       "184   1.000000                 letter                             was   \n",
       "185   1.000000                   they                   going through   \n",
       "186   1.000000                   they              just going through   \n",
       "187   1.000000                      I               still think about   \n",
       "188   1.000000                      I  still think occasionally about   \n",
       "189   1.000000                      I        think occasionally about   \n",
       "190   1.000000                      I                     think about   \n",
       "\n",
       "                                             object  \n",
       "0                                             calls  \n",
       "1                                             calls  \n",
       "2                                             calls  \n",
       "3                                      direct to TV  \n",
       "4                                  direct from wall  \n",
       "5                            direct from wall to TV  \n",
       "6                                      direct to TV  \n",
       "7                                             homes  \n",
       "8                                  direct from wall  \n",
       "9                                  direct from wall  \n",
       "10                           direct from wall to TV  \n",
       "11                           direct from wall to TV  \n",
       "12                                           direct  \n",
       "13                                           direct  \n",
       "14                                     direct to TV  \n",
       "15                                       many homes  \n",
       "16                                           direct  \n",
       "17                                         channels  \n",
       "18                                         channels  \n",
       "19         long ticket histories of types of issues  \n",
       "20                                             call  \n",
       "21                                             call  \n",
       "22                                          machine  \n",
       "23                                             call  \n",
       "24                                            speak  \n",
       "25                               speak with machine  \n",
       "26                                 little different  \n",
       "27                                 little different  \n",
       "28                                        customers  \n",
       "29                                        different  \n",
       "..                                              ...  \n",
       "161                                       man happy  \n",
       "162           man happy again for once in long time  \n",
       "163                  old man happy for once in time  \n",
       "164                                            once  \n",
       "165                                    once in time  \n",
       "166                                            once  \n",
       "167                                   old man happy  \n",
       "168                          once in very long time  \n",
       "169                                 man happy again  \n",
       "170                               once in long time  \n",
       "171            old man happy again for once in time  \n",
       "172                        man happy again for once  \n",
       "173  old man happy again for once in very long time  \n",
       "174       old man happy again for once in long time  \n",
       "175                                    once in time  \n",
       "176                      man happy for once in time  \n",
       "177            man happy for once in very long time  \n",
       "178        old man happy for once in very long time  \n",
       "179           man happy again for once in long time  \n",
       "180      man happy again for once in very long time  \n",
       "181                              our front entrance  \n",
       "182                                          retail  \n",
       "183                                    our entrance  \n",
       "184                                          framed  \n",
       "185                                             lot  \n",
       "186                                             lot  \n",
       "187                                       Mr. Smith  \n",
       "188                                       Mr. Smith  \n",
       "189                                       Mr. Smith  \n",
       "190                                       Mr. Smith  \n",
       "\n",
       "[191 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 200 triples in only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redditTopScores['sentences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in redditTopScores['sentences'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find at the most common subject in this story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I                        48\n",
       "it                       42\n",
       "he                       19\n",
       "He                       18\n",
       "we                       11\n",
       "man                       8\n",
       "old man                   8\n",
       "call                      4\n",
       "straight analog cable     4\n",
       "our booking calendar      4\n",
       "analog cable              4\n",
       "letter                    4\n",
       "my supervisor             3\n",
       "you                       2\n",
       "they                      2\n",
       "TV                        2\n",
       "his TV set                2\n",
       "our digital equipment     1\n",
       "people                    1\n",
       "our equipment             1\n",
       "me                        1\n",
       "handling                  1\n",
       "repeat offenders          1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I is followed by various male pronouns and compound nouns (e.g., \"old man\"). 'I' occures most often with the following verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "could come                        8\n",
       "even brought                      5\n",
       "brought                           5\n",
       "was                               4\n",
       "had                               4\n",
       "speak for                         3\n",
       "had cable within                  1\n",
       "So anyway get                     1\n",
       "complaint in                      1\n",
       "instantly felt                    1\n",
       "get                               1\n",
       "think occasionally about          1\n",
       "still think occasionally about    1\n",
       "still think about                 1\n",
       "do                                1\n",
       "anyway get                        1\n",
       "think about                       1\n",
       "eventually had                    1\n",
       "'ve dealt with                    1\n",
       "felt                              1\n",
       "speak with                        1\n",
       "have                              1\n",
       "get to                            1\n",
       "took                              1\n",
       "ask                               1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr. Smith                                             4\n",
       "him                                                   3\n",
       "call                                                  3\n",
       "remote for his set top box                            2\n",
       "simplified remote for his set top box                 2\n",
       "simplified remote                                     2\n",
       "remote                                                2\n",
       "willing                                               2\n",
       "bad                                                   2\n",
       "get                                                   2\n",
       "this                                                  2\n",
       "cable running again                                   1\n",
       "speak for bit about account                           1\n",
       "it                                                    1\n",
       "her                                                   1\n",
       "cable running                                         1\n",
       "cable                                                 1\n",
       "useless                                               1\n",
       "speak for bit about account for Mr. Smith             1\n",
       "30 seconds                                            1\n",
       "speak for bit                                         1\n",
       "speak with her for bit                                1\n",
       "how useless                                           1\n",
       "book                                                  1\n",
       "speak with her for bit about account for Mr. Smith    1\n",
       "residence                                             1\n",
       "speak with her for bit about account                  1\n",
       "bit about account for Mr. Smith                       1\n",
       "bit                                                   1\n",
       "bit about account                                     1\n",
       "experience                                            1\n",
       "speak with her                                        1\n",
       "speak                                                 1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the corenlp server. When you run this server (with the command below), you can click on the browswer link provided to experiment with it. Note that when we run the server, executing the command below, it interrupts the current jupyter process and you will not be able to run code here again (processes will \"hang\" and never finish) until you interrup the process by clicking \"Kernel\" and then \"Interrupt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 5*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa. Describe how you would select among these statements to create a database of high-value statements for your project and then do it by extracting relevant statements into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 18.315 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [20.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0190 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/f8/0jh4kzb145vf0215db6q0_bh0000gn/T/tmpwrx7hbht\n",
      "[main] WARN edu.stanford.nlp.process.PTBLexer - Untokenizable:  (U+83, decimal: 131)\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOSieDF = stanford.openIE(DOSSpeechesDF['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>event</td>\n",
       "      <td>happening during</td>\n",
       "      <td>anniversary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>is</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>is</td>\n",
       "      <td>also worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>event</td>\n",
       "      <td>happening during</td>\n",
       "      <td>25th anniversary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>am</td>\n",
       "      <td>honored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>have</td>\n",
       "      <td>opportunity speak today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>am</td>\n",
       "      <td>Therefore honored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>have</td>\n",
       "      <td>opportunity speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>have</td>\n",
       "      <td>opportunity speak with you today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>have</td>\n",
       "      <td>opportunity speak with you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>be in</td>\n",
       "      <td>here Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>am</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>be in</td>\n",
       "      <td>Singapore again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>be in</td>\n",
       "      <td>here Singapore again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>be in</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>am</td>\n",
       "      <td>equally happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My first visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker years ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association years ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker years ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association of Prosec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker many years ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My first visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My first visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker many years ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My first visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association of Prosec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My first visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association many year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My first visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association of Prosec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>My visit</td>\n",
       "      <td>was as</td>\n",
       "      <td>speaker at International Association many year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our digital future</td>\n",
       "      <td>will depend As</td>\n",
       "      <td>theme for week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>can make</td>\n",
       "      <td>difference in world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.814022</td>\n",
       "      <td>we</td>\n",
       "      <td>difference in</td>\n",
       "      <td>world of safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>difference</td>\n",
       "      <td>is in</td>\n",
       "      <td>world of online safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our future</td>\n",
       "      <td>will depend As</td>\n",
       "      <td>theme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>can make</td>\n",
       "      <td>difference in world of online safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>can make difference through</td>\n",
       "      <td>partnership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our future</td>\n",
       "      <td>will depend on</td>\n",
       "      <td>partnership _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our digital future</td>\n",
       "      <td>will depend on</td>\n",
       "      <td>partnership _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our future</td>\n",
       "      <td>will depend As</td>\n",
       "      <td>theme for week highlights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging with everyone over few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging over next few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging with everyone over days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging over days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>engaging over</td>\n",
       "      <td>few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging over next days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging with everyone over next few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging over few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging over next days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>engaging over</td>\n",
       "      <td>days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>engaging over</td>\n",
       "      <td>next days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging with everyone over days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging with everyone over next days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging with everyone over next days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>engaging over</td>\n",
       "      <td>next few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging over few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging over next few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging over days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look forward</td>\n",
       "      <td>engaging with everyone over next few days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>look</td>\n",
       "      <td>engaging with everyone over few days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty             subject                         verb  \\\n",
       "0     1.000000               event             happening during   \n",
       "1     1.000000                  It                           is   \n",
       "2     1.000000                  It                           is   \n",
       "3     1.000000               event             happening during   \n",
       "4     1.000000                   I                           am   \n",
       "5     1.000000                   I                         have   \n",
       "6     1.000000                   I                           am   \n",
       "7     1.000000                   I                         have   \n",
       "8     1.000000                   I                         have   \n",
       "9     1.000000                   I                         have   \n",
       "10    1.000000                   I                        be in   \n",
       "11    1.000000                   I                           am   \n",
       "12    1.000000                   I                        be in   \n",
       "13    1.000000                   I                        be in   \n",
       "14    1.000000                   I                        be in   \n",
       "15    1.000000                   I                           am   \n",
       "16    1.000000      My first visit                       was as   \n",
       "17    1.000000            My visit                       was as   \n",
       "18    1.000000            My visit                       was as   \n",
       "19    1.000000            My visit                       was as   \n",
       "20    1.000000            My visit                       was as   \n",
       "21    1.000000      My first visit                       was as   \n",
       "22    1.000000            My visit                       was as   \n",
       "23    1.000000      My first visit                       was as   \n",
       "24    1.000000      My first visit                       was as   \n",
       "25    1.000000            My visit                       was as   \n",
       "26    1.000000            My visit                       was as   \n",
       "27    1.000000      My first visit                       was as   \n",
       "28    1.000000      My first visit                       was as   \n",
       "29    1.000000            My visit                       was as   \n",
       "..         ...                 ...                          ...   \n",
       "330   1.000000  our digital future               will depend As   \n",
       "331   1.000000                  we                     can make   \n",
       "332   0.814022                  we                difference in   \n",
       "333   1.000000          difference                        is in   \n",
       "334   1.000000          our future               will depend As   \n",
       "335   1.000000                  we                     can make   \n",
       "336   1.000000                  we  can make difference through   \n",
       "337   1.000000          our future               will depend on   \n",
       "338   1.000000  our digital future               will depend on   \n",
       "339   1.000000          our future               will depend As   \n",
       "340   1.000000                   I                 look forward   \n",
       "341   1.000000                   I                 look forward   \n",
       "342   1.000000                   I                         look   \n",
       "343   1.000000                   I                 look forward   \n",
       "344   1.000000                   I                engaging over   \n",
       "345   1.000000                   I                         look   \n",
       "346   1.000000                   I                         look   \n",
       "347   1.000000                   I                 look forward   \n",
       "348   1.000000                   I                 look forward   \n",
       "349   1.000000                   I                engaging over   \n",
       "350   1.000000                   I                engaging over   \n",
       "351   1.000000                   I                 look forward   \n",
       "352   1.000000                   I                         look   \n",
       "353   1.000000                   I                 look forward   \n",
       "354   1.000000                   I                engaging over   \n",
       "355   1.000000                   I                         look   \n",
       "356   1.000000                   I                         look   \n",
       "357   1.000000                   I                         look   \n",
       "358   1.000000                   I                 look forward   \n",
       "359   1.000000                   I                         look   \n",
       "\n",
       "                                                object  \n",
       "0                                          anniversary  \n",
       "1                                                worth  \n",
       "2                                           also worth  \n",
       "3                                     25th anniversary  \n",
       "4                                              honored  \n",
       "5                              opportunity speak today  \n",
       "6                                    Therefore honored  \n",
       "7                                    opportunity speak  \n",
       "8                     opportunity speak with you today  \n",
       "9                           opportunity speak with you  \n",
       "10                                      here Singapore  \n",
       "11                                               happy  \n",
       "12                                     Singapore again  \n",
       "13                                here Singapore again  \n",
       "14                                           Singapore  \n",
       "15                                       equally happy  \n",
       "16                                   speaker years ago  \n",
       "17      speaker at International Association years ago  \n",
       "18                                   speaker years ago  \n",
       "19   speaker at International Association of Prosec...  \n",
       "20                              speaker many years ago  \n",
       "21                                             speaker  \n",
       "22                                             speaker  \n",
       "23                              speaker many years ago  \n",
       "24                speaker at International Association  \n",
       "25                speaker at International Association  \n",
       "26   speaker at International Association of Prosec...  \n",
       "27   speaker at International Association many year...  \n",
       "28   speaker at International Association of Prosec...  \n",
       "29   speaker at International Association many year...  \n",
       "..                                                 ...  \n",
       "330                                     theme for week  \n",
       "331                                difference in world  \n",
       "332                                    world of safety  \n",
       "333                             world of online safety  \n",
       "334                                              theme  \n",
       "335               difference in world of online safety  \n",
       "336                                        partnership  \n",
       "337                                      partnership _  \n",
       "338                                      partnership _  \n",
       "339                          theme for week highlights  \n",
       "340               engaging with everyone over few days  \n",
       "341                        engaging over next few days  \n",
       "342                   engaging with everyone over days  \n",
       "343                                 engaging over days  \n",
       "344                                           few days  \n",
       "345                            engaging over next days  \n",
       "346          engaging with everyone over next few days  \n",
       "347                             engaging over few days  \n",
       "348                            engaging over next days  \n",
       "349                                               days  \n",
       "350                                          next days  \n",
       "351                   engaging with everyone over days  \n",
       "352              engaging with everyone over next days  \n",
       "353              engaging with everyone over next days  \n",
       "354                                      next few days  \n",
       "355                             engaging over few days  \n",
       "356                        engaging over next few days  \n",
       "357                                 engaging over days  \n",
       "358          engaging with everyone over next few days  \n",
       "359               engaging with everyone over few days  \n",
       "\n",
       "[360 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOSieDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DOSSpeechesDF['sentences'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in DOSSpeechesDF['sentences'][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I                                     38\n",
       "we                                    35\n",
       "states                                23\n",
       "We                                    19\n",
       "GGE report                            14\n",
       "2015 GGE report                       14\n",
       "it                                    12\n",
       "measures                              11\n",
       "United States                         11\n",
       "It                                     9\n",
       "My visit                               9\n",
       "My first visit                         9\n",
       "like-minded states                     8\n",
       "cybersecurity memorandum               8\n",
       "they                                   7\n",
       "Similar measures                       7\n",
       "some                                   7\n",
       "first office                           5\n",
       "John Kerry                             5\n",
       "internet revolution                    5\n",
       "office                                 5\n",
       "other nations                          4\n",
       "our digital future                     4\n",
       "pillar                                 4\n",
       "our future                             4\n",
       "each                                   3\n",
       "nation                                 3\n",
       "building                               3\n",
       "its people                             3\n",
       "our security                           3\n",
       "                                      ..\n",
       "key bilateral engagements              1\n",
       "its recommendations                    1\n",
       "responsible state behavior             1\n",
       "core priority                          1\n",
       "core diplomatic priority               1\n",
       "G20 leaders                            1\n",
       "help                                   1\n",
       "high level                             1\n",
       "raise                                  1\n",
       "stability                              1\n",
       "Our businesses                         1\n",
       "strategic framework                    1\n",
       "Summits                                1\n",
       "medical records                        1\n",
       "difference                             1\n",
       "expansion                              1\n",
       "confidence                             1\n",
       "citizens                               1\n",
       "Singapore                              1\n",
       "concepts                               1\n",
       "state conduct                          1\n",
       "lot                                    1\n",
       "steps                                  1\n",
       "responsible state conduct              1\n",
       "social benefits                        1\n",
       "inclusion                              1\n",
       "framework                              1\n",
       "U.S. Cybersecurity Awareness Month     1\n",
       "strengthening transparency             1\n",
       "cyberspace                             1\n",
       "Name: subject, Length: 81, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOSieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "work together against    4\n",
       "work against             4\n",
       "be                       3\n",
       "be guided in             2\n",
       "are                      2\n",
       "fully realize            1\n",
       "take seriously           1\n",
       "realize                  1\n",
       "take                     1\n",
       "_ in                     1\n",
       "act in                   1\n",
       "be guided by             1\n",
       "_ during                 1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOSieDF[DOSieDF['subject'] == 'states']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threats                                              4\n",
       "its benefits                                         2\n",
       "common non-state threats                             2\n",
       "non-state threats                                    2\n",
       "common threats                                       2\n",
       "conflict _                                           1\n",
       "guided in their use of information by 2015 report    1\n",
       "guided in their use by 2015 report                   1\n",
       "2015 report                                          1\n",
       "guided                                               1\n",
       "able                                                 1\n",
       "peacetime                                            1\n",
       "their use of information                             1\n",
       "their use                                            1\n",
       "where able                                           1\n",
       "cyberspace                                           1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOSieDF[DOSieDF['subject'] == 'states']['object'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
